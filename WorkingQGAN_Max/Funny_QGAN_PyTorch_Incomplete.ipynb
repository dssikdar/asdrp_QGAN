{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Funny QGAN PyTorch Incomplete.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_ViiXD4Pnro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bba2fe9-5eb4-4275-f34f-6391023d1c09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pennylane\n",
            "  Downloading PennyLane-0.21.0-py3-none-any.whl (800 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 20.8 MB/s eta 0:00:01\r\u001b[K     |▉                               | 20 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 40 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 61 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 71 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 81 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 92 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 102 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 112 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████                           | 122 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 133 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 143 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 153 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 163 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████                         | 174 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 184 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 194 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 204 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 215 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 225 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 235 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 245 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 256 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 266 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 276 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 286 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 296 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 307 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 317 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 327 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 337 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 348 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 358 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 368 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 378 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 389 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 399 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 409 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 419 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 430 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 440 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 450 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 460 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 471 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 481 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 491 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 501 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 512 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 522 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 532 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 542 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 552 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 563 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 573 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 583 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 593 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 604 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 614 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 624 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 634 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 645 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 655 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 665 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 675 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 686 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 696 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 706 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 716 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 727 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 737 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 747 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 757 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 768 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 778 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 788 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 798 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 800 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pennylane) (1.4.4)\n",
            "Collecting semantic-version==2.6\n",
            "  Downloading semantic_version-2.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.7/dist-packages (from pennylane) (4.2.4)\n",
            "Collecting retworkx\n",
            "  Downloading retworkx-0.11.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 37.6 MB/s \n",
            "\u001b[?25hCollecting autoray\n",
            "  Downloading autoray-0.2.5-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pennylane) (1.4.1)\n",
            "Collecting toml\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from pennylane) (2.6.3)\n",
            "Collecting pennylane-lightning>=0.21\n",
            "  Downloading PennyLane_Lightning-0.21.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.8 MB 24.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: autograd in /usr/local/lib/python3.7/dist-packages (from pennylane) (1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pennylane) (1.21.5)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 46.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd->pennylane) (0.16.0)\n",
            "Installing collected packages: ninja, toml, semantic-version, retworkx, pennylane-lightning, autoray, pennylane\n",
            "Successfully installed autoray-0.2.5 ninja-1.10.2.3 pennylane-0.21.0 pennylane-lightning-0.21.0 retworkx-0.11.0 semantic-version-2.6.0 toml-0.10.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pennylane"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import pennylane as qml\n",
        "\n",
        "# Pytorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ],
      "metadata": {
        "id": "D6-GrrkpP_1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(32, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "EouLfZFbQE-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_qubits = 16  # Total number of qubits / N\n",
        "n_a_qubits = 1  # Number of ancillary qubits / N_A\n",
        "q_depth = 4  # Depth of the parameterised quantum circuit / D #try 4 for now\n",
        "n_generators = 1  # Number of subgenerators for the patch method / N_G well obviously 1"
      ],
      "metadata": {
        "id": "iJ8hXeO6P3SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev = qml.device(\"lightning.qubit\", wires=n_qubits)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #NTS: use GPU runtime"
      ],
      "metadata": {
        "id": "Rk1vWlruQBGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@qml.qnode(dev, interface=\"torch\", diff_method=\"parameter-shift\")\n",
        "def quantum_circuit(noise, weights):\n",
        "    weights = weights.reshape(q_depth, n_qubits)\n",
        "    #Optional: superposition\n",
        "    #for i in range(n_qubits):\n",
        "    #    qml.Hadamard(wires=i)\n",
        "    #    qml.CNOT(wires=[i, (i+1) % n_qubits])\n",
        "\n",
        "    # Initialise latent vectors using noise\n",
        "    for i in range(n_qubits):\n",
        "        qml.RY(noise[i], wires=i)\n",
        "\n",
        "    # Repeated layer\n",
        "    for i in range(q_depth):\n",
        "        # Parameterised layer\n",
        "        for y in range(n_qubits):\n",
        "            qml.RY(weights[i][y], wires=y)\n",
        "            #Optional: more parameters\n",
        "            #qml.RX(weights[i][y], wires=y)\n",
        "            #qml.RZ(weights[i][y], wires=y)\n",
        "\n",
        "        # Control Z gates\n",
        "        for y in range(n_qubits - 1):\n",
        "            qml.CZ(wires=[y, y + 1])\n",
        "\n",
        "    return qml.probs(wires=list(range(n_qubits)))\n"
      ],
      "metadata": {
        "id": "frNn_PvqQE97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Not sure what this does\n",
        "def partial_measure(noise, weights):\n",
        "    # Non-linear Transform\n",
        "    probs = quantum_circuit(noise, weights)\n",
        "    probsgiven0 = probs[: (2 ** (n_qubits - n_a_qubits))]\n",
        "    probsgiven0 /= torch.sum(probs)\n",
        "\n",
        "    # Post-Processing\n",
        "    probsgiven = probsgiven0 / torch.max(probsgiven0)\n",
        "    return probsgiven"
      ],
      "metadata": {
        "id": "YWwnUutuQpxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantumGenerator(nn.Module):\n",
        "    \"\"\"Quantum generator class\"\"\"\n",
        "\n",
        "    def __init__(self, n_generators=1, q_delta=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            n_generators (int): Number of sub-generators to be used in the patch method.\n",
        "            q_delta (float, optional): Spread of the random distribution for parameter initialisation.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.q_params = nn.ParameterList(\n",
        "            [\n",
        "                nn.Parameter(q_delta * torch.rand(q_depth * n_qubits), requires_grad=True)\n",
        "                for _ in range(n_generators)\n",
        "            ]\n",
        "        )\n",
        "        self.n_generators = n_generators\n",
        "\n",
        "    def forward(self, x): #x must be valid molecule stuff\n",
        "        # Size of each sub-generator output\n",
        "        patch_size = 2 ** (n_qubits - n_a_qubits)\n",
        "\n",
        "        # Create a Tensor to 'catch' a batch of images from the for loop. x.size(0) is the batch size.\n",
        "        molecules = torch.Tensor(x.size(0), 0).to(device)\n",
        "\n",
        "        # Iterate over all sub-generators\n",
        "        for params in self.q_params:\n",
        "\n",
        "            # Create a Tensor to 'catch' a batch of the patches from a single sub-generator there is only one so who cares\n",
        "            patches = torch.Tensor(0, patch_size).to(device)\n",
        "            for elem in x:\n",
        "                q_out = partial_measure(elem, params).float().unsqueeze(0)\n",
        "                patches = torch.cat((patches, q_out))\n",
        "\n",
        "            molecules = torch.cat((molecules, patches), 1)\n",
        "\n",
        "        return molecules"
      ],
      "metadata": {
        "id": "P9jWLTl2Qx1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lrG = 0.3  # Learning rate for the generator\n",
        "lrD = 0.01  # Learning rate for the discriminator\n",
        "num_iter = 420  # Number of training iterations"
      ],
      "metadata": {
        "id": "B_Jdb0s2RjnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = Discriminator().to(device)\n",
        "generator = QuantumGenerator().to(device)"
      ],
      "metadata": {
        "id": "D5j-6lIMRW_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```python\n",
        "# This is formatted as code\n",
        "\n",
        "criterion = nn.BCELoss() #TODO FIND A LOSS FUNCTION\n",
        "\n",
        "# Optimisers\n",
        "optD = optim.SGD(discriminator.parameters(), lr=lrD)\n",
        "optG = optim.SGD(generator.parameters(), lr=lrG)\n",
        "\n",
        "batch_size = None #TODO: DEFINE A BATCH SIZE\n",
        "\n",
        "real_labels = torch.full((batch_size,), 1.0, dtype=torch.float, device=device)\n",
        "fake_labels = torch.full((batch_size,), 0.0, dtype=torch.float, device=device)\n",
        "\n",
        "# Fixed noise allows us to visually track the generated MOLECULES throughout training\n",
        "fixed_noise = torch.rand(8, n_qubits, device=device) * math.pi / 2\n",
        "\n",
        "# Iteration counter\n",
        "counter = 0\n",
        "\n",
        "# Collect images for plotting later\n",
        "results = []\n",
        "\n",
        "while True:\n",
        "    for i, (data, _) in enumerate(dataloader):\n",
        "\n",
        "        # Data for training the discriminator\n",
        "        data = data.reshape(-1, image_size * image_size)\n",
        "        real_data = data.to(device)\n",
        "\n",
        "        # Noise follwing a uniform distribution in range [0,pi/2)\n",
        "        noise = torch.rand(batch_size, n_qubits, device=device) * math.pi / 2\n",
        "        fake_data = generator(noise)\n",
        "\n",
        "        # Training the discriminator\n",
        "        discriminator.zero_grad()\n",
        "        outD_real = discriminator(real_data).view(-1)\n",
        "        outD_fake = discriminator(fake_data.detach()).view(-1)\n",
        "\n",
        "        errD_real = criterion(outD_real, real_labels)\n",
        "        errD_fake = criterion(outD_fake, fake_labels)\n",
        "        # Propagate gradients\n",
        "        errD_real.backward()\n",
        "        errD_fake.backward()\n",
        "\n",
        "        errD = errD_real + errD_fake\n",
        "        optD.step()\n",
        "\n",
        "        # Training the generator\n",
        "        generator.zero_grad()\n",
        "        outD_fake = discriminator(fake_data).view(-1)\n",
        "        errG = criterion(outD_fake, real_labels)\n",
        "        errG.backward()\n",
        "        optG.step()\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "        # Show loss values\n",
        "        if counter % 10 == 0:\n",
        "            print(f'Iteration: {counter}, Discriminator Loss: {errD:0.3f}, Generator Loss: {errG:0.3f}')\n",
        "            test_images = generator(fixed_noise).view(8,1,image_size,image_size).cpu().detach()\n",
        "\n",
        "            # Save images every 50 iterations\n",
        "            if counter % 50 == 0:\n",
        "                results.append(test_images)\n",
        "\n",
        "        if counter == num_iter:\n",
        "            break\n",
        "    if counter == num_iter:\n",
        "        break\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "WYnZAdDsSCTC"
      }
    }
  ]
}