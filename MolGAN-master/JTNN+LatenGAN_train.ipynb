{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'razorback (1).mp3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#url = 'http://www.futurecrew.com/skaven/song_files/mp3/razorback.mp3'\n",
    "#filename = wget.download(url)\n",
    "#filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "colab_type": "code",
    "id": "k85YYlp3wi9a",
    "outputId": "9f2f79b7-4d98-4008-cdbe-11479e946cbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X_JTVAE_250k_rndm_zinc.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wget.download('http://molcyclegan.ardigen.com/250k_rndm_zinc_drugs_clean_3_canonized.csv')\n",
    "wget.download('http://molcyclegan.ardigen.com/X_JTVAE_250k_rndm_zinc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TjywBaDEVFCx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0cTrQarwwOk5"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('X_JTVAE_250k_rndm_zinc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jxlFYRMHTPf0"
   },
   "outputs": [],
   "source": [
    "smiles = data['SMILES'].values\n",
    "np.savetxt(r'smiles.txt', smiles, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mh-408HFrktC"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, data_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.data_shape = data_shape\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(self.data_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, mol):\n",
    "        validity = self.model(mol)\n",
    "        return validity\n",
    "\n",
    "    def save(self, path):\n",
    "        save_dict = {\n",
    "            'model': self.model.state_dict(),\n",
    "            'data_shape': self.data_shape,\n",
    "        }\n",
    "        torch.save(save_dict, path)\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        save_dict = torch.load(path)\n",
    "        D = Discriminator(save_dict['data_shape'])\n",
    "        D.model.load_state_dict(save_dict[\"model\"])\n",
    "\n",
    "        return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d0ZmwIN5VPeZ"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, data_shape, latent_dim=None):\n",
    "        super(Generator, self).__init__()\n",
    "        self.data_shape = data_shape\n",
    "\n",
    "        # latent dim of the generator is one of the hyperparams.\n",
    "        # by default it is set to the prod of data_shapes\n",
    "        self.latent_dim = int(np.prod(self.data_shape)) if latent_dim is None else latent_dim\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(self.latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(self.data_shape))),\n",
    "            # nn.Tanh() # expecting latent vectors to be not normalized\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.model(z)\n",
    "        return out\n",
    "\n",
    "    def save(self, path):\n",
    "        save_dict = {\n",
    "            'latent_dim': self.latent_dim,\n",
    "            'model': self.model.state_dict(),\n",
    "            'data_shape': self.data_shape,\n",
    "        }\n",
    "        torch.save(save_dict, path)\n",
    "\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        save_dict = torch.load(path)\n",
    "        G = Generator(save_dict['data_shape'], latent_dim=save_dict['latent_dim'])\n",
    "        G.model.load_state_dict(save_dict[\"model\"])\n",
    "\n",
    "        return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_NWvRUvVS5G"
   },
   "outputs": [],
   "source": [
    "class Sampler(object):\n",
    "    \"\"\"\n",
    "    Sampling the mols the generator.\n",
    "    All scripts should use this class for sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, generator: Generator):\n",
    "        self.set_generator(generator)\n",
    "\n",
    "    def set_generator(self, generator):\n",
    "        self.G = generator\n",
    "\n",
    "    def sample(self, n):\n",
    "        # Sample noise as generator input\n",
    "        #z = torch.cuda.FloatTensor(np.random.uniform(-1, 1, (n, self.G.latent_dim)))\n",
    "        z = torch.FloatTensor(np.random.uniform(-1, 1, (n, self.G.latent_dim)))\n",
    "        # Generate a batch of mols\n",
    "        return self.G(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qlo1tKSDAuHo"
   },
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LatentMolsDataset(data.Dataset):\n",
    "    def __init__(self, latent_space_mols):\n",
    "        self.data = latent_space_mols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2xjPf5FRV4xT"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "class TrainModelRunner:\n",
    "    # Loss weight for gradient penalty\n",
    "    lambda_gp = 10\n",
    "\n",
    "    def __init__(self, input_data_path, output_model_folder, decode_mols_save_path='', n_epochs=2000, starting_epoch=1,\n",
    "                 batch_size=2500, lr=0.0002, b1=0.5, b2=0.999,  n_critic=5,\n",
    "                 save_interval=1000, sample_after_training=30000, message=\"\"):\n",
    "        self.message = message\n",
    "\n",
    "        # init params\n",
    "        self.input_data_path = input_data_path\n",
    "        self.output_model_folder = output_model_folder\n",
    "        self.n_epochs = n_epochs\n",
    "        self.starting_epoch = starting_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "        self.n_critic = n_critic\n",
    "        self.save_interval = save_interval\n",
    "        self.sample_after_training = sample_after_training\n",
    "        self.decode_mols_save_path = decode_mols_save_path\n",
    "\n",
    "        # initialize dataloader\n",
    "        smiles_lat = pd.read_csv(input_data_path)\n",
    "        latent_space_mols = smiles_lat.drop('SMILES', axis=1).values\n",
    "        latent_space_mols = latent_space_mols.reshape(latent_space_mols.shape[0], 56)\n",
    "\n",
    "        self.dataloader = torch.utils.data.DataLoader(LatentMolsDataset(latent_space_mols), shuffle=True,\n",
    "                                                      batch_size=self.batch_size, drop_last=True)\n",
    "\n",
    "        # load discriminator\n",
    "        discriminator_name = 'discriminator.txt' if self.starting_epoch == 1 else str(\n",
    "            self.starting_epoch) + '_discriminator.txt'\n",
    "        discriminator_path = os.path.join(output_model_folder, discriminator_name)\n",
    "        #self.D = Discriminator.load(discriminator_path)\n",
    "        self.D = Discriminator(latent_space_mols[0].shape)\n",
    "        # load generator\n",
    "        generator_name = 'generator.txt' if self.starting_epoch == 1 else str(\n",
    "            self.starting_epoch) + '_generator.txt'\n",
    "        generator_path = os.path.join(output_model_folder, generator_name)\n",
    "        #self.G = Generator.load(generator_path)\n",
    "        self.G = Generator(latent_space_mols[0].shape)\n",
    "        # initialize sampler\n",
    "        self.Sampler = Sampler(self.G)\n",
    "\n",
    "        # initialize optimizer\n",
    "        self.optimizer_G = torch.optim.Adam(self.G.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
    "        self.optimizer_D = torch.optim.Adam(self.D.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
    "\n",
    "        # Tensor\n",
    "        cuda = False #True if torch.cuda.is_available() else False\n",
    "        if cuda:\n",
    "            self.G.cuda()\n",
    "            self.D.cuda()\n",
    "        self.Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        print(\"Run began.\")\n",
    "        print(\"Message: %s\" % self.message)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        batches_done = 0\n",
    "        disc_loss_log = []\n",
    "        g_loss_log = []\n",
    "\n",
    "        for epoch in range(self.starting_epoch, self.n_epochs + self.starting_epoch):\n",
    "            disc_loss_per_batch = []\n",
    "            g_loss_log_per_batch = []\n",
    "            for i, real_mols in enumerate(self.dataloader):\n",
    "\n",
    "                # Configure input\n",
    "                real_mols = real_mols.type(self.Tensor)\n",
    "                # real_mols = np.squeeze(real_mols, axis=1)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                self.optimizer_D.zero_grad()\n",
    "\n",
    "                # Generate a batch of mols from noise\n",
    "                fake_mols = self.Sampler.sample(real_mols.shape[0])\n",
    "\n",
    "                # Real mols\n",
    "                real_validity = self.D(real_mols)\n",
    "                # Fake mols\n",
    "                fake_validity = self.D(fake_mols)\n",
    "                # Gradient penalty\n",
    "                gradient_penalty = self.compute_gradient_penalty(real_mols.data, fake_mols.data)\n",
    "                # Adversarial loss\n",
    "                d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + self.lambda_gp * gradient_penalty\n",
    "                disc_loss_per_batch.append(d_loss.item())\n",
    "\n",
    "                d_loss.backward()\n",
    "                self.optimizer_D.step()\n",
    "                self.optimizer_G.zero_grad()\n",
    "\n",
    "                # Train the generator every n_critic steps\n",
    "                if i % self.n_critic == 0:\n",
    "                    # -----------------\n",
    "                    #  Train Generator\n",
    "                    # -----------------\n",
    "\n",
    "                    # Generate a batch of mols\n",
    "                    fake_mols = self.Sampler.sample(real_mols.shape[0])\n",
    "                    # Loss measures generator's ability to fool the discriminator\n",
    "                    # Train on fake images\n",
    "                    fake_validity = self.D(fake_mols)\n",
    "                    g_loss = -torch.mean(fake_validity)\n",
    "                    g_loss_log_per_batch.append(g_loss.item())\n",
    "\n",
    "                    g_loss.backward()\n",
    "                    self.optimizer_G.step()\n",
    "\n",
    "                    batches_done += self.n_critic\n",
    "\n",
    "                # If last batch in the set\n",
    "                if i == len(self.dataloader) - 1:\n",
    "                    if epoch % self.save_interval == 0:\n",
    "                        generator_save_path = os.path.join(self.output_model_folder,\n",
    "                                                           str(epoch) + '_generator.txt')\n",
    "                        discriminator_save_path = os.path.join(self.output_model_folder,\n",
    "                                                               str(epoch) + '_discriminator.txt')\n",
    "                        self.G.save(generator_save_path)\n",
    "                        self.D.save(discriminator_save_path)\n",
    "\n",
    "                    disc_loss_log.append([time.time(), epoch, np.mean(disc_loss_per_batch)])\n",
    "                    g_loss_log.append([time.time(), epoch, np.mean(g_loss_log_per_batch)])\n",
    "\n",
    "                    # Print and log\n",
    "                    print(\n",
    "                        \"[Epoch %d/%d]  [Disc loss: %f] [Gen loss: %f] \"\n",
    "                        % (epoch, self.n_epochs + self.starting_epoch, disc_loss_log[-1][2], g_loss_log[-1][2])\n",
    "                    )\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "        # log the losses\n",
    "        with open(os.path.join(self.output_model_folder, 'disc_loss.json'), 'w') as json_file:\n",
    "            json.dump(disc_loss_log, json_file)\n",
    "        with open(os.path.join(self.output_model_folder, 'gen_loss.json'), 'w') as json_file:\n",
    "            json.dump(g_loss_log, json_file)\n",
    "\n",
    "        # Sampling after training\n",
    "        if self.sample_after_training > 0:\n",
    "            print(\"Training finished. Generating sample of latent vectors\")\n",
    "            # sampling mode\n",
    "            torch.no_grad()\n",
    "            self.G.eval()\n",
    "\n",
    "            S = Sampler(generator=self.G)\n",
    "            latent = S.sample(self.sample_after_training)\n",
    "            latent = latent.detach().cpu().numpy().tolist()\n",
    "\n",
    "            sampled_mols_save_path = os.path.join(self.output_model_folder, 'sampled')\n",
    "            np.save(sampled_mols_save_path+f'_epoch{epoch}', latent)\n",
    "\n",
    "            # decoding sampled mols\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def compute_gradient_penalty(self, real_samples, fake_samples):\n",
    "        \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "        # Random weight term for interpolation between real and fake samples\n",
    "        alpha = self.Tensor(np.random.random((real_samples.size(0), 1)))\n",
    "\n",
    "        # Get random interpolation between real and fake samples\n",
    "        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "        d_interpolates = self.D(interpolates)\n",
    "        fake = self.Tensor(real_samples.shape[0], 1).fill_(1.0)\n",
    "\n",
    "        # Get gradient w.r.t. interpolates\n",
    "        gradients = autograd.grad(\n",
    "            outputs=d_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "        return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGNNUWy1Az_B"
   },
   "outputs": [],
   "source": [
    "trainer = TrainModelRunner('X_JTVAE_250k_rndm_zinc.csv', output_model_folder='sussyoutput', starting_epoch=200,\n",
    "                           save_interval=100, message='Starting training', batch_size=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gAV2y8c_kHWc",
    "outputId": "a83a21ea-37b5-417c-c238-b00d1480b819"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run began.\n",
      "Message: Starting training\n",
      "[Epoch 200/2200]  [Disc loss: -4.645137] [Gen loss: -0.374319] \n",
      "[Epoch 201/2200]  [Disc loss: -10.986084] [Gen loss: -1.259094] \n",
      "[Epoch 202/2200]  [Disc loss: -10.300328] [Gen loss: -0.402960] \n",
      "[Epoch 203/2200]  [Disc loss: -8.755223] [Gen loss: -0.567260] \n",
      "[Epoch 204/2200]  [Disc loss: -8.522351] [Gen loss: -0.831483] \n",
      "[Epoch 205/2200]  [Disc loss: -7.882027] [Gen loss: -0.743010] \n",
      "[Epoch 206/2200]  [Disc loss: -7.706859] [Gen loss: -0.698369] \n",
      "[Epoch 207/2200]  [Disc loss: -7.293935] [Gen loss: -0.704014] \n",
      "[Epoch 208/2200]  [Disc loss: -6.898403] [Gen loss: -0.808117] \n",
      "[Epoch 209/2200]  [Disc loss: -6.483658] [Gen loss: -0.788569] \n",
      "[Epoch 210/2200]  [Disc loss: -6.178068] [Gen loss: -0.770316] \n",
      "[Epoch 211/2200]  [Disc loss: -5.837696] [Gen loss: -0.832042] \n",
      "[Epoch 212/2200]  [Disc loss: -5.513508] [Gen loss: -0.637416] \n",
      "[Epoch 213/2200]  [Disc loss: -5.297909] [Gen loss: -0.618674] \n",
      "[Epoch 214/2200]  [Disc loss: -4.989776] [Gen loss: -0.811181] \n",
      "[Epoch 215/2200]  [Disc loss: -4.809803] [Gen loss: -0.609585] \n",
      "[Epoch 216/2200]  [Disc loss: -4.665066] [Gen loss: -0.268409] \n",
      "[Epoch 217/2200]  [Disc loss: -4.709522] [Gen loss: -0.345816] \n",
      "[Epoch 218/2200]  [Disc loss: -4.667938] [Gen loss: -0.115734] \n",
      "[Epoch 219/2200]  [Disc loss: -4.683046] [Gen loss: -0.167608] \n",
      "[Epoch 220/2200]  [Disc loss: -4.610766] [Gen loss: -0.752871] \n",
      "[Epoch 221/2200]  [Disc loss: -4.314657] [Gen loss: -0.580867] \n",
      "[Epoch 222/2200]  [Disc loss: -4.401647] [Gen loss: -0.322388] \n",
      "[Epoch 223/2200]  [Disc loss: -4.327722] [Gen loss: -0.525208] \n",
      "[Epoch 224/2200]  [Disc loss: -4.359265] [Gen loss: -0.477516] \n",
      "[Epoch 225/2200]  [Disc loss: -4.275883] [Gen loss: -0.376933] \n",
      "[Epoch 226/2200]  [Disc loss: -4.142551] [Gen loss: -0.528872] \n",
      "[Epoch 227/2200]  [Disc loss: -4.013832] [Gen loss: -0.705562] \n",
      "[Epoch 228/2200]  [Disc loss: -4.065412] [Gen loss: -0.498651] \n",
      "[Epoch 229/2200]  [Disc loss: -4.103538] [Gen loss: -0.789042] \n",
      "[Epoch 230/2200]  [Disc loss: -3.827679] [Gen loss: -0.977071] \n",
      "[Epoch 231/2200]  [Disc loss: -3.687377] [Gen loss: -0.978501] \n",
      "[Epoch 232/2200]  [Disc loss: -3.726333] [Gen loss: -0.736124] \n",
      "[Epoch 233/2200]  [Disc loss: -3.601880] [Gen loss: -0.676832] \n",
      "[Epoch 234/2200]  [Disc loss: -3.416700] [Gen loss: -0.608056] \n",
      "[Epoch 235/2200]  [Disc loss: -3.284731] [Gen loss: -0.547257] \n",
      "[Epoch 236/2200]  [Disc loss: -3.475786] [Gen loss: -0.218294] \n",
      "[Epoch 237/2200]  [Disc loss: -3.428564] [Gen loss: -0.159039] \n",
      "[Epoch 238/2200]  [Disc loss: -3.344426] [Gen loss: -0.210266] \n",
      "[Epoch 239/2200]  [Disc loss: -3.408441] [Gen loss: -0.631955] \n",
      "[Epoch 240/2200]  [Disc loss: -3.254121] [Gen loss: -0.227904] \n",
      "[Epoch 241/2200]  [Disc loss: -3.288319] [Gen loss: -0.363726] \n",
      "[Epoch 242/2200]  [Disc loss: -3.064795] [Gen loss: -0.278859] \n",
      "[Epoch 243/2200]  [Disc loss: -3.231022] [Gen loss: 0.073465] \n",
      "[Epoch 244/2200]  [Disc loss: -3.048724] [Gen loss: -0.034803] \n",
      "[Epoch 245/2200]  [Disc loss: -2.989093] [Gen loss: -0.051935] \n",
      "[Epoch 246/2200]  [Disc loss: -2.897839] [Gen loss: 0.017262] \n",
      "[Epoch 247/2200]  [Disc loss: -2.824837] [Gen loss: 0.476185] \n",
      "[Epoch 248/2200]  [Disc loss: -2.838191] [Gen loss: 0.204444] \n",
      "[Epoch 249/2200]  [Disc loss: -2.823248] [Gen loss: 0.098941] \n",
      "[Epoch 250/2200]  [Disc loss: -2.888714] [Gen loss: -0.204310] \n",
      "[Epoch 251/2200]  [Disc loss: -2.697994] [Gen loss: -0.490290] \n",
      "[Epoch 252/2200]  [Disc loss: -2.626655] [Gen loss: 0.060386] \n",
      "[Epoch 253/2200]  [Disc loss: -2.552255] [Gen loss: 0.363406] \n",
      "[Epoch 254/2200]  [Disc loss: -2.614437] [Gen loss: -0.044211] \n",
      "[Epoch 255/2200]  [Disc loss: -2.612020] [Gen loss: -0.467796] \n",
      "[Epoch 256/2200]  [Disc loss: -2.506001] [Gen loss: -0.170991] \n",
      "[Epoch 257/2200]  [Disc loss: -2.387641] [Gen loss: 0.134177] \n",
      "[Epoch 258/2200]  [Disc loss: -2.381699] [Gen loss: -0.369555] \n",
      "[Epoch 259/2200]  [Disc loss: -2.344848] [Gen loss: -0.248976] \n",
      "[Epoch 260/2200]  [Disc loss: -2.216041] [Gen loss: 0.052686] \n",
      "[Epoch 261/2200]  [Disc loss: -2.268747] [Gen loss: 0.003759] \n",
      "[Epoch 262/2200]  [Disc loss: -2.232794] [Gen loss: -0.112807] \n",
      "[Epoch 263/2200]  [Disc loss: -2.161149] [Gen loss: 0.230371] \n",
      "[Epoch 264/2200]  [Disc loss: -2.196621] [Gen loss: -0.312578] \n",
      "[Epoch 265/2200]  [Disc loss: -2.124990] [Gen loss: -0.368732] \n",
      "[Epoch 266/2200]  [Disc loss: -2.051842] [Gen loss: -0.434888] \n",
      "[Epoch 267/2200]  [Disc loss: -1.958783] [Gen loss: 0.033588] \n",
      "[Epoch 268/2200]  [Disc loss: -2.047412] [Gen loss: -0.200751] \n",
      "[Epoch 269/2200]  [Disc loss: -1.906165] [Gen loss: 0.064118] \n",
      "[Epoch 270/2200]  [Disc loss: -1.695775] [Gen loss: 0.278455] \n",
      "[Epoch 271/2200]  [Disc loss: -1.703290] [Gen loss: 0.158664] \n",
      "[Epoch 272/2200]  [Disc loss: -1.748724] [Gen loss: 0.336399] \n",
      "[Epoch 273/2200]  [Disc loss: -1.702869] [Gen loss: 0.317053] \n",
      "[Epoch 274/2200]  [Disc loss: -1.598797] [Gen loss: -0.290632] \n",
      "[Epoch 275/2200]  [Disc loss: -1.619249] [Gen loss: -0.134961] \n",
      "[Epoch 276/2200]  [Disc loss: -1.699750] [Gen loss: 0.538803] \n",
      "[Epoch 277/2200]  [Disc loss: -1.660018] [Gen loss: 0.217635] \n",
      "[Epoch 278/2200]  [Disc loss: -1.548535] [Gen loss: 0.287828] \n",
      "[Epoch 279/2200]  [Disc loss: -1.528018] [Gen loss: 0.139797] \n",
      "[Epoch 280/2200]  [Disc loss: -1.572802] [Gen loss: 0.144595] \n",
      "[Epoch 281/2200]  [Disc loss: -1.632586] [Gen loss: 0.299314] \n",
      "[Epoch 282/2200]  [Disc loss: -1.639195] [Gen loss: 0.179526] \n",
      "[Epoch 283/2200]  [Disc loss: -1.634763] [Gen loss: -0.228986] \n",
      "[Epoch 284/2200]  [Disc loss: -1.543812] [Gen loss: -0.172982] \n",
      "[Epoch 285/2200]  [Disc loss: -1.510948] [Gen loss: -0.545545] \n",
      "[Epoch 286/2200]  [Disc loss: -1.572387] [Gen loss: -0.179226] \n",
      "[Epoch 287/2200]  [Disc loss: -1.500626] [Gen loss: -0.100278] \n",
      "[Epoch 288/2200]  [Disc loss: -1.510902] [Gen loss: -0.334536] \n",
      "[Epoch 289/2200]  [Disc loss: -1.556245] [Gen loss: -0.637829] \n",
      "[Epoch 290/2200]  [Disc loss: -1.610458] [Gen loss: -0.519477] \n",
      "[Epoch 291/2200]  [Disc loss: -1.577901] [Gen loss: -0.487031] \n",
      "[Epoch 292/2200]  [Disc loss: -1.596725] [Gen loss: -1.013986] \n",
      "[Epoch 293/2200]  [Disc loss: -1.641175] [Gen loss: -1.234527] \n",
      "[Epoch 294/2200]  [Disc loss: -1.621856] [Gen loss: -1.311956] \n",
      "[Epoch 295/2200]  [Disc loss: -1.566868] [Gen loss: -1.618863] \n",
      "[Epoch 296/2200]  [Disc loss: -1.527681] [Gen loss: -1.757928] \n",
      "[Epoch 297/2200]  [Disc loss: -1.526100] [Gen loss: -1.957436] \n",
      "[Epoch 298/2200]  [Disc loss: -1.569552] [Gen loss: -2.007563] \n",
      "[Epoch 299/2200]  [Disc loss: -1.599865] [Gen loss: -2.211337] \n",
      "[Epoch 300/2200]  [Disc loss: -1.538469] [Gen loss: -2.472295] \n",
      "[Epoch 301/2200]  [Disc loss: -1.476363] [Gen loss: -2.805848] \n",
      "[Epoch 302/2200]  [Disc loss: -1.498502] [Gen loss: -3.045902] \n",
      "[Epoch 303/2200]  [Disc loss: -1.487155] [Gen loss: -3.046689] \n",
      "[Epoch 304/2200]  [Disc loss: -1.462696] [Gen loss: -3.140959] \n",
      "[Epoch 305/2200]  [Disc loss: -1.463551] [Gen loss: -3.508479] \n",
      "[Epoch 306/2200]  [Disc loss: -1.482211] [Gen loss: -3.550812] \n",
      "[Epoch 307/2200]  [Disc loss: -1.427858] [Gen loss: -3.746980] \n",
      "[Epoch 308/2200]  [Disc loss: -1.427208] [Gen loss: -3.851006] \n",
      "[Epoch 309/2200]  [Disc loss: -1.389615] [Gen loss: -4.008176] \n",
      "[Epoch 310/2200]  [Disc loss: -1.443482] [Gen loss: -4.124233] \n",
      "[Epoch 311/2200]  [Disc loss: -1.433730] [Gen loss: -4.232843] \n",
      "[Epoch 312/2200]  [Disc loss: -1.414273] [Gen loss: -4.304809] \n",
      "[Epoch 313/2200]  [Disc loss: -1.398877] [Gen loss: -4.395834] \n",
      "[Epoch 314/2200]  [Disc loss: -1.400152] [Gen loss: -4.186730] \n",
      "[Epoch 315/2200]  [Disc loss: -1.402877] [Gen loss: -4.389483] \n",
      "[Epoch 316/2200]  [Disc loss: -1.388761] [Gen loss: -4.600476] \n",
      "[Epoch 317/2200]  [Disc loss: -1.371659] [Gen loss: -4.944676] \n",
      "[Epoch 318/2200]  [Disc loss: -1.374512] [Gen loss: -4.905607] \n",
      "[Epoch 319/2200]  [Disc loss: -1.326311] [Gen loss: -5.161886] \n",
      "[Epoch 320/2200]  [Disc loss: -1.331116] [Gen loss: -5.374899] \n",
      "[Epoch 321/2200]  [Disc loss: -1.321555] [Gen loss: -5.535959] \n",
      "[Epoch 322/2200]  [Disc loss: -1.333576] [Gen loss: -5.615419] \n",
      "[Epoch 323/2200]  [Disc loss: -1.309019] [Gen loss: -5.712251] \n",
      "[Epoch 324/2200]  [Disc loss: -1.276223] [Gen loss: -5.760229] \n",
      "[Epoch 325/2200]  [Disc loss: -1.244083] [Gen loss: -5.749027] \n",
      "[Epoch 326/2200]  [Disc loss: -1.242864] [Gen loss: -6.000592] \n",
      "[Epoch 327/2200]  [Disc loss: -1.253654] [Gen loss: -5.897684] \n",
      "[Epoch 328/2200]  [Disc loss: -1.244052] [Gen loss: -6.068196] \n",
      "[Epoch 329/2200]  [Disc loss: -1.218153] [Gen loss: -6.291288] \n",
      "[Epoch 330/2200]  [Disc loss: -1.207480] [Gen loss: -5.885095] \n",
      "[Epoch 331/2200]  [Disc loss: -1.199009] [Gen loss: -6.623001] \n",
      "[Epoch 332/2200]  [Disc loss: -1.170244] [Gen loss: -6.353455] \n",
      "[Epoch 333/2200]  [Disc loss: -1.161610] [Gen loss: -6.448466] \n",
      "[Epoch 334/2200]  [Disc loss: -1.134688] [Gen loss: -6.385810] \n",
      "[Epoch 335/2200]  [Disc loss: -1.139256] [Gen loss: -6.534909] \n",
      "[Epoch 336/2200]  [Disc loss: -1.133353] [Gen loss: -6.676728] \n",
      "[Epoch 337/2200]  [Disc loss: -1.127096] [Gen loss: -6.176657] \n",
      "[Epoch 338/2200]  [Disc loss: -1.120697] [Gen loss: -6.365463] \n",
      "[Epoch 339/2200]  [Disc loss: -1.114133] [Gen loss: -6.584645] \n",
      "[Epoch 340/2200]  [Disc loss: -1.102858] [Gen loss: -6.529817] \n",
      "[Epoch 341/2200]  [Disc loss: -1.090578] [Gen loss: -6.420847] \n",
      "[Epoch 342/2200]  [Disc loss: -1.082142] [Gen loss: -6.621168] \n",
      "[Epoch 343/2200]  [Disc loss: -1.071195] [Gen loss: -6.779552] \n",
      "[Epoch 344/2200]  [Disc loss: -1.075729] [Gen loss: -6.583139] \n",
      "[Epoch 345/2200]  [Disc loss: -1.046688] [Gen loss: -6.431948] \n",
      "[Epoch 346/2200]  [Disc loss: -1.056060] [Gen loss: -6.204725] \n",
      "[Epoch 347/2200]  [Disc loss: -1.054220] [Gen loss: -6.619718] \n",
      "[Epoch 348/2200]  [Disc loss: -1.052610] [Gen loss: -6.612596] \n",
      "[Epoch 349/2200]  [Disc loss: -1.028079] [Gen loss: -6.646823] \n",
      "[Epoch 350/2200]  [Disc loss: -1.015639] [Gen loss: -6.132437] \n",
      "[Epoch 351/2200]  [Disc loss: -1.021648] [Gen loss: -6.472796] \n",
      "[Epoch 352/2200]  [Disc loss: -1.006046] [Gen loss: -6.181208] \n",
      "[Epoch 353/2200]  [Disc loss: -1.001452] [Gen loss: -6.179534] \n",
      "[Epoch 354/2200]  [Disc loss: -0.974348] [Gen loss: -6.026219] \n",
      "[Epoch 355/2200]  [Disc loss: -0.948656] [Gen loss: -5.966831] \n",
      "[Epoch 356/2200]  [Disc loss: -0.986849] [Gen loss: -6.051103] \n",
      "[Epoch 357/2200]  [Disc loss: -0.960681] [Gen loss: -5.893930] \n",
      "[Epoch 358/2200]  [Disc loss: -0.936024] [Gen loss: -5.663980] \n",
      "[Epoch 359/2200]  [Disc loss: -0.925588] [Gen loss: -5.945272] \n",
      "[Epoch 360/2200]  [Disc loss: -0.932747] [Gen loss: -5.815886] \n",
      "[Epoch 361/2200]  [Disc loss: -0.903484] [Gen loss: -5.682419] \n",
      "[Epoch 362/2200]  [Disc loss: -0.926948] [Gen loss: -5.617712] \n",
      "[Epoch 363/2200]  [Disc loss: -0.922552] [Gen loss: -5.208666] \n",
      "[Epoch 364/2200]  [Disc loss: -0.885506] [Gen loss: -5.200359] \n",
      "[Epoch 365/2200]  [Disc loss: -0.882590] [Gen loss: -5.501667] \n",
      "[Epoch 366/2200]  [Disc loss: -0.904069] [Gen loss: -5.175295] \n",
      "[Epoch 367/2200]  [Disc loss: -0.885667] [Gen loss: -5.784217] \n",
      "[Epoch 368/2200]  [Disc loss: -0.902838] [Gen loss: -5.219991] \n",
      "[Epoch 369/2200]  [Disc loss: -0.884048] [Gen loss: -5.222029] \n",
      "[Epoch 370/2200]  [Disc loss: -0.899539] [Gen loss: -5.404914] \n",
      "[Epoch 371/2200]  [Disc loss: -0.827707] [Gen loss: -4.910993] \n",
      "[Epoch 372/2200]  [Disc loss: -0.857027] [Gen loss: -6.113506] \n",
      "[Epoch 373/2200]  [Disc loss: -0.851227] [Gen loss: -5.362814] \n",
      "[Epoch 374/2200]  [Disc loss: -0.847581] [Gen loss: -5.172914] \n",
      "[Epoch 375/2200]  [Disc loss: -0.868955] [Gen loss: -4.901954] \n",
      "[Epoch 376/2200]  [Disc loss: -0.855447] [Gen loss: -5.081998] \n",
      "[Epoch 377/2200]  [Disc loss: -0.853109] [Gen loss: -5.092548] \n",
      "[Epoch 378/2200]  [Disc loss: -0.846290] [Gen loss: -4.340775] \n",
      "[Epoch 379/2200]  [Disc loss: -0.864283] [Gen loss: -4.901682] \n",
      "[Epoch 380/2200]  [Disc loss: -0.860536] [Gen loss: -4.846464] \n",
      "[Epoch 381/2200]  [Disc loss: -0.851426] [Gen loss: -5.065937] \n",
      "[Epoch 382/2200]  [Disc loss: -0.837605] [Gen loss: -5.180237] \n",
      "[Epoch 383/2200]  [Disc loss: -0.840075] [Gen loss: -5.124348] \n",
      "[Epoch 384/2200]  [Disc loss: -0.858548] [Gen loss: -4.548618] \n",
      "[Epoch 385/2200]  [Disc loss: -0.860227] [Gen loss: -4.829281] \n",
      "[Epoch 386/2200]  [Disc loss: -0.845436] [Gen loss: -4.809653] \n",
      "[Epoch 387/2200]  [Disc loss: -0.850546] [Gen loss: -4.982522] \n",
      "[Epoch 388/2200]  [Disc loss: -0.845740] [Gen loss: -4.226796] \n",
      "[Epoch 389/2200]  [Disc loss: -0.858790] [Gen loss: -4.940168] \n",
      "[Epoch 390/2200]  [Disc loss: -0.850918] [Gen loss: -4.864144] \n",
      "[Epoch 391/2200]  [Disc loss: -0.864680] [Gen loss: -4.776400] \n",
      "[Epoch 392/2200]  [Disc loss: -0.855264] [Gen loss: -4.641455] \n",
      "[Epoch 393/2200]  [Disc loss: -0.838234] [Gen loss: -4.339564] \n",
      "[Epoch 394/2200]  [Disc loss: -0.882653] [Gen loss: -4.774176] \n",
      "[Epoch 395/2200]  [Disc loss: -0.848174] [Gen loss: -4.608309] \n",
      "[Epoch 396/2200]  [Disc loss: -0.879556] [Gen loss: -5.245739] \n",
      "[Epoch 397/2200]  [Disc loss: -0.858745] [Gen loss: -4.580600] \n",
      "[Epoch 398/2200]  [Disc loss: -0.868481] [Gen loss: -5.173626] \n",
      "[Epoch 399/2200]  [Disc loss: -0.864514] [Gen loss: -4.991089] \n",
      "[Epoch 400/2200]  [Disc loss: -0.893628] [Gen loss: -5.115938] \n",
      "[Epoch 401/2200]  [Disc loss: -0.871842] [Gen loss: -5.267660] \n",
      "[Epoch 402/2200]  [Disc loss: -0.885852] [Gen loss: -4.927482] \n",
      "[Epoch 403/2200]  [Disc loss: -0.887116] [Gen loss: -4.995295] \n",
      "[Epoch 404/2200]  [Disc loss: -0.895628] [Gen loss: -5.142259] \n",
      "[Epoch 405/2200]  [Disc loss: -0.865002] [Gen loss: -4.920329] \n",
      "[Epoch 406/2200]  [Disc loss: -0.858923] [Gen loss: -5.408206] \n",
      "[Epoch 407/2200]  [Disc loss: -0.890305] [Gen loss: -4.874144] \n",
      "[Epoch 408/2200]  [Disc loss: -0.875299] [Gen loss: -5.444512] \n",
      "[Epoch 409/2200]  [Disc loss: -0.866959] [Gen loss: -5.671825] \n",
      "[Epoch 410/2200]  [Disc loss: -0.890437] [Gen loss: -5.100810] \n",
      "[Epoch 411/2200]  [Disc loss: -0.893632] [Gen loss: -5.624760] \n",
      "[Epoch 412/2200]  [Disc loss: -0.877704] [Gen loss: -5.290623] \n",
      "[Epoch 413/2200]  [Disc loss: -0.905893] [Gen loss: -5.180910] \n",
      "[Epoch 414/2200]  [Disc loss: -0.904246] [Gen loss: -5.777655] \n",
      "[Epoch 415/2200]  [Disc loss: -0.874558] [Gen loss: -5.245349] \n",
      "[Epoch 416/2200]  [Disc loss: -0.871488] [Gen loss: -5.339161] \n",
      "[Epoch 417/2200]  [Disc loss: -0.875759] [Gen loss: -5.658796] \n",
      "[Epoch 418/2200]  [Disc loss: -0.895133] [Gen loss: -5.754110] \n",
      "[Epoch 419/2200]  [Disc loss: -0.893454] [Gen loss: -5.832302] \n",
      "[Epoch 420/2200]  [Disc loss: -0.886002] [Gen loss: -5.570603] \n",
      "[Epoch 421/2200]  [Disc loss: -0.899904] [Gen loss: -5.911430] \n",
      "[Epoch 422/2200]  [Disc loss: -0.897503] [Gen loss: -6.058200] \n",
      "[Epoch 423/2200]  [Disc loss: -0.895665] [Gen loss: -5.905733] \n",
      "[Epoch 424/2200]  [Disc loss: -0.915505] [Gen loss: -5.903923] \n",
      "[Epoch 425/2200]  [Disc loss: -0.882154] [Gen loss: -5.609540] \n",
      "[Epoch 426/2200]  [Disc loss: -0.887623] [Gen loss: -6.097473] \n",
      "[Epoch 427/2200]  [Disc loss: -0.911499] [Gen loss: -5.915137] \n",
      "[Epoch 428/2200]  [Disc loss: -0.898332] [Gen loss: -5.938374] \n",
      "[Epoch 429/2200]  [Disc loss: -0.904138] [Gen loss: -6.085936] \n",
      "[Epoch 430/2200]  [Disc loss: -0.883224] [Gen loss: -6.036183] \n",
      "[Epoch 431/2200]  [Disc loss: -0.899855] [Gen loss: -6.024168] \n",
      "[Epoch 432/2200]  [Disc loss: -0.890290] [Gen loss: -5.949632] \n",
      "[Epoch 433/2200]  [Disc loss: -0.902456] [Gen loss: -5.971854] \n",
      "[Epoch 434/2200]  [Disc loss: -0.897732] [Gen loss: -5.723347] \n",
      "[Epoch 435/2200]  [Disc loss: -0.892780] [Gen loss: -6.055366] \n",
      "[Epoch 436/2200]  [Disc loss: -0.891057] [Gen loss: -5.994202] \n",
      "[Epoch 437/2200]  [Disc loss: -0.903551] [Gen loss: -5.886051] \n",
      "[Epoch 438/2200]  [Disc loss: -0.888241] [Gen loss: -6.159058] \n",
      "[Epoch 439/2200]  [Disc loss: -0.896597] [Gen loss: -5.960883] \n",
      "[Epoch 440/2200]  [Disc loss: -0.899091] [Gen loss: -5.910816] \n",
      "[Epoch 441/2200]  [Disc loss: -0.872242] [Gen loss: -5.894016] \n",
      "[Epoch 442/2200]  [Disc loss: -0.888528] [Gen loss: -5.993793] \n",
      "[Epoch 443/2200]  [Disc loss: -0.892491] [Gen loss: -6.130906] \n",
      "[Epoch 444/2200]  [Disc loss: -0.894805] [Gen loss: -6.479062] \n",
      "[Epoch 445/2200]  [Disc loss: -0.888665] [Gen loss: -6.218759] \n",
      "[Epoch 446/2200]  [Disc loss: -0.880009] [Gen loss: -5.818519] \n",
      "[Epoch 447/2200]  [Disc loss: -0.883940] [Gen loss: -6.388978] \n",
      "[Epoch 448/2200]  [Disc loss: -0.881213] [Gen loss: -6.303128] \n",
      "[Epoch 449/2200]  [Disc loss: -0.880134] [Gen loss: -6.221680] \n",
      "[Epoch 450/2200]  [Disc loss: -0.868439] [Gen loss: -6.641235] \n",
      "[Epoch 451/2200]  [Disc loss: -0.887030] [Gen loss: -6.426869] \n",
      "[Epoch 452/2200]  [Disc loss: -0.868076] [Gen loss: -6.160416] \n",
      "[Epoch 453/2200]  [Disc loss: -0.876873] [Gen loss: -6.269498] \n",
      "[Epoch 454/2200]  [Disc loss: -0.866494] [Gen loss: -6.469564] \n",
      "[Epoch 455/2200]  [Disc loss: -0.872975] [Gen loss: -6.583209] \n",
      "[Epoch 456/2200]  [Disc loss: -0.875024] [Gen loss: -6.404130] \n",
      "[Epoch 457/2200]  [Disc loss: -0.870660] [Gen loss: -6.576680] \n",
      "[Epoch 458/2200]  [Disc loss: -0.857873] [Gen loss: -6.478085] \n",
      "[Epoch 459/2200]  [Disc loss: -0.851933] [Gen loss: -6.530990] \n",
      "[Epoch 460/2200]  [Disc loss: -0.870352] [Gen loss: -6.683427] \n",
      "[Epoch 461/2200]  [Disc loss: -0.864508] [Gen loss: -6.498440] \n",
      "[Epoch 462/2200]  [Disc loss: -0.857415] [Gen loss: -6.741230] \n",
      "[Epoch 463/2200]  [Disc loss: -0.854880] [Gen loss: -6.352811] \n",
      "[Epoch 464/2200]  [Disc loss: -0.851897] [Gen loss: -6.519185] \n",
      "[Epoch 465/2200]  [Disc loss: -0.847709] [Gen loss: -6.859356] \n",
      "[Epoch 466/2200]  [Disc loss: -0.855534] [Gen loss: -7.029489] \n",
      "[Epoch 467/2200]  [Disc loss: -0.839545] [Gen loss: -6.701718] \n",
      "[Epoch 468/2200]  [Disc loss: -0.848619] [Gen loss: -6.837277] \n",
      "[Epoch 469/2200]  [Disc loss: -0.850105] [Gen loss: -6.650199] \n",
      "[Epoch 470/2200]  [Disc loss: -0.843632] [Gen loss: -6.700645] \n",
      "[Epoch 471/2200]  [Disc loss: -0.855355] [Gen loss: -6.970561] \n",
      "[Epoch 472/2200]  [Disc loss: -0.843182] [Gen loss: -6.571633] \n",
      "[Epoch 473/2200]  [Disc loss: -0.832834] [Gen loss: -6.774298] \n",
      "[Epoch 474/2200]  [Disc loss: -0.849790] [Gen loss: -6.916550] \n",
      "[Epoch 475/2200]  [Disc loss: -0.842767] [Gen loss: -6.744171] \n",
      "[Epoch 476/2200]  [Disc loss: -0.845963] [Gen loss: -6.778042] \n",
      "[Epoch 477/2200]  [Disc loss: -0.839892] [Gen loss: -6.724039] \n",
      "[Epoch 478/2200]  [Disc loss: -0.834761] [Gen loss: -6.932236] \n",
      "[Epoch 479/2200]  [Disc loss: -0.839690] [Gen loss: -6.541193] \n",
      "[Epoch 480/2200]  [Disc loss: -0.838472] [Gen loss: -7.213259] \n",
      "[Epoch 481/2200]  [Disc loss: -0.840539] [Gen loss: -6.787565] \n",
      "[Epoch 482/2200]  [Disc loss: -0.834317] [Gen loss: -6.700853] \n",
      "[Epoch 483/2200]  [Disc loss: -0.835265] [Gen loss: -6.905849] \n",
      "[Epoch 484/2200]  [Disc loss: -0.841138] [Gen loss: -6.846971] \n",
      "[Epoch 485/2200]  [Disc loss: -0.831230] [Gen loss: -6.813761] \n",
      "[Epoch 486/2200]  [Disc loss: -0.819434] [Gen loss: -6.803041] \n",
      "[Epoch 487/2200]  [Disc loss: -0.830835] [Gen loss: -6.607029] \n",
      "[Epoch 488/2200]  [Disc loss: -0.840793] [Gen loss: -6.942728] \n",
      "[Epoch 489/2200]  [Disc loss: -0.820276] [Gen loss: -6.902906] \n",
      "[Epoch 490/2200]  [Disc loss: -0.827033] [Gen loss: -6.683030] \n",
      "[Epoch 491/2200]  [Disc loss: -0.828580] [Gen loss: -6.727464] \n",
      "[Epoch 492/2200]  [Disc loss: -0.818074] [Gen loss: -6.840336] \n",
      "[Epoch 493/2200]  [Disc loss: -0.817962] [Gen loss: -6.606933] \n",
      "[Epoch 494/2200]  [Disc loss: -0.827423] [Gen loss: -6.505163] \n",
      "[Epoch 495/2200]  [Disc loss: -0.814009] [Gen loss: -6.673371] \n",
      "[Epoch 496/2200]  [Disc loss: -0.824900] [Gen loss: -6.790767] \n",
      "[Epoch 497/2200]  [Disc loss: -0.824695] [Gen loss: -6.641900] \n",
      "[Epoch 498/2200]  [Disc loss: -0.804568] [Gen loss: -6.642980] \n",
      "[Epoch 499/2200]  [Disc loss: -0.815079] [Gen loss: -6.695965] \n",
      "[Epoch 500/2200]  [Disc loss: -0.812353] [Gen loss: -6.913031] \n",
      "[Epoch 501/2200]  [Disc loss: -0.807064] [Gen loss: -6.699804] \n",
      "[Epoch 502/2200]  [Disc loss: -0.818719] [Gen loss: -6.786720] \n",
      "[Epoch 503/2200]  [Disc loss: -0.811303] [Gen loss: -6.790214] \n",
      "[Epoch 504/2200]  [Disc loss: -0.808335] [Gen loss: -6.646380] \n",
      "[Epoch 505/2200]  [Disc loss: -0.807353] [Gen loss: -6.490721] \n",
      "[Epoch 506/2200]  [Disc loss: -0.808550] [Gen loss: -6.446241] \n",
      "[Epoch 507/2200]  [Disc loss: -0.805162] [Gen loss: -6.326350] \n",
      "[Epoch 508/2200]  [Disc loss: -0.804283] [Gen loss: -6.605924] \n",
      "[Epoch 509/2200]  [Disc loss: -0.797206] [Gen loss: -6.571170] \n",
      "[Epoch 510/2200]  [Disc loss: -0.804405] [Gen loss: -6.550117] \n",
      "[Epoch 511/2200]  [Disc loss: -0.793551] [Gen loss: -6.522540] \n",
      "[Epoch 512/2200]  [Disc loss: -0.794694] [Gen loss: -6.418210] \n",
      "[Epoch 513/2200]  [Disc loss: -0.804061] [Gen loss: -6.277861] \n",
      "[Epoch 514/2200]  [Disc loss: -0.796927] [Gen loss: -6.712312] \n",
      "[Epoch 515/2200]  [Disc loss: -0.795700] [Gen loss: -6.204972] \n",
      "[Epoch 516/2200]  [Disc loss: -0.801452] [Gen loss: -6.511290] \n",
      "[Epoch 517/2200]  [Disc loss: -0.792109] [Gen loss: -6.528406] \n",
      "[Epoch 518/2200]  [Disc loss: -0.787213] [Gen loss: -6.228870] \n",
      "[Epoch 519/2200]  [Disc loss: -0.795926] [Gen loss: -6.339276] \n",
      "[Epoch 520/2200]  [Disc loss: -0.793623] [Gen loss: -6.422624] \n",
      "[Epoch 521/2200]  [Disc loss: -0.782687] [Gen loss: -6.325009] \n",
      "[Epoch 522/2200]  [Disc loss: -0.786142] [Gen loss: -6.494127] \n",
      "[Epoch 523/2200]  [Disc loss: -0.789900] [Gen loss: -6.445646] \n",
      "[Epoch 524/2200]  [Disc loss: -0.784531] [Gen loss: -6.111361] \n",
      "[Epoch 525/2200]  [Disc loss: -0.781612] [Gen loss: -6.519699] \n",
      "[Epoch 526/2200]  [Disc loss: -0.783258] [Gen loss: -6.299625] \n",
      "[Epoch 527/2200]  [Disc loss: -0.782413] [Gen loss: -6.200252] \n",
      "[Epoch 528/2200]  [Disc loss: -0.779847] [Gen loss: -6.049929] \n",
      "[Epoch 529/2200]  [Disc loss: -0.779862] [Gen loss: -6.422221] \n",
      "[Epoch 530/2200]  [Disc loss: -0.772603] [Gen loss: -6.164477] \n",
      "[Epoch 531/2200]  [Disc loss: -0.776903] [Gen loss: -6.322818] \n",
      "[Epoch 532/2200]  [Disc loss: -0.780875] [Gen loss: -6.053968] \n",
      "[Epoch 533/2200]  [Disc loss: -0.778216] [Gen loss: -5.893704] \n",
      "[Epoch 534/2200]  [Disc loss: -0.775040] [Gen loss: -6.174174] \n",
      "[Epoch 535/2200]  [Disc loss: -0.775256] [Gen loss: -6.231442] \n",
      "[Epoch 536/2200]  [Disc loss: -0.768882] [Gen loss: -5.917171] \n",
      "[Epoch 537/2200]  [Disc loss: -0.776074] [Gen loss: -6.074671] \n",
      "[Epoch 538/2200]  [Disc loss: -0.773411] [Gen loss: -5.908661] \n",
      "[Epoch 539/2200]  [Disc loss: -0.766004] [Gen loss: -5.837196] \n",
      "[Epoch 540/2200]  [Disc loss: -0.774379] [Gen loss: -5.851483] \n",
      "[Epoch 541/2200]  [Disc loss: -0.769809] [Gen loss: -6.075953] \n",
      "[Epoch 542/2200]  [Disc loss: -0.764183] [Gen loss: -6.049675] \n",
      "[Epoch 543/2200]  [Disc loss: -0.766441] [Gen loss: -5.728900] \n",
      "[Epoch 544/2200]  [Disc loss: -0.761194] [Gen loss: -5.902209] \n",
      "[Epoch 545/2200]  [Disc loss: -0.764730] [Gen loss: -5.662648] \n",
      "[Epoch 546/2200]  [Disc loss: -0.766714] [Gen loss: -5.827016] \n",
      "[Epoch 547/2200]  [Disc loss: -0.772833] [Gen loss: -5.692500] \n",
      "[Epoch 548/2200]  [Disc loss: -0.758407] [Gen loss: -5.626881] \n",
      "[Epoch 549/2200]  [Disc loss: -0.765197] [Gen loss: -5.447930] \n",
      "[Epoch 550/2200]  [Disc loss: -0.763041] [Gen loss: -5.390104] \n",
      "[Epoch 551/2200]  [Disc loss: -0.760273] [Gen loss: -5.605671] \n",
      "[Epoch 552/2200]  [Disc loss: -0.764870] [Gen loss: -5.374916] \n",
      "[Epoch 553/2200]  [Disc loss: -0.762752] [Gen loss: -5.208270] \n",
      "[Epoch 554/2200]  [Disc loss: -0.759483] [Gen loss: -5.205347] \n",
      "[Epoch 555/2200]  [Disc loss: -0.761573] [Gen loss: -5.330522] \n",
      "[Epoch 556/2200]  [Disc loss: -0.755680] [Gen loss: -5.195420] \n",
      "[Epoch 557/2200]  [Disc loss: -0.758505] [Gen loss: -5.123785] \n",
      "[Epoch 558/2200]  [Disc loss: -0.764335] [Gen loss: -5.004283] \n",
      "[Epoch 559/2200]  [Disc loss: -0.761367] [Gen loss: -5.048477] \n",
      "[Epoch 560/2200]  [Disc loss: -0.763078] [Gen loss: -5.153479] \n",
      "[Epoch 561/2200]  [Disc loss: -0.755014] [Gen loss: -4.742919] \n",
      "[Epoch 562/2200]  [Disc loss: -0.757683] [Gen loss: -5.000678] \n",
      "[Epoch 563/2200]  [Disc loss: -0.758253] [Gen loss: -4.961355] \n",
      "[Epoch 564/2200]  [Disc loss: -0.757482] [Gen loss: -4.774759] \n",
      "[Epoch 565/2200]  [Disc loss: -0.755759] [Gen loss: -4.885235] \n",
      "[Epoch 566/2200]  [Disc loss: -0.750864] [Gen loss: -4.923600] \n",
      "[Epoch 567/2200]  [Disc loss: -0.754615] [Gen loss: -4.823207] \n",
      "[Epoch 568/2200]  [Disc loss: -0.748529] [Gen loss: -4.709320] \n",
      "[Epoch 569/2200]  [Disc loss: -0.753048] [Gen loss: -4.796772] \n",
      "[Epoch 570/2200]  [Disc loss: -0.754059] [Gen loss: -4.705105] \n",
      "[Epoch 571/2200]  [Disc loss: -0.749379] [Gen loss: -4.697263] \n",
      "[Epoch 572/2200]  [Disc loss: -0.752926] [Gen loss: -4.812952] \n",
      "[Epoch 573/2200]  [Disc loss: -0.742699] [Gen loss: -4.626870] \n",
      "[Epoch 574/2200]  [Disc loss: -0.758499] [Gen loss: -4.659970] \n",
      "[Epoch 575/2200]  [Disc loss: -0.752215] [Gen loss: -4.660224] \n",
      "[Epoch 576/2200]  [Disc loss: -0.749317] [Gen loss: -4.775862] \n",
      "[Epoch 577/2200]  [Disc loss: -0.744098] [Gen loss: -4.592557] \n",
      "[Epoch 578/2200]  [Disc loss: -0.751059] [Gen loss: -4.594971] \n",
      "[Epoch 579/2200]  [Disc loss: -0.740755] [Gen loss: -4.379546] \n",
      "[Epoch 580/2200]  [Disc loss: -0.743814] [Gen loss: -4.393404] \n",
      "[Epoch 581/2200]  [Disc loss: -0.743844] [Gen loss: -4.380514] \n",
      "[Epoch 582/2200]  [Disc loss: -0.743713] [Gen loss: -4.220655] \n",
      "[Epoch 583/2200]  [Disc loss: -0.739977] [Gen loss: -4.297634] \n",
      "[Epoch 584/2200]  [Disc loss: -0.744983] [Gen loss: -4.369061] \n",
      "[Epoch 585/2200]  [Disc loss: -0.739549] [Gen loss: -4.465563] \n",
      "[Epoch 586/2200]  [Disc loss: -0.740287] [Gen loss: -4.418539] \n",
      "[Epoch 587/2200]  [Disc loss: -0.741270] [Gen loss: -4.285123] \n",
      "[Epoch 588/2200]  [Disc loss: -0.743723] [Gen loss: -4.351115] \n",
      "[Epoch 589/2200]  [Disc loss: -0.738888] [Gen loss: -4.285753] \n",
      "[Epoch 590/2200]  [Disc loss: -0.734500] [Gen loss: -4.381762] \n",
      "[Epoch 591/2200]  [Disc loss: -0.737486] [Gen loss: -4.222180] \n",
      "[Epoch 592/2200]  [Disc loss: -0.739328] [Gen loss: -4.184220] \n",
      "[Epoch 593/2200]  [Disc loss: -0.743895] [Gen loss: -3.904654] \n",
      "[Epoch 594/2200]  [Disc loss: -0.738790] [Gen loss: -4.106467] \n",
      "[Epoch 595/2200]  [Disc loss: -0.733134] [Gen loss: -3.969227] \n",
      "[Epoch 596/2200]  [Disc loss: -0.732254] [Gen loss: -4.101250] \n",
      "[Epoch 597/2200]  [Disc loss: -0.733488] [Gen loss: -3.893102] \n",
      "[Epoch 598/2200]  [Disc loss: -0.732830] [Gen loss: -3.873172] \n",
      "[Epoch 599/2200]  [Disc loss: -0.738338] [Gen loss: -4.188364] \n",
      "[Epoch 600/2200]  [Disc loss: -0.736357] [Gen loss: -4.029026] \n",
      "[Epoch 601/2200]  [Disc loss: -0.736483] [Gen loss: -4.033127] \n",
      "[Epoch 602/2200]  [Disc loss: -0.728773] [Gen loss: -4.077773] \n",
      "[Epoch 603/2200]  [Disc loss: -0.734132] [Gen loss: -4.052666] \n",
      "[Epoch 604/2200]  [Disc loss: -0.727201] [Gen loss: -3.849561] \n",
      "[Epoch 605/2200]  [Disc loss: -0.735773] [Gen loss: -3.911885] \n",
      "[Epoch 606/2200]  [Disc loss: -0.731643] [Gen loss: -3.939021] \n",
      "[Epoch 607/2200]  [Disc loss: -0.730813] [Gen loss: -3.823599] \n",
      "[Epoch 608/2200]  [Disc loss: -0.730486] [Gen loss: -3.781115] \n",
      "[Epoch 609/2200]  [Disc loss: -0.731334] [Gen loss: -3.802995] \n",
      "[Epoch 610/2200]  [Disc loss: -0.727273] [Gen loss: -3.754918] \n",
      "[Epoch 611/2200]  [Disc loss: -0.731514] [Gen loss: -3.674468] \n",
      "[Epoch 612/2200]  [Disc loss: -0.730109] [Gen loss: -3.528369] \n",
      "[Epoch 613/2200]  [Disc loss: -0.726888] [Gen loss: -3.530321] \n",
      "[Epoch 614/2200]  [Disc loss: -0.721704] [Gen loss: -3.671965] \n",
      "[Epoch 615/2200]  [Disc loss: -0.725078] [Gen loss: -3.436481] \n",
      "[Epoch 616/2200]  [Disc loss: -0.721094] [Gen loss: -3.430348] \n",
      "[Epoch 617/2200]  [Disc loss: -0.726369] [Gen loss: -3.375364] \n",
      "[Epoch 618/2200]  [Disc loss: -0.722032] [Gen loss: -3.486519] \n",
      "[Epoch 619/2200]  [Disc loss: -0.722210] [Gen loss: -3.588580] \n",
      "[Epoch 620/2200]  [Disc loss: -0.722935] [Gen loss: -3.470113] \n",
      "[Epoch 621/2200]  [Disc loss: -0.718573] [Gen loss: -3.297699] \n",
      "[Epoch 622/2200]  [Disc loss: -0.719165] [Gen loss: -3.261052] \n",
      "[Epoch 623/2200]  [Disc loss: -0.722774] [Gen loss: -3.571563] \n",
      "[Epoch 624/2200]  [Disc loss: -0.718805] [Gen loss: -3.063558] \n",
      "[Epoch 625/2200]  [Disc loss: -0.721207] [Gen loss: -3.543585] \n",
      "[Epoch 626/2200]  [Disc loss: -0.724147] [Gen loss: -3.319354] \n",
      "[Epoch 627/2200]  [Disc loss: -0.717802] [Gen loss: -3.298143] \n",
      "[Epoch 628/2200]  [Disc loss: -0.721446] [Gen loss: -3.281162] \n",
      "[Epoch 629/2200]  [Disc loss: -0.713675] [Gen loss: -3.323421] \n",
      "[Epoch 630/2200]  [Disc loss: -0.716445] [Gen loss: -3.033241] \n",
      "[Epoch 631/2200]  [Disc loss: -0.715259] [Gen loss: -3.020077] \n",
      "[Epoch 632/2200]  [Disc loss: -0.708777] [Gen loss: -2.944450] \n",
      "[Epoch 633/2200]  [Disc loss: -0.713995] [Gen loss: -3.046262] \n",
      "[Epoch 634/2200]  [Disc loss: -0.723008] [Gen loss: -2.832639] \n",
      "[Epoch 635/2200]  [Disc loss: -0.716331] [Gen loss: -3.022614] \n",
      "[Epoch 636/2200]  [Disc loss: -0.709749] [Gen loss: -3.135107] \n",
      "[Epoch 637/2200]  [Disc loss: -0.716131] [Gen loss: -2.991799] \n",
      "[Epoch 638/2200]  [Disc loss: -0.718239] [Gen loss: -2.883036] \n",
      "[Epoch 639/2200]  [Disc loss: -0.709109] [Gen loss: -2.888504] \n",
      "[Epoch 640/2200]  [Disc loss: -0.714140] [Gen loss: -3.003252] \n",
      "[Epoch 641/2200]  [Disc loss: -0.714929] [Gen loss: -2.869681] \n",
      "[Epoch 642/2200]  [Disc loss: -0.703514] [Gen loss: -2.787064] \n",
      "[Epoch 643/2200]  [Disc loss: -0.706849] [Gen loss: -2.648399] \n",
      "[Epoch 644/2200]  [Disc loss: -0.706240] [Gen loss: -2.936077] \n",
      "[Epoch 645/2200]  [Disc loss: -0.713934] [Gen loss: -2.847218] \n",
      "[Epoch 646/2200]  [Disc loss: -0.699171] [Gen loss: -2.779001] \n",
      "[Epoch 647/2200]  [Disc loss: -0.707445] [Gen loss: -2.638935] \n",
      "[Epoch 648/2200]  [Disc loss: -0.705184] [Gen loss: -2.703445] \n",
      "[Epoch 649/2200]  [Disc loss: -0.701030] [Gen loss: -2.624741] \n",
      "[Epoch 650/2200]  [Disc loss: -0.705686] [Gen loss: -2.772250] \n",
      "[Epoch 651/2200]  [Disc loss: -0.700719] [Gen loss: -2.704216] \n",
      "[Epoch 652/2200]  [Disc loss: -0.704340] [Gen loss: -2.504054] \n",
      "[Epoch 653/2200]  [Disc loss: -0.703867] [Gen loss: -2.463139] \n",
      "[Epoch 654/2200]  [Disc loss: -0.698566] [Gen loss: -2.564754] \n",
      "[Epoch 655/2200]  [Disc loss: -0.711234] [Gen loss: -2.615919] \n",
      "[Epoch 656/2200]  [Disc loss: -0.696451] [Gen loss: -2.506240] \n",
      "[Epoch 657/2200]  [Disc loss: -0.703837] [Gen loss: -2.494984] \n",
      "[Epoch 658/2200]  [Disc loss: -0.699953] [Gen loss: -2.386376] \n",
      "[Epoch 659/2200]  [Disc loss: -0.701727] [Gen loss: -2.448729] \n",
      "[Epoch 660/2200]  [Disc loss: -0.701570] [Gen loss: -2.424444] \n",
      "[Epoch 661/2200]  [Disc loss: -0.695397] [Gen loss: -2.482147] \n",
      "[Epoch 662/2200]  [Disc loss: -0.693473] [Gen loss: -2.410966] \n",
      "[Epoch 663/2200]  [Disc loss: -0.697693] [Gen loss: -2.511143] \n",
      "[Epoch 664/2200]  [Disc loss: -0.691776] [Gen loss: -2.312850] \n",
      "[Epoch 665/2200]  [Disc loss: -0.697984] [Gen loss: -2.437572] \n",
      "[Epoch 666/2200]  [Disc loss: -0.699517] [Gen loss: -2.262380] \n",
      "[Epoch 667/2200]  [Disc loss: -0.694380] [Gen loss: -2.318522] \n",
      "[Epoch 668/2200]  [Disc loss: -0.698296] [Gen loss: -2.185503] \n",
      "[Epoch 669/2200]  [Disc loss: -0.695403] [Gen loss: -2.310046] \n",
      "[Epoch 670/2200]  [Disc loss: -0.690411] [Gen loss: -2.254725] \n",
      "[Epoch 671/2200]  [Disc loss: -0.688161] [Gen loss: -2.473039] \n",
      "[Epoch 672/2200]  [Disc loss: -0.690200] [Gen loss: -2.405656] \n",
      "[Epoch 673/2200]  [Disc loss: -0.696074] [Gen loss: -2.361850] \n",
      "[Epoch 674/2200]  [Disc loss: -0.693495] [Gen loss: -2.206579] \n",
      "[Epoch 675/2200]  [Disc loss: -0.687639] [Gen loss: -2.360242] \n",
      "[Epoch 676/2200]  [Disc loss: -0.691796] [Gen loss: -2.332782] \n",
      "[Epoch 677/2200]  [Disc loss: -0.686056] [Gen loss: -2.348865] \n",
      "[Epoch 678/2200]  [Disc loss: -0.694931] [Gen loss: -2.107606] \n",
      "[Epoch 679/2200]  [Disc loss: -0.687628] [Gen loss: -2.211906] \n",
      "[Epoch 680/2200]  [Disc loss: -0.688235] [Gen loss: -2.116859] \n",
      "[Epoch 681/2200]  [Disc loss: -0.690501] [Gen loss: -1.921776] \n",
      "[Epoch 682/2200]  [Disc loss: -0.687253] [Gen loss: -2.158057] \n",
      "[Epoch 683/2200]  [Disc loss: -0.688733] [Gen loss: -2.074551] \n",
      "[Epoch 684/2200]  [Disc loss: -0.685724] [Gen loss: -1.865629] \n",
      "[Epoch 685/2200]  [Disc loss: -0.687456] [Gen loss: -1.927622] \n",
      "[Epoch 686/2200]  [Disc loss: -0.681053] [Gen loss: -2.075056] \n",
      "[Epoch 687/2200]  [Disc loss: -0.685778] [Gen loss: -2.105442] \n",
      "[Epoch 688/2200]  [Disc loss: -0.682168] [Gen loss: -1.968404] \n",
      "[Epoch 689/2200]  [Disc loss: -0.684797] [Gen loss: -1.997605] \n",
      "[Epoch 690/2200]  [Disc loss: -0.687539] [Gen loss: -2.100342] \n",
      "[Epoch 691/2200]  [Disc loss: -0.683998] [Gen loss: -1.818283] \n",
      "[Epoch 692/2200]  [Disc loss: -0.685554] [Gen loss: -1.924352] \n",
      "[Epoch 693/2200]  [Disc loss: -0.681237] [Gen loss: -1.991757] \n",
      "[Epoch 694/2200]  [Disc loss: -0.681433] [Gen loss: -1.856342] \n",
      "[Epoch 695/2200]  [Disc loss: -0.674123] [Gen loss: -2.050508] \n",
      "[Epoch 696/2200]  [Disc loss: -0.684831] [Gen loss: -1.951895] \n",
      "[Epoch 697/2200]  [Disc loss: -0.683581] [Gen loss: -1.800361] \n",
      "[Epoch 698/2200]  [Disc loss: -0.684663] [Gen loss: -1.649358] \n",
      "[Epoch 699/2200]  [Disc loss: -0.678177] [Gen loss: -2.064512] \n",
      "[Epoch 700/2200]  [Disc loss: -0.678701] [Gen loss: -1.819106] \n",
      "[Epoch 701/2200]  [Disc loss: -0.677336] [Gen loss: -1.691257] \n",
      "[Epoch 702/2200]  [Disc loss: -0.680651] [Gen loss: -1.897748] \n",
      "[Epoch 703/2200]  [Disc loss: -0.678226] [Gen loss: -1.716742] \n",
      "[Epoch 704/2200]  [Disc loss: -0.679177] [Gen loss: -1.747854] \n",
      "[Epoch 705/2200]  [Disc loss: -0.681021] [Gen loss: -1.658246] \n",
      "[Epoch 706/2200]  [Disc loss: -0.681979] [Gen loss: -1.685268] \n",
      "[Epoch 707/2200]  [Disc loss: -0.682357] [Gen loss: -1.671159] \n",
      "[Epoch 708/2200]  [Disc loss: -0.674542] [Gen loss: -1.558505] \n",
      "[Epoch 709/2200]  [Disc loss: -0.674822] [Gen loss: -1.620607] \n",
      "[Epoch 710/2200]  [Disc loss: -0.678335] [Gen loss: -1.533462] \n",
      "[Epoch 711/2200]  [Disc loss: -0.675881] [Gen loss: -1.639020] \n",
      "[Epoch 712/2200]  [Disc loss: -0.674225] [Gen loss: -1.438684] \n",
      "[Epoch 713/2200]  [Disc loss: -0.683462] [Gen loss: -1.775241] \n",
      "[Epoch 714/2200]  [Disc loss: -0.675407] [Gen loss: -1.729359] \n",
      "[Epoch 715/2200]  [Disc loss: -0.676217] [Gen loss: -1.485774] \n",
      "[Epoch 716/2200]  [Disc loss: -0.676416] [Gen loss: -1.961118] \n",
      "[Epoch 717/2200]  [Disc loss: -0.674299] [Gen loss: -1.423022] \n",
      "[Epoch 718/2200]  [Disc loss: -0.671675] [Gen loss: -1.596751] \n",
      "[Epoch 719/2200]  [Disc loss: -0.673798] [Gen loss: -1.673852] \n",
      "[Epoch 720/2200]  [Disc loss: -0.670178] [Gen loss: -1.618834] \n",
      "[Epoch 721/2200]  [Disc loss: -0.666528] [Gen loss: -1.609599] \n",
      "[Epoch 722/2200]  [Disc loss: -0.677339] [Gen loss: -1.320067] \n",
      "[Epoch 723/2200]  [Disc loss: -0.673618] [Gen loss: -1.458145] \n",
      "[Epoch 724/2200]  [Disc loss: -0.672297] [Gen loss: -1.599102] \n",
      "[Epoch 725/2200]  [Disc loss: -0.673476] [Gen loss: -1.179021] \n",
      "[Epoch 726/2200]  [Disc loss: -0.671275] [Gen loss: -1.654794] \n",
      "[Epoch 727/2200]  [Disc loss: -0.668130] [Gen loss: -1.647233] \n",
      "[Epoch 728/2200]  [Disc loss: -0.670424] [Gen loss: -1.625556] \n",
      "[Epoch 729/2200]  [Disc loss: -0.664877] [Gen loss: -1.496187] \n",
      "[Epoch 730/2200]  [Disc loss: -0.667201] [Gen loss: -1.525676] \n",
      "[Epoch 731/2200]  [Disc loss: -0.668977] [Gen loss: -1.640838] \n",
      "[Epoch 732/2200]  [Disc loss: -0.667643] [Gen loss: -1.394192] \n",
      "[Epoch 733/2200]  [Disc loss: -0.668408] [Gen loss: -1.612314] \n",
      "[Epoch 734/2200]  [Disc loss: -0.664906] [Gen loss: -1.506216] \n",
      "[Epoch 735/2200]  [Disc loss: -0.663285] [Gen loss: -1.654328] \n",
      "[Epoch 736/2200]  [Disc loss: -0.665102] [Gen loss: -1.521313] \n",
      "[Epoch 737/2200]  [Disc loss: -0.661572] [Gen loss: -1.311111] \n",
      "[Epoch 738/2200]  [Disc loss: -0.662772] [Gen loss: -1.677052] \n",
      "[Epoch 739/2200]  [Disc loss: -0.663145] [Gen loss: -1.315977] \n",
      "[Epoch 740/2200]  [Disc loss: -0.665498] [Gen loss: -1.714415] \n",
      "[Epoch 741/2200]  [Disc loss: -0.662703] [Gen loss: -1.306171] \n",
      "[Epoch 742/2200]  [Disc loss: -0.662631] [Gen loss: -1.350908] \n",
      "[Epoch 743/2200]  [Disc loss: -0.662665] [Gen loss: -1.401886] \n",
      "[Epoch 744/2200]  [Disc loss: -0.660788] [Gen loss: -1.389082] \n",
      "[Epoch 745/2200]  [Disc loss: -0.660437] [Gen loss: -1.621594] \n",
      "[Epoch 746/2200]  [Disc loss: -0.662253] [Gen loss: -1.333054] \n",
      "[Epoch 747/2200]  [Disc loss: -0.658504] [Gen loss: -1.506533] \n",
      "[Epoch 748/2200]  [Disc loss: -0.657813] [Gen loss: -1.480796] \n",
      "[Epoch 749/2200]  [Disc loss: -0.657547] [Gen loss: -1.307253] \n",
      "[Epoch 750/2200]  [Disc loss: -0.663539] [Gen loss: -1.233503] \n",
      "[Epoch 751/2200]  [Disc loss: -0.661996] [Gen loss: -1.406407] \n",
      "[Epoch 752/2200]  [Disc loss: -0.661498] [Gen loss: -1.288716] \n"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eTARoSEVlICZ"
   },
   "outputs": [],
   "source": [
    "torch.no_grad()\n",
    "trainer.G.eval()\n",
    "\n",
    "S = Sampler(generator=trainer.G)\n",
    "latent = S.sample(10) #10 samples\n",
    "latent = latent.detach().cpu().numpy().tolist()\n",
    "\n",
    "sampled_mols_save_path = os.path.join(trainer.output_model_folder, 'sampled')\n",
    "np.save(sampled_mols_save_path+f'_epoch{200}', latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w1VyOt8pH0Dn"
   },
   "outputs": [],
   "source": [
    "x = np.load('/content/model/sampled_epoch200.npy')\n",
    "pd.DataFrame(x).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avXnOi9WH9tg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "JTNN+LatenGAN_train",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
