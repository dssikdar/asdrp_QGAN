{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'razorback (1).mp3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#url = 'http://www.futurecrew.com/skaven/song_files/mp3/razorback.mp3'\n",
    "#filename = wget.download(url)\n",
    "#filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "colab_type": "code",
    "id": "k85YYlp3wi9a",
    "outputId": "9f2f79b7-4d98-4008-cdbe-11479e946cbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X_JTVAE_250k_rndm_zinc (1).csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wget.download('http://molcyclegan.ardigen.com/250k_rndm_zinc_drugs_clean_3_canonized.csv')\n",
    "wget.download('http://molcyclegan.ardigen.com/X_JTVAE_250k_rndm_zinc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TjywBaDEVFCx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0cTrQarwwOk5"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('X_JTVAE_250k_rndm_zinc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jxlFYRMHTPf0"
   },
   "outputs": [],
   "source": [
    "smiles = data['SMILES'].values\n",
    "np.savetxt(r'smiles.txt', smiles, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mh-408HFrktC"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, data_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.data_shape = data_shape\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(self.data_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, mol):\n",
    "        validity = self.model(mol)\n",
    "        return validity\n",
    "\n",
    "    def save(self, path):\n",
    "        save_dict = {\n",
    "            'model': self.model.state_dict(),\n",
    "            'data_shape': self.data_shape,\n",
    "        }\n",
    "        torch.save(save_dict, path)\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        save_dict = torch.load(path)\n",
    "        D = Discriminator(save_dict['data_shape'])\n",
    "        D.model.load_state_dict(save_dict[\"model\"])\n",
    "\n",
    "        return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d0ZmwIN5VPeZ"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, data_shape, latent_dim=None):\n",
    "        super(Generator, self).__init__()\n",
    "        self.data_shape = data_shape\n",
    "\n",
    "        # latent dim of the generator is one of the hyperparams.\n",
    "        # by default it is set to the prod of data_shapes\n",
    "        self.latent_dim = int(np.prod(self.data_shape)) if latent_dim is None else latent_dim\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(self.latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(self.data_shape))),\n",
    "            # nn.Tanh() # expecting latent vectors to be not normalized\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.model(z)\n",
    "        return out\n",
    "\n",
    "    def save(self, path):\n",
    "        save_dict = {\n",
    "            'latent_dim': self.latent_dim,\n",
    "            'model': self.model.state_dict(),\n",
    "            'data_shape': self.data_shape,\n",
    "        }\n",
    "        torch.save(save_dict, path)\n",
    "\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        save_dict = torch.load(path)\n",
    "        G = Generator(save_dict['data_shape'], latent_dim=save_dict['latent_dim'])\n",
    "        G.model.load_state_dict(save_dict[\"model\"])\n",
    "\n",
    "        return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_NWvRUvVS5G"
   },
   "outputs": [],
   "source": [
    "class Sampler(object):\n",
    "    \"\"\"\n",
    "    Sampling the mols the generator.\n",
    "    All scripts should use this class for sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, generator: Generator):\n",
    "        self.set_generator(generator)\n",
    "\n",
    "    def set_generator(self, generator):\n",
    "        self.G = generator\n",
    "\n",
    "    def sample(self, n):\n",
    "        # Sample noise as generator input\n",
    "        z = torch.cuda.FloatTensor(np.random.uniform(-1, 1, (n, self.G.latent_dim)))\n",
    "        # Generate a batch of mols\n",
    "        return self.G(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qlo1tKSDAuHo"
   },
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LatentMolsDataset(data.Dataset):\n",
    "    def __init__(self, latent_space_mols):\n",
    "        self.data = latent_space_mols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2xjPf5FRV4xT"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "class TrainModelRunner:\n",
    "    # Loss weight for gradient penalty\n",
    "    lambda_gp = 10\n",
    "\n",
    "    def __init__(self, input_data_path, output_model_folder, decode_mols_save_path='', n_epochs=2000, starting_epoch=1,\n",
    "                 batch_size=2500, lr=0.0002, b1=0.5, b2=0.999,  n_critic=5,\n",
    "                 save_interval=1000, sample_after_training=30000, message=\"\"):\n",
    "        self.message = message\n",
    "\n",
    "        # init params\n",
    "        self.input_data_path = input_data_path\n",
    "        self.output_model_folder = output_model_folder\n",
    "        self.n_epochs = n_epochs\n",
    "        self.starting_epoch = starting_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "        self.n_critic = n_critic\n",
    "        self.save_interval = save_interval\n",
    "        self.sample_after_training = sample_after_training\n",
    "        self.decode_mols_save_path = decode_mols_save_path\n",
    "\n",
    "        # initialize dataloader\n",
    "        smiles_lat = pd.read_csv(input_data_path)\n",
    "        latent_space_mols = smiles_lat.drop('SMILES', axis=1).values\n",
    "        latent_space_mols = latent_space_mols.reshape(latent_space_mols.shape[0], 56)\n",
    "\n",
    "        self.dataloader = torch.utils.data.DataLoader(LatentMolsDataset(latent_space_mols), shuffle=True,\n",
    "                                                      batch_size=self.batch_size, drop_last=True)\n",
    "\n",
    "        # load discriminator\n",
    "        discriminator_name = 'discriminator.txt' if self.starting_epoch == 1 else str(\n",
    "            self.starting_epoch) + '_discriminator.txt'\n",
    "        discriminator_path = os.path.join(output_model_folder, discriminator_name)\n",
    "        #self.D = Discriminator.load(discriminator_path)\n",
    "        self.D = Discriminator(latent_space_mols[0].shape)\n",
    "        # load generator\n",
    "        generator_name = 'generator.txt' if self.starting_epoch == 1 else str(\n",
    "            self.starting_epoch) + '_generator.txt'\n",
    "        generator_path = os.path.join(output_model_folder, generator_name)\n",
    "        #self.G = Generator.load(generator_path)\n",
    "        self.G = Generator(latent_space_mols[0].shape)\n",
    "        # initialize sampler\n",
    "        self.Sampler = Sampler(self.G)\n",
    "\n",
    "        # initialize optimizer\n",
    "        self.optimizer_G = torch.optim.Adam(self.G.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
    "        self.optimizer_D = torch.optim.Adam(self.D.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
    "\n",
    "        # Tensor\n",
    "        cuda = True if torch.cuda.is_available() else False\n",
    "        if cuda:\n",
    "            self.G.cuda()\n",
    "            self.D.cuda()\n",
    "        self.Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        print(\"Run began.\")\n",
    "        print(\"Message: %s\" % self.message)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        batches_done = 0\n",
    "        disc_loss_log = []\n",
    "        g_loss_log = []\n",
    "\n",
    "        for epoch in range(self.starting_epoch, self.n_epochs + self.starting_epoch):\n",
    "            disc_loss_per_batch = []\n",
    "            g_loss_log_per_batch = []\n",
    "            for i, real_mols in enumerate(self.dataloader):\n",
    "\n",
    "                # Configure input\n",
    "                real_mols = real_mols.type(self.Tensor)\n",
    "                # real_mols = np.squeeze(real_mols, axis=1)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                self.optimizer_D.zero_grad()\n",
    "\n",
    "                # Generate a batch of mols from noise\n",
    "                fake_mols = self.Sampler.sample(real_mols.shape[0])\n",
    "\n",
    "                # Real mols\n",
    "                real_validity = self.D(real_mols)\n",
    "                # Fake mols\n",
    "                fake_validity = self.D(fake_mols)\n",
    "                # Gradient penalty\n",
    "                gradient_penalty = self.compute_gradient_penalty(real_mols.data, fake_mols.data)\n",
    "                # Adversarial loss\n",
    "                d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + self.lambda_gp * gradient_penalty\n",
    "                disc_loss_per_batch.append(d_loss.item())\n",
    "\n",
    "                d_loss.backward()\n",
    "                self.optimizer_D.step()\n",
    "                self.optimizer_G.zero_grad()\n",
    "\n",
    "                # Train the generator every n_critic steps\n",
    "                if i % self.n_critic == 0:\n",
    "                    # -----------------\n",
    "                    #  Train Generator\n",
    "                    # -----------------\n",
    "\n",
    "                    # Generate a batch of mols\n",
    "                    fake_mols = self.Sampler.sample(real_mols.shape[0])\n",
    "                    # Loss measures generator's ability to fool the discriminator\n",
    "                    # Train on fake images\n",
    "                    fake_validity = self.D(fake_mols)\n",
    "                    g_loss = -torch.mean(fake_validity)\n",
    "                    g_loss_log_per_batch.append(g_loss.item())\n",
    "\n",
    "                    g_loss.backward()\n",
    "                    self.optimizer_G.step()\n",
    "\n",
    "                    batches_done += self.n_critic\n",
    "\n",
    "                # If last batch in the set\n",
    "                if i == len(self.dataloader) - 1:\n",
    "                    if epoch % self.save_interval == 0:\n",
    "                        generator_save_path = os.path.join(self.output_model_folder,\n",
    "                                                           str(epoch) + '_generator.txt')\n",
    "                        discriminator_save_path = os.path.join(self.output_model_folder,\n",
    "                                                               str(epoch) + '_discriminator.txt')\n",
    "                        self.G.save(generator_save_path)\n",
    "                        self.D.save(discriminator_save_path)\n",
    "\n",
    "                    disc_loss_log.append([time.time(), epoch, np.mean(disc_loss_per_batch)])\n",
    "                    g_loss_log.append([time.time(), epoch, np.mean(g_loss_log_per_batch)])\n",
    "\n",
    "                    # Print and log\n",
    "                    print(\n",
    "                        \"[Epoch %d/%d]  [Disc loss: %f] [Gen loss: %f] \"\n",
    "                        % (epoch, self.n_epochs + self.starting_epoch, disc_loss_log[-1][2], g_loss_log[-1][2])\n",
    "                    )\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "        # log the losses\n",
    "        with open(os.path.join(self.output_model_folder, 'disc_loss.json'), 'w') as json_file:\n",
    "            json.dump(disc_loss_log, json_file)\n",
    "        with open(os.path.join(self.output_model_folder, 'gen_loss.json'), 'w') as json_file:\n",
    "            json.dump(g_loss_log, json_file)\n",
    "\n",
    "        # Sampling after training\n",
    "        if self.sample_after_training > 0:\n",
    "            print(\"Training finished. Generating sample of latent vectors\")\n",
    "            # sampling mode\n",
    "            torch.no_grad()\n",
    "            self.G.eval()\n",
    "\n",
    "            S = Sampler(generator=self.G)\n",
    "            latent = S.sample(self.sample_after_training)\n",
    "            latent = latent.detach().cpu().numpy().tolist()\n",
    "\n",
    "            sampled_mols_save_path = os.path.join(self.output_model_folder, 'sampled')\n",
    "            np.save(sampled_mols_save_path+f'_epoch{epoch}', latent)\n",
    "\n",
    "            # decoding sampled mols\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def compute_gradient_penalty(self, real_samples, fake_samples):\n",
    "        \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "        # Random weight term for interpolation between real and fake samples\n",
    "        alpha = self.Tensor(np.random.random((real_samples.size(0), 1)))\n",
    "\n",
    "        # Get random interpolation between real and fake samples\n",
    "        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "        d_interpolates = self.D(interpolates)\n",
    "        fake = self.Tensor(real_samples.shape[0], 1).fill_(1.0)\n",
    "\n",
    "        # Get gradient w.r.t. interpolates\n",
    "        gradients = autograd.grad(\n",
    "            outputs=d_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "        return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGNNUWy1Az_B"
   },
   "outputs": [],
   "source": [
    "trainer = TrainModelRunner('X_JTVAE_250k_rndm_zinc.csv', output_model_folder='/content/model', starting_epoch=200,\n",
    "                           save_interval=100, message='Starting training', batch_size=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gAV2y8c_kHWc",
    "outputId": "a83a21ea-37b5-417c-c238-b00d1480b819"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run began.\n",
      "Message: Starting training\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-041e2033e90a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-68270a99a427>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[1;31m# Generate a batch of mols from noise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                 \u001b[0mfake_mols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSampler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_mols\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[1;31m# Real mols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-c911edc6fda4>\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# Sample noise as generator input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;31m# Generate a batch of mols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[1;31m# are found or any other error occurs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m         \u001b[1;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[1;31m# we need to just return without initializing in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eTARoSEVlICZ"
   },
   "outputs": [],
   "source": [
    "torch.no_grad()\n",
    "trainer.G.eval()\n",
    "\n",
    "S = Sampler(generator=trainer.G)\n",
    "latent = S.sample(10) #10 samples\n",
    "latent = latent.detach().cpu().numpy().tolist()\n",
    "\n",
    "sampled_mols_save_path = os.path.join(trainer.output_model_folder, 'sampled')\n",
    "np.save(sampled_mols_save_path+f'_epoch{200}', latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w1VyOt8pH0Dn"
   },
   "outputs": [],
   "source": [
    "x = np.load('/content/model/sampled_epoch200.npy')\n",
    "pd.DataFrame(x).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avXnOi9WH9tg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "JTNN+LatenGAN_train",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
