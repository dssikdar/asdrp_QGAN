{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def download_file(url):\n",
    "    local_filename = url.split('/')[-1]\n",
    "    # NOTE the stream=True parameter below\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                # If you have chunk encoded response uncomment if\n",
    "                # and set chunk_size parameter to None.\n",
    "                #if chunk: \n",
    "                f.write(chunk)\n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'razorback.mp3'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#url = 'http://www.futurecrew.com/skaven/song_files/mp3/razorback.mp3'\n",
    "#filename = download_file(url)\n",
    "#filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "colab_type": "code",
    "id": "k85YYlp3wi9a",
    "outputId": "9f2f79b7-4d98-4008-cdbe-11479e946cbf"
   },
   "outputs": [],
   "source": [
    "#!wget http://molcyclegan.ardigen.com/250k_rndm_zinc_drugs_clean_3_canonized.csv\n",
    "url_1 = 'http://molcyclegan.ardigen.com/250k_rndm_zinc_drugs_clean_3_canonized.csv'\n",
    "filename1 = download_file(url_1)\n",
    "#!wget http://molcyclegan.ardigen.com/X_JTVAE_250k_rndm_zinc.csv\n",
    "url_2 = 'http://molcyclegan.ardigen.com/X_JTVAE_250k_rndm_zinc.csv'\n",
    "filename2 = download_file(url_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'250k_rndm_zinc_drugs_clean_3_canonized.csv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X_JTVAE_250k_rndm_zinc.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TjywBaDEVFCx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0cTrQarwwOk5"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "      <th>jtvae_ld_0</th>\n",
       "      <th>jtvae_ld_1</th>\n",
       "      <th>jtvae_ld_2</th>\n",
       "      <th>jtvae_ld_3</th>\n",
       "      <th>jtvae_ld_4</th>\n",
       "      <th>jtvae_ld_5</th>\n",
       "      <th>jtvae_ld_6</th>\n",
       "      <th>jtvae_ld_7</th>\n",
       "      <th>jtvae_ld_8</th>\n",
       "      <th>...</th>\n",
       "      <th>jtvae_ld_46</th>\n",
       "      <th>jtvae_ld_47</th>\n",
       "      <th>jtvae_ld_48</th>\n",
       "      <th>jtvae_ld_49</th>\n",
       "      <th>jtvae_ld_50</th>\n",
       "      <th>jtvae_ld_51</th>\n",
       "      <th>jtvae_ld_52</th>\n",
       "      <th>jtvae_ld_53</th>\n",
       "      <th>jtvae_ld_54</th>\n",
       "      <th>jtvae_ld_55</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC(C)(C)c1ccc2occ(CC(=O)Nc3ccccc3F)c2c1</td>\n",
       "      <td>-1.760562</td>\n",
       "      <td>-2.570686</td>\n",
       "      <td>0.036085</td>\n",
       "      <td>0.604086</td>\n",
       "      <td>1.004958</td>\n",
       "      <td>0.839572</td>\n",
       "      <td>-0.673857</td>\n",
       "      <td>-0.537430</td>\n",
       "      <td>-4.839734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.409784</td>\n",
       "      <td>0.158020</td>\n",
       "      <td>0.269396</td>\n",
       "      <td>-0.688609</td>\n",
       "      <td>0.108560</td>\n",
       "      <td>-0.313217</td>\n",
       "      <td>0.266187</td>\n",
       "      <td>-0.173103</td>\n",
       "      <td>-0.218692</td>\n",
       "      <td>0.125347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C[C@@H]1CC(Nc2cncc(-c3nncn3C)c2)C[C@@H](C)C1</td>\n",
       "      <td>0.335171</td>\n",
       "      <td>-1.957197</td>\n",
       "      <td>2.446603</td>\n",
       "      <td>2.097161</td>\n",
       "      <td>0.844135</td>\n",
       "      <td>-2.915459</td>\n",
       "      <td>0.923053</td>\n",
       "      <td>-1.548471</td>\n",
       "      <td>1.267626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.140595</td>\n",
       "      <td>0.901454</td>\n",
       "      <td>0.125713</td>\n",
       "      <td>-0.159662</td>\n",
       "      <td>0.168850</td>\n",
       "      <td>0.168889</td>\n",
       "      <td>-0.468159</td>\n",
       "      <td>0.399968</td>\n",
       "      <td>0.160643</td>\n",
       "      <td>0.033684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N#Cc1ccc(-c2ccc(O[C@@H](C(=O)N3CCCC3)c3ccccc3)...</td>\n",
       "      <td>-2.339868</td>\n",
       "      <td>2.752925</td>\n",
       "      <td>2.923308</td>\n",
       "      <td>-1.494377</td>\n",
       "      <td>2.267280</td>\n",
       "      <td>3.681453</td>\n",
       "      <td>-0.057489</td>\n",
       "      <td>0.497126</td>\n",
       "      <td>-0.124107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308208</td>\n",
       "      <td>-0.143885</td>\n",
       "      <td>0.006852</td>\n",
       "      <td>-0.086603</td>\n",
       "      <td>0.089586</td>\n",
       "      <td>-1.151665</td>\n",
       "      <td>0.327640</td>\n",
       "      <td>-0.017453</td>\n",
       "      <td>0.222926</td>\n",
       "      <td>-0.169397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCOC(=O)[C@@H]1CCCN(C(=O)c2nc(-c3ccc(C)cc3)n3c...</td>\n",
       "      <td>0.989997</td>\n",
       "      <td>-0.537878</td>\n",
       "      <td>3.158018</td>\n",
       "      <td>-0.319200</td>\n",
       "      <td>1.566594</td>\n",
       "      <td>-0.273303</td>\n",
       "      <td>0.138059</td>\n",
       "      <td>2.715511</td>\n",
       "      <td>-1.951281</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.358162</td>\n",
       "      <td>0.120603</td>\n",
       "      <td>0.149405</td>\n",
       "      <td>-0.179605</td>\n",
       "      <td>0.258707</td>\n",
       "      <td>0.296047</td>\n",
       "      <td>-0.328661</td>\n",
       "      <td>0.917348</td>\n",
       "      <td>0.304539</td>\n",
       "      <td>0.041231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N#CC1=C(SCC(=O)Nc2cccc(Cl)c2)N=C([O-])[C@H](C#...</td>\n",
       "      <td>2.154324</td>\n",
       "      <td>-0.065307</td>\n",
       "      <td>-0.255752</td>\n",
       "      <td>0.834379</td>\n",
       "      <td>1.460813</td>\n",
       "      <td>-0.483744</td>\n",
       "      <td>-2.948271</td>\n",
       "      <td>-1.574727</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197934</td>\n",
       "      <td>0.272452</td>\n",
       "      <td>-0.110255</td>\n",
       "      <td>0.246869</td>\n",
       "      <td>0.072465</td>\n",
       "      <td>1.058471</td>\n",
       "      <td>-0.333425</td>\n",
       "      <td>0.186757</td>\n",
       "      <td>-0.505750</td>\n",
       "      <td>0.409591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              SMILES  jtvae_ld_0  jtvae_ld_1  \\\n",
       "0            CC(C)(C)c1ccc2occ(CC(=O)Nc3ccccc3F)c2c1   -1.760562   -2.570686   \n",
       "1       C[C@@H]1CC(Nc2cncc(-c3nncn3C)c2)C[C@@H](C)C1    0.335171   -1.957197   \n",
       "2  N#Cc1ccc(-c2ccc(O[C@@H](C(=O)N3CCCC3)c3ccccc3)...   -2.339868    2.752925   \n",
       "3  CCOC(=O)[C@@H]1CCCN(C(=O)c2nc(-c3ccc(C)cc3)n3c...    0.989997   -0.537878   \n",
       "4  N#CC1=C(SCC(=O)Nc2cccc(Cl)c2)N=C([O-])[C@H](C#...    2.154324   -0.065307   \n",
       "\n",
       "   jtvae_ld_2  jtvae_ld_3  jtvae_ld_4  jtvae_ld_5  jtvae_ld_6  jtvae_ld_7  \\\n",
       "0    0.036085    0.604086    1.004958    0.839572   -0.673857   -0.537430   \n",
       "1    2.446603    2.097161    0.844135   -2.915459    0.923053   -1.548471   \n",
       "2    2.923308   -1.494377    2.267280    3.681453   -0.057489    0.497126   \n",
       "3    3.158018   -0.319200    1.566594   -0.273303    0.138059    2.715511   \n",
       "4   -0.255752    0.834379    1.460813   -0.483744   -2.948271   -1.574727   \n",
       "\n",
       "   jtvae_ld_8  ...  jtvae_ld_46  jtvae_ld_47  jtvae_ld_48  jtvae_ld_49  \\\n",
       "0   -4.839734  ...     0.409784     0.158020     0.269396    -0.688609   \n",
       "1    1.267626  ...    -0.140595     0.901454     0.125713    -0.159662   \n",
       "2   -0.124107  ...     0.308208    -0.143885     0.006852    -0.086603   \n",
       "3   -1.951281  ...    -0.358162     0.120603     0.149405    -0.179605   \n",
       "4    0.607102  ...     0.197934     0.272452    -0.110255     0.246869   \n",
       "\n",
       "   jtvae_ld_50  jtvae_ld_51  jtvae_ld_52  jtvae_ld_53  jtvae_ld_54  \\\n",
       "0     0.108560    -0.313217     0.266187    -0.173103    -0.218692   \n",
       "1     0.168850     0.168889    -0.468159     0.399968     0.160643   \n",
       "2     0.089586    -1.151665     0.327640    -0.017453     0.222926   \n",
       "3     0.258707     0.296047    -0.328661     0.917348     0.304539   \n",
       "4     0.072465     1.058471    -0.333425     0.186757    -0.505750   \n",
       "\n",
       "   jtvae_ld_55  \n",
       "0     0.125347  \n",
       "1     0.033684  \n",
       "2    -0.169397  \n",
       "3     0.041231  \n",
       "4     0.409591  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 249455 entries, 0 to 249454\n",
      "Data columns (total 57 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   SMILES       249455 non-null  object \n",
      " 1   jtvae_ld_0   249455 non-null  float64\n",
      " 2   jtvae_ld_1   249455 non-null  float64\n",
      " 3   jtvae_ld_2   249455 non-null  float64\n",
      " 4   jtvae_ld_3   249455 non-null  float64\n",
      " 5   jtvae_ld_4   249455 non-null  float64\n",
      " 6   jtvae_ld_5   249455 non-null  float64\n",
      " 7   jtvae_ld_6   249455 non-null  float64\n",
      " 8   jtvae_ld_7   249455 non-null  float64\n",
      " 9   jtvae_ld_8   249455 non-null  float64\n",
      " 10  jtvae_ld_9   249455 non-null  float64\n",
      " 11  jtvae_ld_10  249455 non-null  float64\n",
      " 12  jtvae_ld_11  249455 non-null  float64\n",
      " 13  jtvae_ld_12  249455 non-null  float64\n",
      " 14  jtvae_ld_13  249455 non-null  float64\n",
      " 15  jtvae_ld_14  249455 non-null  float64\n",
      " 16  jtvae_ld_15  249455 non-null  float64\n",
      " 17  jtvae_ld_16  249455 non-null  float64\n",
      " 18  jtvae_ld_17  249455 non-null  float64\n",
      " 19  jtvae_ld_18  249455 non-null  float64\n",
      " 20  jtvae_ld_19  249455 non-null  float64\n",
      " 21  jtvae_ld_20  249455 non-null  float64\n",
      " 22  jtvae_ld_21  249455 non-null  float64\n",
      " 23  jtvae_ld_22  249455 non-null  float64\n",
      " 24  jtvae_ld_23  249455 non-null  float64\n",
      " 25  jtvae_ld_24  249455 non-null  float64\n",
      " 26  jtvae_ld_25  249455 non-null  float64\n",
      " 27  jtvae_ld_26  249455 non-null  float64\n",
      " 28  jtvae_ld_27  249455 non-null  float64\n",
      " 29  jtvae_ld_28  249455 non-null  float64\n",
      " 30  jtvae_ld_29  249455 non-null  float64\n",
      " 31  jtvae_ld_30  249455 non-null  float64\n",
      " 32  jtvae_ld_31  249455 non-null  float64\n",
      " 33  jtvae_ld_32  249455 non-null  float64\n",
      " 34  jtvae_ld_33  249455 non-null  float64\n",
      " 35  jtvae_ld_34  249455 non-null  float64\n",
      " 36  jtvae_ld_35  249455 non-null  float64\n",
      " 37  jtvae_ld_36  249455 non-null  float64\n",
      " 38  jtvae_ld_37  249455 non-null  float64\n",
      " 39  jtvae_ld_38  249455 non-null  float64\n",
      " 40  jtvae_ld_39  249455 non-null  float64\n",
      " 41  jtvae_ld_40  249455 non-null  float64\n",
      " 42  jtvae_ld_41  249455 non-null  float64\n",
      " 43  jtvae_ld_42  249455 non-null  float64\n",
      " 44  jtvae_ld_43  249455 non-null  float64\n",
      " 45  jtvae_ld_44  249455 non-null  float64\n",
      " 46  jtvae_ld_45  249455 non-null  float64\n",
      " 47  jtvae_ld_46  249455 non-null  float64\n",
      " 48  jtvae_ld_47  249455 non-null  float64\n",
      " 49  jtvae_ld_48  249455 non-null  float64\n",
      " 50  jtvae_ld_49  249455 non-null  float64\n",
      " 51  jtvae_ld_50  249455 non-null  float64\n",
      " 52  jtvae_ld_51  249455 non-null  float64\n",
      " 53  jtvae_ld_52  249455 non-null  float64\n",
      " 54  jtvae_ld_53  249455 non-null  float64\n",
      " 55  jtvae_ld_54  249455 non-null  float64\n",
      " 56  jtvae_ld_55  249455 non-null  float64\n",
      "dtypes: float64(56), object(1)\n",
      "memory usage: 108.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jxlFYRMHTPf0"
   },
   "outputs": [],
   "source": [
    "smiles = data['SMILES'].values\n",
    "np.savetxt(r'smiles.txt', smiles, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mh-408HFrktC"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, data_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.data_shape = data_shape\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(self.data_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, mol):\n",
    "        validity = self.model(mol)\n",
    "        return validity\n",
    "\n",
    "    def save(self, path):\n",
    "        save_dict = {\n",
    "            'model': self.model.state_dict(),\n",
    "            'data_shape': self.data_shape,\n",
    "        }\n",
    "        torch.save(save_dict, path)\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        save_dict = torch.load(path)\n",
    "        D = Discriminator(save_dict['data_shape'])\n",
    "        D.model.load_state_dict(save_dict[\"model\"])\n",
    "\n",
    "        return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d0ZmwIN5VPeZ"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, data_shape, latent_dim=None):\n",
    "        super(Generator, self).__init__()\n",
    "        self.data_shape = data_shape\n",
    "\n",
    "        # latent dim of the generator is one of the hyperparams.\n",
    "        # by default it is set to the prod of data_shapes\n",
    "        self.latent_dim = int(np.prod(self.data_shape)) if latent_dim is None else latent_dim\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(self.latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(self.data_shape))),\n",
    "            # nn.Tanh() # expecting latent vectors to be not normalized\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.model(z)\n",
    "        return out\n",
    "\n",
    "    def save(self, path):\n",
    "        save_dict = {\n",
    "            'latent_dim': self.latent_dim,\n",
    "            'model': self.model.state_dict(),\n",
    "            'data_shape': self.data_shape,\n",
    "        }\n",
    "        torch.save(save_dict, path)\n",
    "\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        save_dict = torch.load(path)\n",
    "        G = Generator(save_dict['data_shape'], latent_dim=save_dict['latent_dim'])\n",
    "        G.model.load_state_dict(save_dict[\"model\"])\n",
    "\n",
    "        return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_NWvRUvVS5G"
   },
   "outputs": [],
   "source": [
    "class Sampler(object):\n",
    "    \"\"\"\n",
    "    Sampling the mols the generator.\n",
    "    All scripts should use this class for sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, generator: Generator):\n",
    "        self.set_generator(generator)\n",
    "\n",
    "    def set_generator(self, generator):\n",
    "        self.G = generator\n",
    "\n",
    "    def sample(self, n):\n",
    "        # Sample noise as generator input\n",
    "        z = torch.cuda.FloatTensor(np.random.uniform(-1, 1, (n, self.G.latent_dim)))\n",
    "        # Generate a batch of mols\n",
    "        return self.G(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qlo1tKSDAuHo"
   },
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LatentMolsDataset(data.Dataset):\n",
    "    def __init__(self, latent_space_mols):\n",
    "        self.data = latent_space_mols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2xjPf5FRV4xT"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "class TrainModelRunner:\n",
    "    # Loss weight for gradient penalty\n",
    "    lambda_gp = 10\n",
    "\n",
    "    def __init__(self, input_data_path, output_model_folder, decode_mols_save_path='', n_epochs=2000, starting_epoch=1,\n",
    "                 batch_size=2500, lr=0.0002, b1=0.5, b2=0.999,  n_critic=5,\n",
    "                 save_interval=1000, sample_after_training=30000, message=\"\"):\n",
    "        self.message = message\n",
    "\n",
    "        # init params\n",
    "        self.input_data_path = input_data_path\n",
    "        self.output_model_folder = output_model_folder\n",
    "        self.n_epochs = n_epochs\n",
    "        self.starting_epoch = starting_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "        self.n_critic = n_critic\n",
    "        self.save_interval = save_interval\n",
    "        self.sample_after_training = sample_after_training\n",
    "        self.decode_mols_save_path = decode_mols_save_path\n",
    "\n",
    "        # initialize dataloader\n",
    "        smiles_lat = pd.read_csv(input_data_path)\n",
    "        latent_space_mols = smiles_lat.drop('SMILES', axis=1).values\n",
    "        latent_space_mols = latent_space_mols.reshape(latent_space_mols.shape[0], 56)\n",
    "\n",
    "        self.dataloader = torch.utils.data.DataLoader(LatentMolsDataset(latent_space_mols), shuffle=True,\n",
    "                                                      batch_size=self.batch_size, drop_last=True)\n",
    "\n",
    "        # load discriminator\n",
    "        discriminator_name = 'discriminator.txt' if self.starting_epoch == 1 else str(\n",
    "            self.starting_epoch) + '_discriminator.txt'\n",
    "        discriminator_path = os.path.join(output_model_folder, discriminator_name)\n",
    "        self.D = Discriminator.load(discriminator_path)\n",
    "        # self.D = Discriminator(latent_space_mols[0].shape)\n",
    "        # load generator\n",
    "        generator_name = 'generator.txt' if self.starting_epoch == 1 else str(\n",
    "            self.starting_epoch) + '_generator.txt'\n",
    "        generator_path = os.path.join(output_model_folder, generator_name)\n",
    "        self.G = Generator.load(generator_path)\n",
    "        # self.G = Generator(latent_space_mols[0].shape)\n",
    "        # initialize sampler\n",
    "        self.Sampler = Sampler(self.G)\n",
    "\n",
    "        # initialize optimizer\n",
    "        self.optimizer_G = torch.optim.Adam(self.G.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
    "        self.optimizer_D = torch.optim.Adam(self.D.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
    "\n",
    "        # Tensor\n",
    "        cuda = True if torch.cuda.is_available() else False\n",
    "        if cuda:\n",
    "            self.G.cuda()\n",
    "            self.D.cuda()\n",
    "        self.Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        print(\"Run began.\")\n",
    "        print(\"Message: %s\" % self.message)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        batches_done = 0\n",
    "        disc_loss_log = []\n",
    "        g_loss_log = []\n",
    "\n",
    "        for epoch in range(self.starting_epoch, self.n_epochs + self.starting_epoch):\n",
    "            disc_loss_per_batch = []\n",
    "            g_loss_log_per_batch = []\n",
    "            for i, real_mols in enumerate(self.dataloader):\n",
    "\n",
    "                # Configure input\n",
    "                real_mols = real_mols.type(self.Tensor)\n",
    "                # real_mols = np.squeeze(real_mols, axis=1)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                self.optimizer_D.zero_grad()\n",
    "\n",
    "                # Generate a batch of mols from noise\n",
    "                fake_mols = self.Sampler.sample(real_mols.shape[0])\n",
    "\n",
    "                # Real mols\n",
    "                real_validity = self.D(real_mols)\n",
    "                # Fake mols\n",
    "                fake_validity = self.D(fake_mols)\n",
    "                # Gradient penalty\n",
    "                gradient_penalty = self.compute_gradient_penalty(real_mols.data, fake_mols.data)\n",
    "                # Adversarial loss\n",
    "                d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + self.lambda_gp * gradient_penalty\n",
    "                disc_loss_per_batch.append(d_loss.item())\n",
    "\n",
    "                d_loss.backward()\n",
    "                self.optimizer_D.step()\n",
    "                self.optimizer_G.zero_grad()\n",
    "\n",
    "                # Train the generator every n_critic steps\n",
    "                if i % self.n_critic == 0:\n",
    "                    # -----------------\n",
    "                    #  Train Generator\n",
    "                    # -----------------\n",
    "\n",
    "                    # Generate a batch of mols\n",
    "                    fake_mols = self.Sampler.sample(real_mols.shape[0])\n",
    "                    # Loss measures generator's ability to fool the discriminator\n",
    "                    # Train on fake images\n",
    "                    fake_validity = self.D(fake_mols)\n",
    "                    g_loss = -torch.mean(fake_validity)\n",
    "                    g_loss_log_per_batch.append(g_loss.item())\n",
    "\n",
    "                    g_loss.backward()\n",
    "                    self.optimizer_G.step()\n",
    "\n",
    "                    batches_done += self.n_critic\n",
    "\n",
    "                # If last batch in the set\n",
    "                if i == len(self.dataloader) - 1:\n",
    "                    if epoch % self.save_interval == 0:\n",
    "                        generator_save_path = os.path.join(self.output_model_folder,\n",
    "                                                           str(epoch) + '_generator.txt')\n",
    "                        discriminator_save_path = os.path.join(self.output_model_folder,\n",
    "                                                               str(epoch) + '_discriminator.txt')\n",
    "                        self.G.save(generator_save_path)\n",
    "                        self.D.save(discriminator_save_path)\n",
    "\n",
    "                    disc_loss_log.append([time.time(), epoch, np.mean(disc_loss_per_batch)])\n",
    "                    g_loss_log.append([time.time(), epoch, np.mean(g_loss_log_per_batch)])\n",
    "\n",
    "                    # Print and log\n",
    "                    print(\n",
    "                        \"[Epoch %d/%d]  [Disc loss: %f] [Gen loss: %f] \"\n",
    "                        % (epoch, self.n_epochs + self.starting_epoch, disc_loss_log[-1][2], g_loss_log[-1][2])\n",
    "                    )\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "        # log the losses\n",
    "        with open(os.path.join(self.output_model_folder, 'disc_loss.json'), 'w') as json_file:\n",
    "            json.dump(disc_loss_log, json_file)\n",
    "        with open(os.path.join(self.output_model_folder, 'gen_loss.json'), 'w') as json_file:\n",
    "            json.dump(g_loss_log, json_file)\n",
    "\n",
    "        # Sampling after training\n",
    "        if self.sample_after_training > 0:\n",
    "            print(\"Training finished. Generating sample of latent vectors\")\n",
    "            # sampling mode\n",
    "            torch.no_grad()\n",
    "            self.G.eval()\n",
    "\n",
    "            S = Sampler(generator=self.G)\n",
    "            latent = S.sample(self.sample_after_training)\n",
    "            latent = latent.detach().cpu().numpy().tolist()\n",
    "\n",
    "            sampled_mols_save_path = os.path.join(self.output_model_folder, 'sampled')\n",
    "            np.save(sampled_mols_save_path+f'_epoch{epoch}', latent)\n",
    "\n",
    "            # decoding sampled mols\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def compute_gradient_penalty(self, real_samples, fake_samples):\n",
    "        \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "        # Random weight term for interpolation between real and fake samples\n",
    "        alpha = self.Tensor(np.random.random((real_samples.size(0), 1)))\n",
    "\n",
    "        # Get random interpolation between real and fake samples\n",
    "        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "        d_interpolates = self.D(interpolates)\n",
    "        fake = self.Tensor(real_samples.shape[0], 1).fill_(1.0)\n",
    "\n",
    "        # Get gradient w.r.t. interpolates\n",
    "        gradients = autograd.grad(\n",
    "            outputs=d_interpolates,\n",
    "            inputs=interpolates,\n",
    "            grad_outputs=fake,\n",
    "            create_graph=True,\n",
    "            retain_graph=True,\n",
    "            only_inputs=True,\n",
    "        )[0]\n",
    "        gradients = gradients.view(gradients.size(0), -1)\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "        return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGNNUWy1Az_B"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/model\\\\200_discriminator.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-b5fb7024e99a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#                           save_interval=100, message='Starting training', batch_size=2500)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m trainer = TrainModelRunner(filename2, output_model_folder='/content/model', starting_epoch=200,\n\u001b[0m\u001b[0;32m      5\u001b[0m                            save_interval=100, message='Starting training', batch_size=2500)\n",
      "\u001b[1;32m<ipython-input-29-92a2c7526e5b>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_data_path, output_model_folder, decode_mols_save_path, n_epochs, starting_epoch, batch_size, lr, b1, b2, n_critic, save_interval, sample_after_training, message)\u001b[0m\n\u001b[0;32m     44\u001b[0m             self.starting_epoch) + '_discriminator.txt'\n\u001b[0;32m     45\u001b[0m         \u001b[0mdiscriminator_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_model_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiscriminator_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[1;31m# self.D = Discriminator(latent_space_mols[0].shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# load generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-38fb7bb4855a>\u001b[0m in \u001b[0;36mload\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0msave_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data_shape'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/model\\\\200_discriminator.txt'"
     ]
    }
   ],
   "source": [
    "#trainer = TrainModelRunner('/content/X_JTVAE_250k_rndm_zinc.csv', output_model_folder='/content/model', starting_epoch=200,\n",
    "#                           save_interval=100, message='Starting training', batch_size=2500)\n",
    "\n",
    "trainer = TrainModelRunner(filename2, output_model_folder='/content/model', starting_epoch=200,\n",
    "                           save_interval=100, message='Starting training', batch_size=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gAV2y8c_kHWc",
    "outputId": "a83a21ea-37b5-417c-c238-b00d1480b819"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run began.\n",
      "Message: Starting training\n",
      "[Epoch 200/2200]  [Disc loss: -5.017041] [Gen loss: -0.352216] \n",
      "[Epoch 201/2200]  [Disc loss: -10.961712] [Gen loss: -1.055464] \n",
      "[Epoch 202/2200]  [Disc loss: -9.807225] [Gen loss: -0.696715] \n",
      "[Epoch 203/2200]  [Disc loss: -8.607686] [Gen loss: 0.060340] \n",
      "[Epoch 204/2200]  [Disc loss: -8.559876] [Gen loss: -0.505259] \n",
      "[Epoch 205/2200]  [Disc loss: -8.005614] [Gen loss: -0.392123] \n",
      "[Epoch 206/2200]  [Disc loss: -7.827546] [Gen loss: -0.499194] \n",
      "[Epoch 207/2200]  [Disc loss: -7.369315] [Gen loss: -0.555965] \n",
      "[Epoch 208/2200]  [Disc loss: -6.921161] [Gen loss: -0.694019] \n",
      "[Epoch 209/2200]  [Disc loss: -6.528371] [Gen loss: -0.980572] \n",
      "[Epoch 210/2200]  [Disc loss: -6.317981] [Gen loss: -0.929785] \n",
      "[Epoch 211/2200]  [Disc loss: -5.947452] [Gen loss: -0.827895] \n",
      "[Epoch 212/2200]  [Disc loss: -5.440610] [Gen loss: -0.762877] \n",
      "[Epoch 213/2200]  [Disc loss: -5.192881] [Gen loss: -0.232353] \n",
      "[Epoch 214/2200]  [Disc loss: -5.134250] [Gen loss: -0.235080] \n",
      "[Epoch 215/2200]  [Disc loss: -5.081403] [Gen loss: -0.714943] \n",
      "[Epoch 216/2200]  [Disc loss: -4.880120] [Gen loss: -0.476513] \n",
      "[Epoch 217/2200]  [Disc loss: -4.704821] [Gen loss: -0.450339] \n",
      "[Epoch 218/2200]  [Disc loss: -4.530038] [Gen loss: -0.473468] \n",
      "[Epoch 219/2200]  [Disc loss: -4.374415] [Gen loss: -0.521149] \n",
      "[Epoch 220/2200]  [Disc loss: -4.025223] [Gen loss: -0.561843] \n",
      "[Epoch 221/2200]  [Disc loss: -3.925795] [Gen loss: -0.151995] \n",
      "[Epoch 222/2200]  [Disc loss: -3.904253] [Gen loss: -0.024956] \n",
      "[Epoch 223/2200]  [Disc loss: -3.940362] [Gen loss: 0.034093] \n",
      "[Epoch 224/2200]  [Disc loss: -4.037410] [Gen loss: -0.045827] \n",
      "[Epoch 225/2200]  [Disc loss: -3.701595] [Gen loss: -0.021928] \n",
      "[Epoch 226/2200]  [Disc loss: -3.723526] [Gen loss: -0.100332] \n",
      "[Epoch 227/2200]  [Disc loss: -3.703090] [Gen loss: 0.015572] \n",
      "[Epoch 228/2200]  [Disc loss: -3.811382] [Gen loss: -0.111449] \n",
      "[Epoch 229/2200]  [Disc loss: -3.792235] [Gen loss: 0.081344] \n",
      "[Epoch 230/2200]  [Disc loss: -3.684087] [Gen loss: -0.089867] \n",
      "[Epoch 231/2200]  [Disc loss: -3.678284] [Gen loss: -0.065724] \n",
      "[Epoch 232/2200]  [Disc loss: -3.663446] [Gen loss: -0.233045] \n",
      "[Epoch 233/2200]  [Disc loss: -3.641567] [Gen loss: -0.345498] \n",
      "[Epoch 234/2200]  [Disc loss: -3.603720] [Gen loss: -0.037739] \n",
      "[Epoch 235/2200]  [Disc loss: -3.740910] [Gen loss: -0.223345] \n",
      "[Epoch 236/2200]  [Disc loss: -3.540832] [Gen loss: -0.422719] \n",
      "[Epoch 237/2200]  [Disc loss: -3.523251] [Gen loss: -0.203432] \n",
      "[Epoch 238/2200]  [Disc loss: -3.572305] [Gen loss: -0.055942] \n",
      "[Epoch 239/2200]  [Disc loss: -3.517419] [Gen loss: 0.015189] \n",
      "[Epoch 240/2200]  [Disc loss: -3.572458] [Gen loss: 0.011041] \n",
      "[Epoch 241/2200]  [Disc loss: -3.516521] [Gen loss: 0.306754] \n",
      "[Epoch 242/2200]  [Disc loss: -3.450153] [Gen loss: 0.065120] \n",
      "[Epoch 243/2200]  [Disc loss: -3.401613] [Gen loss: -0.025090] \n",
      "[Epoch 244/2200]  [Disc loss: -3.251059] [Gen loss: 0.478297] \n",
      "[Epoch 245/2200]  [Disc loss: -3.313743] [Gen loss: 0.181706] \n",
      "[Epoch 246/2200]  [Disc loss: -3.157013] [Gen loss: 0.003687] \n",
      "[Epoch 247/2200]  [Disc loss: -3.134795] [Gen loss: 0.292341] \n",
      "[Epoch 248/2200]  [Disc loss: -3.092691] [Gen loss: 0.059656] \n",
      "[Epoch 249/2200]  [Disc loss: -2.966076] [Gen loss: 0.368408] \n",
      "[Epoch 250/2200]  [Disc loss: -2.930228] [Gen loss: 0.185300] \n",
      "[Epoch 251/2200]  [Disc loss: -2.818892] [Gen loss: 0.569694] \n",
      "[Epoch 252/2200]  [Disc loss: -2.828107] [Gen loss: 0.590824] \n",
      "[Epoch 253/2200]  [Disc loss: -2.813336] [Gen loss: 0.737498] \n",
      "[Epoch 254/2200]  [Disc loss: -2.747740] [Gen loss: 0.182152] \n",
      "[Epoch 255/2200]  [Disc loss: -2.637736] [Gen loss: 0.830277] \n",
      "[Epoch 256/2200]  [Disc loss: -2.650179] [Gen loss: 1.287294] \n",
      "[Epoch 257/2200]  [Disc loss: -2.535461] [Gen loss: 1.308706] \n",
      "[Epoch 258/2200]  [Disc loss: -2.497066] [Gen loss: 1.198558] \n",
      "[Epoch 259/2200]  [Disc loss: -2.401879] [Gen loss: 1.077180] \n",
      "[Epoch 260/2200]  [Disc loss: -2.318964] [Gen loss: 1.507879] \n",
      "[Epoch 261/2200]  [Disc loss: -2.303543] [Gen loss: 1.496685] \n",
      "[Epoch 262/2200]  [Disc loss: -2.211269] [Gen loss: 1.147511] \n",
      "[Epoch 263/2200]  [Disc loss: -2.257532] [Gen loss: 1.120767] \n",
      "[Epoch 264/2200]  [Disc loss: -2.198907] [Gen loss: 1.133123] \n",
      "[Epoch 265/2200]  [Disc loss: -2.068275] [Gen loss: 0.679901] \n",
      "[Epoch 266/2200]  [Disc loss: -1.985042] [Gen loss: 0.872793] \n",
      "[Epoch 267/2200]  [Disc loss: -1.932076] [Gen loss: 1.052629] \n",
      "[Epoch 268/2200]  [Disc loss: -1.937424] [Gen loss: 1.340926] \n",
      "[Epoch 269/2200]  [Disc loss: -1.976456] [Gen loss: 1.144656] \n",
      "[Epoch 270/2200]  [Disc loss: -1.941551] [Gen loss: 1.156511] \n",
      "[Epoch 271/2200]  [Disc loss: -1.968282] [Gen loss: 0.960536] \n",
      "[Epoch 272/2200]  [Disc loss: -1.921433] [Gen loss: 0.854313] \n",
      "[Epoch 273/2200]  [Disc loss: -1.909383] [Gen loss: 0.981030] \n",
      "[Epoch 274/2200]  [Disc loss: -1.826516] [Gen loss: 0.732458] \n",
      "[Epoch 275/2200]  [Disc loss: -1.815717] [Gen loss: 0.609180] \n",
      "[Epoch 276/2200]  [Disc loss: -1.843957] [Gen loss: 0.319889] \n",
      "[Epoch 277/2200]  [Disc loss: -1.941851] [Gen loss: 0.242850] \n",
      "[Epoch 278/2200]  [Disc loss: -1.840467] [Gen loss: -0.177547] \n",
      "[Epoch 279/2200]  [Disc loss: -1.731236] [Gen loss: -0.083945] \n",
      "[Epoch 280/2200]  [Disc loss: -1.752822] [Gen loss: -0.018114] \n",
      "[Epoch 281/2200]  [Disc loss: -1.688568] [Gen loss: -0.023014] \n",
      "[Epoch 282/2200]  [Disc loss: -1.819399] [Gen loss: -0.203845] \n",
      "[Epoch 283/2200]  [Disc loss: -1.796953] [Gen loss: -0.627702] \n",
      "[Epoch 284/2200]  [Disc loss: -1.755891] [Gen loss: -0.326968] \n",
      "[Epoch 285/2200]  [Disc loss: -1.681689] [Gen loss: -0.303226] \n",
      "[Epoch 286/2200]  [Disc loss: -1.718951] [Gen loss: -0.426176] \n",
      "[Epoch 287/2200]  [Disc loss: -1.740280] [Gen loss: -0.899344] \n",
      "[Epoch 288/2200]  [Disc loss: -1.791575] [Gen loss: -0.816408] \n",
      "[Epoch 289/2200]  [Disc loss: -1.793939] [Gen loss: -1.243540] \n",
      "[Epoch 290/2200]  [Disc loss: -1.703684] [Gen loss: -1.243543] \n",
      "[Epoch 291/2200]  [Disc loss: -1.702419] [Gen loss: -1.655199] \n",
      "[Epoch 292/2200]  [Disc loss: -1.704014] [Gen loss: -1.611693] \n",
      "[Epoch 293/2200]  [Disc loss: -1.662315] [Gen loss: -1.663146] \n",
      "[Epoch 294/2200]  [Disc loss: -1.696413] [Gen loss: -1.979790] \n",
      "[Epoch 295/2200]  [Disc loss: -1.673304] [Gen loss: -2.043141] \n",
      "[Epoch 296/2200]  [Disc loss: -1.639915] [Gen loss: -2.118345] \n",
      "[Epoch 297/2200]  [Disc loss: -1.688391] [Gen loss: -2.659922] \n",
      "[Epoch 298/2200]  [Disc loss: -1.622955] [Gen loss: -2.853716] \n",
      "[Epoch 299/2200]  [Disc loss: -1.572950] [Gen loss: -3.128155] \n",
      "[Epoch 300/2200]  [Disc loss: -1.566193] [Gen loss: -3.658693] \n",
      "[Epoch 301/2200]  [Disc loss: -1.491941] [Gen loss: -3.442438] \n",
      "[Epoch 302/2200]  [Disc loss: -1.516764] [Gen loss: -3.569192] \n",
      "[Epoch 303/2200]  [Disc loss: -1.524392] [Gen loss: -3.694869] \n",
      "[Epoch 304/2200]  [Disc loss: -1.501938] [Gen loss: -3.712918] \n",
      "[Epoch 305/2200]  [Disc loss: -1.513721] [Gen loss: -3.960139] \n",
      "[Epoch 306/2200]  [Disc loss: -1.551051] [Gen loss: -4.128880] \n",
      "[Epoch 307/2200]  [Disc loss: -1.526078] [Gen loss: -4.289722] \n",
      "[Epoch 308/2200]  [Disc loss: -1.514038] [Gen loss: -4.346810] \n",
      "[Epoch 309/2200]  [Disc loss: -1.521941] [Gen loss: -4.293392] \n",
      "[Epoch 310/2200]  [Disc loss: -1.446084] [Gen loss: -4.232822] \n",
      "[Epoch 311/2200]  [Disc loss: -1.429348] [Gen loss: -4.397637] \n",
      "[Epoch 312/2200]  [Disc loss: -1.387050] [Gen loss: -4.588672] \n",
      "[Epoch 313/2200]  [Disc loss: -1.373814] [Gen loss: -4.671765] \n",
      "[Epoch 314/2200]  [Disc loss: -1.374680] [Gen loss: -4.840564] \n",
      "[Epoch 315/2200]  [Disc loss: -1.422158] [Gen loss: -5.168292] \n",
      "[Epoch 316/2200]  [Disc loss: -1.433068] [Gen loss: -5.086351] \n",
      "[Epoch 317/2200]  [Disc loss: -1.416262] [Gen loss: -4.955817] \n",
      "[Epoch 318/2200]  [Disc loss: -1.387504] [Gen loss: -5.074335] \n",
      "[Epoch 319/2200]  [Disc loss: -1.361992] [Gen loss: -5.100768] \n",
      "[Epoch 320/2200]  [Disc loss: -1.364667] [Gen loss: -5.200928] \n",
      "[Epoch 321/2200]  [Disc loss: -1.347440] [Gen loss: -5.175721] \n",
      "[Epoch 322/2200]  [Disc loss: -1.309388] [Gen loss: -4.982399] \n",
      "[Epoch 323/2200]  [Disc loss: -1.333518] [Gen loss: -4.685997] \n",
      "[Epoch 324/2200]  [Disc loss: -1.342177] [Gen loss: -4.498147] \n",
      "[Epoch 325/2200]  [Disc loss: -1.322499] [Gen loss: -4.856684] \n",
      "[Epoch 326/2200]  [Disc loss: -1.310046] [Gen loss: -4.404205] \n",
      "[Epoch 327/2200]  [Disc loss: -1.270514] [Gen loss: -4.531365] \n",
      "[Epoch 328/2200]  [Disc loss: -1.280817] [Gen loss: -4.527577] \n",
      "[Epoch 329/2200]  [Disc loss: -1.276020] [Gen loss: -4.586621] \n",
      "[Epoch 330/2200]  [Disc loss: -1.260365] [Gen loss: -4.430836] \n",
      "[Epoch 331/2200]  [Disc loss: -1.229136] [Gen loss: -4.443196] \n",
      "[Epoch 332/2200]  [Disc loss: -1.250140] [Gen loss: -4.407587] \n",
      "[Epoch 333/2200]  [Disc loss: -1.255044] [Gen loss: -4.334491] \n",
      "[Epoch 334/2200]  [Disc loss: -1.215114] [Gen loss: -4.330167] \n",
      "[Epoch 335/2200]  [Disc loss: -1.228758] [Gen loss: -4.029363] \n",
      "[Epoch 336/2200]  [Disc loss: -1.200103] [Gen loss: -4.051472] \n",
      "[Epoch 337/2200]  [Disc loss: -1.195324] [Gen loss: -3.921735] \n",
      "[Epoch 338/2200]  [Disc loss: -1.190208] [Gen loss: -4.108174] \n",
      "[Epoch 339/2200]  [Disc loss: -1.161266] [Gen loss: -3.837571] \n",
      "[Epoch 340/2200]  [Disc loss: -1.163637] [Gen loss: -4.168358] \n",
      "[Epoch 341/2200]  [Disc loss: -1.152795] [Gen loss: -4.073507] \n",
      "[Epoch 342/2200]  [Disc loss: -1.138790] [Gen loss: -4.077219] \n",
      "[Epoch 343/2200]  [Disc loss: -1.101112] [Gen loss: -3.802134] \n",
      "[Epoch 344/2200]  [Disc loss: -1.122136] [Gen loss: -3.774379] \n",
      "[Epoch 345/2200]  [Disc loss: -1.110178] [Gen loss: -3.819111] \n",
      "[Epoch 346/2200]  [Disc loss: -1.104577] [Gen loss: -3.841465] \n",
      "[Epoch 347/2200]  [Disc loss: -1.092772] [Gen loss: -3.424379] \n",
      "[Epoch 348/2200]  [Disc loss: -1.084623] [Gen loss: -3.819793] \n",
      "[Epoch 349/2200]  [Disc loss: -1.074420] [Gen loss: -3.408955] \n",
      "[Epoch 350/2200]  [Disc loss: -1.072435] [Gen loss: -3.465896] \n",
      "[Epoch 351/2200]  [Disc loss: -1.069377] [Gen loss: -3.504648] \n",
      "[Epoch 352/2200]  [Disc loss: -1.065971] [Gen loss: -3.307632] \n",
      "[Epoch 353/2200]  [Disc loss: -1.048212] [Gen loss: -3.509270] \n",
      "[Epoch 354/2200]  [Disc loss: -1.044946] [Gen loss: -3.855411] \n",
      "[Epoch 355/2200]  [Disc loss: -1.026497] [Gen loss: -3.405024] \n",
      "[Epoch 356/2200]  [Disc loss: -1.029200] [Gen loss: -3.146929] \n",
      "[Epoch 357/2200]  [Disc loss: -1.005153] [Gen loss: -3.295963] \n",
      "[Epoch 358/2200]  [Disc loss: -1.007252] [Gen loss: -3.367899] \n",
      "[Epoch 359/2200]  [Disc loss: -0.991993] [Gen loss: -3.118472] \n",
      "[Epoch 360/2200]  [Disc loss: -0.984504] [Gen loss: -3.037322] \n",
      "[Epoch 361/2200]  [Disc loss: -0.969356] [Gen loss: -3.300383] \n",
      "[Epoch 362/2200]  [Disc loss: -0.977698] [Gen loss: -3.117788] \n",
      "[Epoch 363/2200]  [Disc loss: -0.952167] [Gen loss: -3.087108] \n",
      "[Epoch 364/2200]  [Disc loss: -0.951162] [Gen loss: -3.008841] \n",
      "[Epoch 365/2200]  [Disc loss: -0.944107] [Gen loss: -2.699324] \n",
      "[Epoch 366/2200]  [Disc loss: -0.935090] [Gen loss: -2.973713] \n",
      "[Epoch 367/2200]  [Disc loss: -0.941787] [Gen loss: -2.982509] \n",
      "[Epoch 368/2200]  [Disc loss: -0.909521] [Gen loss: -2.927888] \n",
      "[Epoch 369/2200]  [Disc loss: -0.920121] [Gen loss: -2.608974] \n",
      "[Epoch 370/2200]  [Disc loss: -0.908981] [Gen loss: -2.582179] \n",
      "[Epoch 371/2200]  [Disc loss: -0.901117] [Gen loss: -2.567744] \n",
      "[Epoch 372/2200]  [Disc loss: -0.891893] [Gen loss: -2.681151] \n",
      "[Epoch 373/2200]  [Disc loss: -0.885372] [Gen loss: -2.635534] \n",
      "[Epoch 374/2200]  [Disc loss: -0.879748] [Gen loss: -2.963879] \n",
      "[Epoch 375/2200]  [Disc loss: -0.893062] [Gen loss: -2.831360] \n",
      "[Epoch 376/2200]  [Disc loss: -0.874580] [Gen loss: -2.669862] \n",
      "[Epoch 377/2200]  [Disc loss: -0.860998] [Gen loss: -2.389403] \n",
      "[Epoch 378/2200]  [Disc loss: -0.859688] [Gen loss: -2.671691] \n",
      "[Epoch 379/2200]  [Disc loss: -0.864611] [Gen loss: -2.592614] \n",
      "[Epoch 380/2200]  [Disc loss: -0.848549] [Gen loss: -2.740215] \n",
      "[Epoch 381/2200]  [Disc loss: -0.855337] [Gen loss: -2.763658] \n",
      "[Epoch 382/2200]  [Disc loss: -0.841731] [Gen loss: -2.467003] \n",
      "[Epoch 383/2200]  [Disc loss: -0.836354] [Gen loss: -2.649811] \n",
      "[Epoch 384/2200]  [Disc loss: -0.850290] [Gen loss: -2.749975] \n",
      "[Epoch 385/2200]  [Disc loss: -0.829359] [Gen loss: -2.459538] \n",
      "[Epoch 386/2200]  [Disc loss: -0.836854] [Gen loss: -2.547627] \n",
      "[Epoch 387/2200]  [Disc loss: -0.837234] [Gen loss: -2.500879] \n",
      "[Epoch 388/2200]  [Disc loss: -0.818633] [Gen loss: -2.836128] \n",
      "[Epoch 389/2200]  [Disc loss: -0.834794] [Gen loss: -2.501917] \n",
      "[Epoch 390/2200]  [Disc loss: -0.807582] [Gen loss: -2.496119] \n",
      "[Epoch 391/2200]  [Disc loss: -0.824904] [Gen loss: -2.427961] \n",
      "[Epoch 392/2200]  [Disc loss: -0.837709] [Gen loss: -2.848602] \n",
      "[Epoch 393/2200]  [Disc loss: -0.838751] [Gen loss: -2.527502] \n",
      "[Epoch 394/2200]  [Disc loss: -0.817339] [Gen loss: -2.239550] \n",
      "[Epoch 395/2200]  [Disc loss: -0.810167] [Gen loss: -2.303991] \n",
      "[Epoch 396/2200]  [Disc loss: -0.821140] [Gen loss: -2.639476] \n",
      "[Epoch 397/2200]  [Disc loss: -0.808097] [Gen loss: -2.145336] \n",
      "[Epoch 398/2200]  [Disc loss: -0.828246] [Gen loss: -2.245388] \n",
      "[Epoch 399/2200]  [Disc loss: -0.823409] [Gen loss: -2.342989] \n",
      "[Epoch 400/2200]  [Disc loss: -0.832846] [Gen loss: -2.311007] \n",
      "[Epoch 401/2200]  [Disc loss: -0.824820] [Gen loss: -2.075441] \n",
      "[Epoch 402/2200]  [Disc loss: -0.826607] [Gen loss: -2.476459] \n",
      "[Epoch 403/2200]  [Disc loss: -0.823277] [Gen loss: -2.074825] \n",
      "[Epoch 404/2200]  [Disc loss: -0.823688] [Gen loss: -2.228415] \n",
      "[Epoch 405/2200]  [Disc loss: -0.795553] [Gen loss: -2.369539] \n",
      "[Epoch 406/2200]  [Disc loss: -0.817262] [Gen loss: -2.151992] \n",
      "[Epoch 407/2200]  [Disc loss: -0.822119] [Gen loss: -2.062857] \n",
      "[Epoch 408/2200]  [Disc loss: -0.819470] [Gen loss: -1.697990] \n",
      "[Epoch 409/2200]  [Disc loss: -0.824053] [Gen loss: -1.770488] \n",
      "[Epoch 410/2200]  [Disc loss: -0.825224] [Gen loss: -1.518713] \n",
      "[Epoch 411/2200]  [Disc loss: -0.825335] [Gen loss: -1.537574] \n",
      "[Epoch 412/2200]  [Disc loss: -0.823908] [Gen loss: -1.606377] \n",
      "[Epoch 413/2200]  [Disc loss: -0.813928] [Gen loss: -1.764885] \n",
      "[Epoch 414/2200]  [Disc loss: -0.820696] [Gen loss: -1.844366] \n",
      "[Epoch 415/2200]  [Disc loss: -0.817220] [Gen loss: -1.952701] \n",
      "[Epoch 416/2200]  [Disc loss: -0.823976] [Gen loss: -1.611307] \n",
      "[Epoch 417/2200]  [Disc loss: -0.818498] [Gen loss: -1.793460] \n",
      "[Epoch 418/2200]  [Disc loss: -0.796674] [Gen loss: -1.612708] \n",
      "[Epoch 419/2200]  [Disc loss: -0.806508] [Gen loss: -1.780335] \n",
      "[Epoch 420/2200]  [Disc loss: -0.808009] [Gen loss: -1.772609] \n",
      "[Epoch 421/2200]  [Disc loss: -0.814088] [Gen loss: -1.592527] \n",
      "[Epoch 422/2200]  [Disc loss: -0.818590] [Gen loss: -1.432064] \n",
      "[Epoch 423/2200]  [Disc loss: -0.828364] [Gen loss: -1.528806] \n",
      "[Epoch 424/2200]  [Disc loss: -0.821622] [Gen loss: -1.494337] \n",
      "[Epoch 425/2200]  [Disc loss: -0.818598] [Gen loss: -1.249477] \n",
      "[Epoch 426/2200]  [Disc loss: -0.815542] [Gen loss: -1.279876] \n",
      "[Epoch 427/2200]  [Disc loss: -0.816349] [Gen loss: -1.480220] \n",
      "[Epoch 428/2200]  [Disc loss: -0.820743] [Gen loss: -1.395891] \n",
      "[Epoch 429/2200]  [Disc loss: -0.833733] [Gen loss: -1.567581] \n",
      "[Epoch 430/2200]  [Disc loss: -0.829782] [Gen loss: -1.221283] \n",
      "[Epoch 431/2200]  [Disc loss: -0.802914] [Gen loss: -1.096634] \n",
      "[Epoch 432/2200]  [Disc loss: -0.827347] [Gen loss: -1.218699] \n",
      "[Epoch 433/2200]  [Disc loss: -0.804905] [Gen loss: -1.227813] \n",
      "[Epoch 434/2200]  [Disc loss: -0.823692] [Gen loss: -1.482976] \n",
      "[Epoch 435/2200]  [Disc loss: -0.827412] [Gen loss: -1.413378] \n",
      "[Epoch 436/2200]  [Disc loss: -0.825503] [Gen loss: -1.291666] \n",
      "[Epoch 437/2200]  [Disc loss: -0.820983] [Gen loss: -1.526166] \n",
      "[Epoch 438/2200]  [Disc loss: -0.818548] [Gen loss: -1.383940] \n",
      "[Epoch 439/2200]  [Disc loss: -0.832288] [Gen loss: -1.423372] \n",
      "[Epoch 440/2200]  [Disc loss: -0.833644] [Gen loss: -1.389178] \n",
      "[Epoch 441/2200]  [Disc loss: -0.824986] [Gen loss: -1.533628] \n",
      "[Epoch 442/2200]  [Disc loss: -0.803505] [Gen loss: -1.501200] \n",
      "[Epoch 443/2200]  [Disc loss: -0.822578] [Gen loss: -1.497213] \n",
      "[Epoch 444/2200]  [Disc loss: -0.836247] [Gen loss: -1.672759] \n",
      "[Epoch 445/2200]  [Disc loss: -0.820425] [Gen loss: -1.672601] \n",
      "[Epoch 446/2200]  [Disc loss: -0.820749] [Gen loss: -1.618701] \n",
      "[Epoch 447/2200]  [Disc loss: -0.818050] [Gen loss: -1.446306] \n",
      "[Epoch 448/2200]  [Disc loss: -0.835135] [Gen loss: -1.493621] \n",
      "[Epoch 449/2200]  [Disc loss: -0.828219] [Gen loss: -1.478759] \n",
      "[Epoch 450/2200]  [Disc loss: -0.831511] [Gen loss: -1.643534] \n",
      "[Epoch 451/2200]  [Disc loss: -0.826069] [Gen loss: -1.663119] \n",
      "[Epoch 452/2200]  [Disc loss: -0.838729] [Gen loss: -1.754108] \n",
      "[Epoch 453/2200]  [Disc loss: -0.831636] [Gen loss: -1.665094] \n",
      "[Epoch 454/2200]  [Disc loss: -0.834038] [Gen loss: -1.737509] \n",
      "[Epoch 455/2200]  [Disc loss: -0.835726] [Gen loss: -1.592845] \n",
      "[Epoch 456/2200]  [Disc loss: -0.838243] [Gen loss: -1.536121] \n",
      "[Epoch 457/2200]  [Disc loss: -0.838204] [Gen loss: -1.698057] \n",
      "[Epoch 458/2200]  [Disc loss: -0.823130] [Gen loss: -1.346567] \n",
      "[Epoch 459/2200]  [Disc loss: -0.832203] [Gen loss: -1.552347] \n",
      "[Epoch 460/2200]  [Disc loss: -0.822989] [Gen loss: -1.487147] \n",
      "[Epoch 461/2200]  [Disc loss: -0.818036] [Gen loss: -1.514173] \n",
      "[Epoch 462/2200]  [Disc loss: -0.819561] [Gen loss: -1.350513] \n",
      "[Epoch 463/2200]  [Disc loss: -0.833391] [Gen loss: -1.368564] \n",
      "[Epoch 464/2200]  [Disc loss: -0.825165] [Gen loss: -1.571428] \n",
      "[Epoch 465/2200]  [Disc loss: -0.804409] [Gen loss: -1.071022] \n",
      "[Epoch 466/2200]  [Disc loss: -0.827138] [Gen loss: -1.332018] \n",
      "[Epoch 467/2200]  [Disc loss: -0.829010] [Gen loss: -1.241086] \n",
      "[Epoch 468/2200]  [Disc loss: -0.815932] [Gen loss: -1.280270] \n",
      "[Epoch 469/2200]  [Disc loss: -0.825365] [Gen loss: -1.460167] \n",
      "[Epoch 470/2200]  [Disc loss: -0.816865] [Gen loss: -1.145733] \n",
      "[Epoch 471/2200]  [Disc loss: -0.810981] [Gen loss: -1.367886] \n",
      "[Epoch 472/2200]  [Disc loss: -0.817054] [Gen loss: -1.380276] \n",
      "[Epoch 473/2200]  [Disc loss: -0.805953] [Gen loss: -1.335856] \n",
      "[Epoch 474/2200]  [Disc loss: -0.817744] [Gen loss: -1.236178] \n",
      "[Epoch 475/2200]  [Disc loss: -0.804573] [Gen loss: -1.337008] \n",
      "[Epoch 476/2200]  [Disc loss: -0.802678] [Gen loss: -1.285317] \n",
      "[Epoch 477/2200]  [Disc loss: -0.813109] [Gen loss: -1.173015] \n",
      "[Epoch 478/2200]  [Disc loss: -0.796013] [Gen loss: -1.127179] \n",
      "[Epoch 479/2200]  [Disc loss: -0.810877] [Gen loss: -1.129511] \n",
      "[Epoch 480/2200]  [Disc loss: -0.792252] [Gen loss: -1.224224] \n",
      "[Epoch 481/2200]  [Disc loss: -0.803120] [Gen loss: -1.142106] \n",
      "[Epoch 482/2200]  [Disc loss: -0.795821] [Gen loss: -1.225928] \n",
      "[Epoch 483/2200]  [Disc loss: -0.798514] [Gen loss: -1.059758] \n",
      "[Epoch 484/2200]  [Disc loss: -0.799865] [Gen loss: -1.288540] \n",
      "[Epoch 485/2200]  [Disc loss: -0.791534] [Gen loss: -1.037686] \n",
      "[Epoch 486/2200]  [Disc loss: -0.791363] [Gen loss: -1.078362] \n",
      "[Epoch 487/2200]  [Disc loss: -0.793754] [Gen loss: -0.793979] \n",
      "[Epoch 488/2200]  [Disc loss: -0.789410] [Gen loss: -0.911032] \n",
      "[Epoch 489/2200]  [Disc loss: -0.784522] [Gen loss: -0.996996] \n",
      "[Epoch 490/2200]  [Disc loss: -0.788312] [Gen loss: -0.939743] \n",
      "[Epoch 491/2200]  [Disc loss: -0.782592] [Gen loss: -0.905510] \n",
      "[Epoch 492/2200]  [Disc loss: -0.799820] [Gen loss: -1.061881] \n",
      "[Epoch 493/2200]  [Disc loss: -0.773269] [Gen loss: -0.766492] \n",
      "[Epoch 494/2200]  [Disc loss: -0.783265] [Gen loss: -0.588051] \n",
      "[Epoch 495/2200]  [Disc loss: -0.774678] [Gen loss: -0.599555] \n",
      "[Epoch 496/2200]  [Disc loss: -0.776539] [Gen loss: -0.570309] \n",
      "[Epoch 497/2200]  [Disc loss: -0.785334] [Gen loss: -0.756488] \n",
      "[Epoch 498/2200]  [Disc loss: -0.774489] [Gen loss: -0.389916] \n",
      "[Epoch 499/2200]  [Disc loss: -0.776400] [Gen loss: -0.379551] \n",
      "[Epoch 500/2200]  [Disc loss: -0.768148] [Gen loss: -0.265743] \n",
      "[Epoch 501/2200]  [Disc loss: -0.780922] [Gen loss: -0.270625] \n",
      "[Epoch 502/2200]  [Disc loss: -0.775438] [Gen loss: -0.329315] \n",
      "[Epoch 503/2200]  [Disc loss: -0.759932] [Gen loss: -0.179925] \n",
      "[Epoch 504/2200]  [Disc loss: -0.766441] [Gen loss: -0.153211] \n",
      "[Epoch 505/2200]  [Disc loss: -0.771429] [Gen loss: -0.384034] \n",
      "[Epoch 506/2200]  [Disc loss: -0.765286] [Gen loss: -0.205484] \n",
      "[Epoch 507/2200]  [Disc loss: -0.767552] [Gen loss: -0.039072] \n",
      "[Epoch 508/2200]  [Disc loss: -0.773129] [Gen loss: 0.129722] \n",
      "[Epoch 509/2200]  [Disc loss: -0.760854] [Gen loss: -0.053491] \n",
      "[Epoch 510/2200]  [Disc loss: -0.758060] [Gen loss: 0.201179] \n",
      "[Epoch 511/2200]  [Disc loss: -0.763878] [Gen loss: 0.016165] \n",
      "[Epoch 512/2200]  [Disc loss: -0.761093] [Gen loss: -0.048001] \n",
      "[Epoch 513/2200]  [Disc loss: -0.759431] [Gen loss: 0.229217] \n",
      "[Epoch 514/2200]  [Disc loss: -0.757967] [Gen loss: 0.392639] \n",
      "[Epoch 515/2200]  [Disc loss: -0.759755] [Gen loss: 0.187658] \n",
      "[Epoch 516/2200]  [Disc loss: -0.759461] [Gen loss: 0.243622] \n",
      "[Epoch 517/2200]  [Disc loss: -0.762528] [Gen loss: 0.524230] \n",
      "[Epoch 518/2200]  [Disc loss: -0.756341] [Gen loss: 0.370916] \n",
      "[Epoch 519/2200]  [Disc loss: -0.756488] [Gen loss: 0.578812] \n",
      "[Epoch 520/2200]  [Disc loss: -0.770162] [Gen loss: 0.529597] \n",
      "[Epoch 521/2200]  [Disc loss: -0.754033] [Gen loss: 0.755539] \n",
      "[Epoch 522/2200]  [Disc loss: -0.754852] [Gen loss: 0.747610] \n",
      "[Epoch 523/2200]  [Disc loss: -0.748728] [Gen loss: 0.680489] \n",
      "[Epoch 524/2200]  [Disc loss: -0.748318] [Gen loss: 0.904822] \n",
      "[Epoch 525/2200]  [Disc loss: -0.763740] [Gen loss: 0.732898] \n",
      "[Epoch 526/2200]  [Disc loss: -0.749974] [Gen loss: 0.812757] \n",
      "[Epoch 527/2200]  [Disc loss: -0.753341] [Gen loss: 0.738265] \n",
      "[Epoch 528/2200]  [Disc loss: -0.748641] [Gen loss: 1.062800] \n",
      "[Epoch 529/2200]  [Disc loss: -0.752687] [Gen loss: 0.919534] \n",
      "[Epoch 530/2200]  [Disc loss: -0.755549] [Gen loss: 0.978567] \n",
      "[Epoch 531/2200]  [Disc loss: -0.743763] [Gen loss: 1.000889] \n",
      "[Epoch 532/2200]  [Disc loss: -0.751441] [Gen loss: 1.233049] \n",
      "[Epoch 533/2200]  [Disc loss: -0.751529] [Gen loss: 1.067545] \n",
      "[Epoch 534/2200]  [Disc loss: -0.742756] [Gen loss: 1.102067] \n",
      "[Epoch 535/2200]  [Disc loss: -0.756569] [Gen loss: 1.293265] \n",
      "[Epoch 536/2200]  [Disc loss: -0.744124] [Gen loss: 1.029816] \n",
      "[Epoch 537/2200]  [Disc loss: -0.738163] [Gen loss: 1.349131] \n",
      "[Epoch 538/2200]  [Disc loss: -0.749573] [Gen loss: 1.418231] \n",
      "[Epoch 539/2200]  [Disc loss: -0.752078] [Gen loss: 1.436919] \n",
      "[Epoch 540/2200]  [Disc loss: -0.744111] [Gen loss: 1.489065] \n",
      "[Epoch 541/2200]  [Disc loss: -0.748219] [Gen loss: 1.532228] \n",
      "[Epoch 542/2200]  [Disc loss: -0.742873] [Gen loss: 1.565326] \n",
      "[Epoch 543/2200]  [Disc loss: -0.739115] [Gen loss: 1.830625] \n",
      "[Epoch 544/2200]  [Disc loss: -0.739183] [Gen loss: 1.590131] \n",
      "[Epoch 545/2200]  [Disc loss: -0.737214] [Gen loss: 1.615922] \n",
      "[Epoch 546/2200]  [Disc loss: -0.745708] [Gen loss: 1.710002] \n",
      "[Epoch 547/2200]  [Disc loss: -0.744368] [Gen loss: 1.837503] \n",
      "[Epoch 548/2200]  [Disc loss: -0.730872] [Gen loss: 1.837494] \n",
      "[Epoch 549/2200]  [Disc loss: -0.740976] [Gen loss: 1.799195] \n",
      "[Epoch 550/2200]  [Disc loss: -0.736176] [Gen loss: 2.011450] \n",
      "[Epoch 551/2200]  [Disc loss: -0.742459] [Gen loss: 1.921789] \n",
      "[Epoch 552/2200]  [Disc loss: -0.736961] [Gen loss: 2.077532] \n",
      "[Epoch 553/2200]  [Disc loss: -0.740314] [Gen loss: 1.935221] \n",
      "[Epoch 554/2200]  [Disc loss: -0.736726] [Gen loss: 2.264963] \n",
      "[Epoch 555/2200]  [Disc loss: -0.739223] [Gen loss: 2.154356] \n",
      "[Epoch 556/2200]  [Disc loss: -0.737252] [Gen loss: 2.090648] \n",
      "[Epoch 557/2200]  [Disc loss: -0.737334] [Gen loss: 2.111667] \n",
      "[Epoch 558/2200]  [Disc loss: -0.730448] [Gen loss: 2.090948] \n",
      "[Epoch 559/2200]  [Disc loss: -0.733255] [Gen loss: 2.294806] \n",
      "[Epoch 560/2200]  [Disc loss: -0.736818] [Gen loss: 2.205024] \n",
      "[Epoch 561/2200]  [Disc loss: -0.729793] [Gen loss: 2.001240] \n",
      "[Epoch 562/2200]  [Disc loss: -0.730606] [Gen loss: 2.172125] \n",
      "[Epoch 563/2200]  [Disc loss: -0.734192] [Gen loss: 2.133560] \n",
      "[Epoch 564/2200]  [Disc loss: -0.727973] [Gen loss: 2.063974] \n",
      "[Epoch 565/2200]  [Disc loss: -0.724301] [Gen loss: 2.133815] \n",
      "[Epoch 566/2200]  [Disc loss: -0.726533] [Gen loss: 2.097303] \n",
      "[Epoch 567/2200]  [Disc loss: -0.720146] [Gen loss: 2.263879] \n",
      "[Epoch 568/2200]  [Disc loss: -0.723570] [Gen loss: 2.285886] \n",
      "[Epoch 569/2200]  [Disc loss: -0.728088] [Gen loss: 2.229420] \n",
      "[Epoch 570/2200]  [Disc loss: -0.723095] [Gen loss: 2.215694] \n",
      "[Epoch 571/2200]  [Disc loss: -0.724175] [Gen loss: 2.330273] \n",
      "[Epoch 572/2200]  [Disc loss: -0.728550] [Gen loss: 2.294064] \n",
      "[Epoch 573/2200]  [Disc loss: -0.714479] [Gen loss: 2.478621] \n",
      "[Epoch 574/2200]  [Disc loss: -0.719469] [Gen loss: 2.255154] \n",
      "[Epoch 575/2200]  [Disc loss: -0.723779] [Gen loss: 2.426186] \n",
      "[Epoch 576/2200]  [Disc loss: -0.713551] [Gen loss: 2.567688] \n",
      "[Epoch 577/2200]  [Disc loss: -0.718449] [Gen loss: 2.612807] \n",
      "[Epoch 578/2200]  [Disc loss: -0.714457] [Gen loss: 2.301268] \n",
      "[Epoch 579/2200]  [Disc loss: -0.716242] [Gen loss: 2.587933] \n",
      "[Epoch 580/2200]  [Disc loss: -0.716543] [Gen loss: 2.397465] \n",
      "[Epoch 581/2200]  [Disc loss: -0.710360] [Gen loss: 2.511847] \n",
      "[Epoch 582/2200]  [Disc loss: -0.715955] [Gen loss: 2.384679] \n",
      "[Epoch 583/2200]  [Disc loss: -0.712157] [Gen loss: 2.401055] \n",
      "[Epoch 584/2200]  [Disc loss: -0.708572] [Gen loss: 2.612587] \n",
      "[Epoch 585/2200]  [Disc loss: -0.707039] [Gen loss: 2.585385] \n",
      "[Epoch 586/2200]  [Disc loss: -0.717134] [Gen loss: 2.406567] \n",
      "[Epoch 587/2200]  [Disc loss: -0.704547] [Gen loss: 2.534497] \n",
      "[Epoch 588/2200]  [Disc loss: -0.710785] [Gen loss: 2.605945] \n",
      "[Epoch 589/2200]  [Disc loss: -0.705150] [Gen loss: 2.466588] \n",
      "[Epoch 590/2200]  [Disc loss: -0.703013] [Gen loss: 2.419451] \n",
      "[Epoch 591/2200]  [Disc loss: -0.704421] [Gen loss: 2.648960] \n",
      "[Epoch 592/2200]  [Disc loss: -0.702808] [Gen loss: 2.474509] \n",
      "[Epoch 593/2200]  [Disc loss: -0.705231] [Gen loss: 2.492649] \n",
      "[Epoch 594/2200]  [Disc loss: -0.712041] [Gen loss: 2.558961] \n",
      "[Epoch 595/2200]  [Disc loss: -0.701507] [Gen loss: 2.659326] \n",
      "[Epoch 596/2200]  [Disc loss: -0.709391] [Gen loss: 2.757900] \n",
      "[Epoch 597/2200]  [Disc loss: -0.702600] [Gen loss: 2.502431] \n",
      "[Epoch 598/2200]  [Disc loss: -0.701615] [Gen loss: 2.730468] \n",
      "[Epoch 599/2200]  [Disc loss: -0.703371] [Gen loss: 2.441603] \n",
      "[Epoch 600/2200]  [Disc loss: -0.702555] [Gen loss: 2.650876] \n",
      "[Epoch 601/2200]  [Disc loss: -0.701005] [Gen loss: 2.576160] \n",
      "[Epoch 602/2200]  [Disc loss: -0.707117] [Gen loss: 2.384127] \n",
      "[Epoch 603/2200]  [Disc loss: -0.692288] [Gen loss: 2.408761] \n",
      "[Epoch 604/2200]  [Disc loss: -0.699450] [Gen loss: 2.662318] \n",
      "[Epoch 605/2200]  [Disc loss: -0.706328] [Gen loss: 2.514139] \n",
      "[Epoch 606/2200]  [Disc loss: -0.693676] [Gen loss: 2.576729] \n",
      "[Epoch 607/2200]  [Disc loss: -0.697507] [Gen loss: 2.630642] \n",
      "[Epoch 608/2200]  [Disc loss: -0.703054] [Gen loss: 2.546436] \n",
      "[Epoch 609/2200]  [Disc loss: -0.701815] [Gen loss: 2.637831] \n",
      "[Epoch 610/2200]  [Disc loss: -0.701636] [Gen loss: 2.468956] \n",
      "[Epoch 611/2200]  [Disc loss: -0.700717] [Gen loss: 2.728492] \n",
      "[Epoch 612/2200]  [Disc loss: -0.703720] [Gen loss: 2.682989] \n",
      "[Epoch 613/2200]  [Disc loss: -0.699272] [Gen loss: 2.679105] \n",
      "[Epoch 614/2200]  [Disc loss: -0.697918] [Gen loss: 2.549998] \n",
      "[Epoch 615/2200]  [Disc loss: -0.696462] [Gen loss: 2.702975] \n",
      "[Epoch 616/2200]  [Disc loss: -0.700856] [Gen loss: 2.588302] \n",
      "[Epoch 617/2200]  [Disc loss: -0.695155] [Gen loss: 2.632350] \n",
      "[Epoch 618/2200]  [Disc loss: -0.696198] [Gen loss: 2.631972] \n",
      "[Epoch 619/2200]  [Disc loss: -0.697514] [Gen loss: 2.661170] \n",
      "[Epoch 620/2200]  [Disc loss: -0.697217] [Gen loss: 2.409304] \n",
      "[Epoch 621/2200]  [Disc loss: -0.702455] [Gen loss: 2.563832] \n",
      "[Epoch 622/2200]  [Disc loss: -0.695323] [Gen loss: 2.680182] \n",
      "[Epoch 623/2200]  [Disc loss: -0.690162] [Gen loss: 2.500806] \n",
      "[Epoch 624/2200]  [Disc loss: -0.694768] [Gen loss: 2.582284] \n",
      "[Epoch 625/2200]  [Disc loss: -0.696603] [Gen loss: 2.500105] \n",
      "[Epoch 626/2200]  [Disc loss: -0.698741] [Gen loss: 2.563338] \n",
      "[Epoch 627/2200]  [Disc loss: -0.688907] [Gen loss: 2.525167] \n",
      "[Epoch 628/2200]  [Disc loss: -0.690816] [Gen loss: 2.569791] \n",
      "[Epoch 629/2200]  [Disc loss: -0.694468] [Gen loss: 2.630482] \n",
      "[Epoch 630/2200]  [Disc loss: -0.691609] [Gen loss: 2.659444] \n",
      "[Epoch 631/2200]  [Disc loss: -0.690946] [Gen loss: 2.524413] \n",
      "[Epoch 632/2200]  [Disc loss: -0.694415] [Gen loss: 2.558736] \n",
      "[Epoch 633/2200]  [Disc loss: -0.690411] [Gen loss: 2.606745] \n",
      "[Epoch 634/2200]  [Disc loss: -0.696608] [Gen loss: 2.608481] \n",
      "[Epoch 635/2200]  [Disc loss: -0.690396] [Gen loss: 2.745299] \n",
      "[Epoch 636/2200]  [Disc loss: -0.685919] [Gen loss: 2.527660] \n",
      "[Epoch 637/2200]  [Disc loss: -0.695256] [Gen loss: 2.678278] \n",
      "[Epoch 638/2200]  [Disc loss: -0.695112] [Gen loss: 2.766417] \n",
      "[Epoch 639/2200]  [Disc loss: -0.691702] [Gen loss: 2.542970] \n",
      "[Epoch 640/2200]  [Disc loss: -0.688491] [Gen loss: 2.597271] \n",
      "[Epoch 641/2200]  [Disc loss: -0.685768] [Gen loss: 2.745403] \n",
      "[Epoch 642/2200]  [Disc loss: -0.693325] [Gen loss: 2.649741] \n",
      "[Epoch 643/2200]  [Disc loss: -0.684172] [Gen loss: 2.763941] \n",
      "[Epoch 644/2200]  [Disc loss: -0.694167] [Gen loss: 2.654984] \n",
      "[Epoch 645/2200]  [Disc loss: -0.690164] [Gen loss: 2.678713] \n",
      "[Epoch 646/2200]  [Disc loss: -0.686222] [Gen loss: 2.799130] \n",
      "[Epoch 647/2200]  [Disc loss: -0.692155] [Gen loss: 2.762336] \n",
      "[Epoch 648/2200]  [Disc loss: -0.689744] [Gen loss: 2.645545] \n",
      "[Epoch 649/2200]  [Disc loss: -0.688803] [Gen loss: 2.702299] \n",
      "[Epoch 650/2200]  [Disc loss: -0.687601] [Gen loss: 2.571638] \n",
      "[Epoch 651/2200]  [Disc loss: -0.694406] [Gen loss: 2.699853] \n",
      "[Epoch 652/2200]  [Disc loss: -0.692576] [Gen loss: 2.694856] \n",
      "[Epoch 653/2200]  [Disc loss: -0.687328] [Gen loss: 2.716950] \n",
      "[Epoch 654/2200]  [Disc loss: -0.683468] [Gen loss: 2.591871] \n",
      "[Epoch 655/2200]  [Disc loss: -0.688937] [Gen loss: 2.648178] \n",
      "[Epoch 656/2200]  [Disc loss: -0.686475] [Gen loss: 2.717277] \n",
      "[Epoch 657/2200]  [Disc loss: -0.689402] [Gen loss: 2.677516] \n",
      "[Epoch 658/2200]  [Disc loss: -0.684034] [Gen loss: 2.785620] \n",
      "[Epoch 659/2200]  [Disc loss: -0.687672] [Gen loss: 2.708038] \n",
      "[Epoch 660/2200]  [Disc loss: -0.688908] [Gen loss: 2.821154] \n",
      "[Epoch 661/2200]  [Disc loss: -0.688560] [Gen loss: 2.607615] \n",
      "[Epoch 662/2200]  [Disc loss: -0.690705] [Gen loss: 2.807008] \n",
      "[Epoch 663/2200]  [Disc loss: -0.687798] [Gen loss: 2.778181] \n",
      "[Epoch 664/2200]  [Disc loss: -0.689683] [Gen loss: 2.850885] \n",
      "[Epoch 665/2200]  [Disc loss: -0.685740] [Gen loss: 2.870724] \n",
      "[Epoch 666/2200]  [Disc loss: -0.688340] [Gen loss: 2.677562] \n",
      "[Epoch 667/2200]  [Disc loss: -0.686691] [Gen loss: 2.689330] \n",
      "[Epoch 668/2200]  [Disc loss: -0.685155] [Gen loss: 2.803170] \n",
      "[Epoch 669/2200]  [Disc loss: -0.689197] [Gen loss: 2.578746] \n",
      "[Epoch 670/2200]  [Disc loss: -0.687378] [Gen loss: 2.717985] \n",
      "[Epoch 671/2200]  [Disc loss: -0.680985] [Gen loss: 2.623370] \n",
      "[Epoch 672/2200]  [Disc loss: -0.687234] [Gen loss: 2.640068] \n",
      "[Epoch 673/2200]  [Disc loss: -0.686140] [Gen loss: 2.579213] \n",
      "[Epoch 674/2200]  [Disc loss: -0.686584] [Gen loss: 2.684074] \n",
      "[Epoch 675/2200]  [Disc loss: -0.687068] [Gen loss: 2.629491] \n",
      "[Epoch 676/2200]  [Disc loss: -0.686011] [Gen loss: 2.611159] \n",
      "[Epoch 677/2200]  [Disc loss: -0.683539] [Gen loss: 2.724376] \n",
      "[Epoch 678/2200]  [Disc loss: -0.683918] [Gen loss: 2.615957] \n",
      "[Epoch 679/2200]  [Disc loss: -0.681724] [Gen loss: 2.563634] \n",
      "[Epoch 680/2200]  [Disc loss: -0.686475] [Gen loss: 2.750651] \n",
      "[Epoch 681/2200]  [Disc loss: -0.687035] [Gen loss: 2.625403] \n",
      "[Epoch 682/2200]  [Disc loss: -0.689251] [Gen loss: 2.616834] \n",
      "[Epoch 683/2200]  [Disc loss: -0.685751] [Gen loss: 2.607412] \n",
      "[Epoch 684/2200]  [Disc loss: -0.685099] [Gen loss: 2.785857] \n",
      "[Epoch 685/2200]  [Disc loss: -0.684114] [Gen loss: 2.528871] \n",
      "[Epoch 686/2200]  [Disc loss: -0.680950] [Gen loss: 2.641200] \n",
      "[Epoch 687/2200]  [Disc loss: -0.685562] [Gen loss: 2.745627] \n",
      "[Epoch 688/2200]  [Disc loss: -0.685680] [Gen loss: 2.661720] \n",
      "[Epoch 689/2200]  [Disc loss: -0.679293] [Gen loss: 2.631378] \n",
      "[Epoch 690/2200]  [Disc loss: -0.685865] [Gen loss: 2.671915] \n",
      "[Epoch 691/2200]  [Disc loss: -0.683585] [Gen loss: 2.749605] \n",
      "[Epoch 692/2200]  [Disc loss: -0.681781] [Gen loss: 2.614844] \n",
      "[Epoch 693/2200]  [Disc loss: -0.685158] [Gen loss: 2.695518] \n",
      "[Epoch 694/2200]  [Disc loss: -0.682716] [Gen loss: 2.722651] \n",
      "[Epoch 695/2200]  [Disc loss: -0.684889] [Gen loss: 2.783605] \n",
      "[Epoch 696/2200]  [Disc loss: -0.683728] [Gen loss: 2.826931] \n",
      "[Epoch 697/2200]  [Disc loss: -0.685781] [Gen loss: 2.804042] \n",
      "[Epoch 698/2200]  [Disc loss: -0.681505] [Gen loss: 2.674049] \n",
      "[Epoch 699/2200]  [Disc loss: -0.681449] [Gen loss: 2.751063] \n",
      "[Epoch 700/2200]  [Disc loss: -0.684165] [Gen loss: 2.691831] \n",
      "[Epoch 701/2200]  [Disc loss: -0.680588] [Gen loss: 2.833680] \n",
      "[Epoch 702/2200]  [Disc loss: -0.683386] [Gen loss: 2.711511] \n",
      "[Epoch 703/2200]  [Disc loss: -0.678455] [Gen loss: 2.816742] \n",
      "[Epoch 704/2200]  [Disc loss: -0.682806] [Gen loss: 2.780990] \n",
      "[Epoch 705/2200]  [Disc loss: -0.684072] [Gen loss: 2.719328] \n",
      "[Epoch 706/2200]  [Disc loss: -0.674217] [Gen loss: 2.741059] \n",
      "[Epoch 707/2200]  [Disc loss: -0.684349] [Gen loss: 2.786214] \n",
      "[Epoch 708/2200]  [Disc loss: -0.677899] [Gen loss: 2.727099] \n",
      "[Epoch 709/2200]  [Disc loss: -0.677512] [Gen loss: 2.803509] \n",
      "[Epoch 710/2200]  [Disc loss: -0.686938] [Gen loss: 2.993296] \n",
      "[Epoch 711/2200]  [Disc loss: -0.678039] [Gen loss: 2.790662] \n",
      "[Epoch 712/2200]  [Disc loss: -0.675861] [Gen loss: 2.853614] \n",
      "[Epoch 713/2200]  [Disc loss: -0.678434] [Gen loss: 2.940523] \n",
      "[Epoch 714/2200]  [Disc loss: -0.677718] [Gen loss: 2.934776] \n",
      "[Epoch 715/2200]  [Disc loss: -0.677482] [Gen loss: 2.791306] \n",
      "[Epoch 716/2200]  [Disc loss: -0.677092] [Gen loss: 3.067458] \n",
      "[Epoch 717/2200]  [Disc loss: -0.676665] [Gen loss: 2.965228] \n",
      "[Epoch 718/2200]  [Disc loss: -0.680270] [Gen loss: 2.988926] \n",
      "[Epoch 719/2200]  [Disc loss: -0.680739] [Gen loss: 2.958516] \n",
      "[Epoch 720/2200]  [Disc loss: -0.670520] [Gen loss: 3.021599] \n",
      "[Epoch 721/2200]  [Disc loss: -0.681041] [Gen loss: 3.073771] \n",
      "[Epoch 722/2200]  [Disc loss: -0.679376] [Gen loss: 2.929974] \n",
      "[Epoch 723/2200]  [Disc loss: -0.678194] [Gen loss: 3.048053] \n",
      "[Epoch 724/2200]  [Disc loss: -0.678782] [Gen loss: 3.065888] \n",
      "[Epoch 725/2200]  [Disc loss: -0.668774] [Gen loss: 3.116100] \n",
      "[Epoch 726/2200]  [Disc loss: -0.676458] [Gen loss: 2.996125] \n",
      "[Epoch 727/2200]  [Disc loss: -0.672621] [Gen loss: 3.096416] \n",
      "[Epoch 728/2200]  [Disc loss: -0.672846] [Gen loss: 2.980255] \n",
      "[Epoch 729/2200]  [Disc loss: -0.679041] [Gen loss: 2.896809] \n",
      "[Epoch 730/2200]  [Disc loss: -0.670016] [Gen loss: 3.199441] \n",
      "[Epoch 731/2200]  [Disc loss: -0.670975] [Gen loss: 3.028498] \n",
      "[Epoch 732/2200]  [Disc loss: -0.674548] [Gen loss: 3.138055] \n",
      "[Epoch 733/2200]  [Disc loss: -0.669491] [Gen loss: 3.005129] \n",
      "[Epoch 734/2200]  [Disc loss: -0.665945] [Gen loss: 3.007581] \n",
      "[Epoch 735/2200]  [Disc loss: -0.665113] [Gen loss: 2.934980] \n",
      "[Epoch 736/2200]  [Disc loss: -0.669636] [Gen loss: 3.093103] \n",
      "[Epoch 737/2200]  [Disc loss: -0.666334] [Gen loss: 2.977670] \n",
      "[Epoch 738/2200]  [Disc loss: -0.665473] [Gen loss: 3.084011] \n",
      "[Epoch 739/2200]  [Disc loss: -0.666523] [Gen loss: 3.033624] \n",
      "[Epoch 740/2200]  [Disc loss: -0.669799] [Gen loss: 3.019297] \n",
      "[Epoch 741/2200]  [Disc loss: -0.669447] [Gen loss: 2.933229] \n",
      "[Epoch 742/2200]  [Disc loss: -0.664672] [Gen loss: 3.048397] \n",
      "[Epoch 743/2200]  [Disc loss: -0.664339] [Gen loss: 3.067168] \n",
      "[Epoch 744/2200]  [Disc loss: -0.667487] [Gen loss: 3.048028] \n",
      "[Epoch 745/2200]  [Disc loss: -0.668679] [Gen loss: 2.933341] \n",
      "[Epoch 746/2200]  [Disc loss: -0.666904] [Gen loss: 3.058561] \n",
      "[Epoch 747/2200]  [Disc loss: -0.667977] [Gen loss: 3.160193] \n",
      "[Epoch 748/2200]  [Disc loss: -0.664876] [Gen loss: 3.145092] \n",
      "[Epoch 749/2200]  [Disc loss: -0.666617] [Gen loss: 3.177554] \n",
      "[Epoch 750/2200]  [Disc loss: -0.664260] [Gen loss: 3.132717] \n",
      "[Epoch 751/2200]  [Disc loss: -0.663235] [Gen loss: 3.154366] \n",
      "[Epoch 752/2200]  [Disc loss: -0.664318] [Gen loss: 3.226736] \n",
      "[Epoch 753/2200]  [Disc loss: -0.665326] [Gen loss: 3.251962] \n",
      "[Epoch 754/2200]  [Disc loss: -0.662510] [Gen loss: 3.247209] \n",
      "[Epoch 755/2200]  [Disc loss: -0.664651] [Gen loss: 3.070396] \n",
      "[Epoch 756/2200]  [Disc loss: -0.661674] [Gen loss: 3.289530] \n",
      "[Epoch 757/2200]  [Disc loss: -0.663378] [Gen loss: 3.294384] \n",
      "[Epoch 758/2200]  [Disc loss: -0.665310] [Gen loss: 3.177635] \n",
      "[Epoch 759/2200]  [Disc loss: -0.656377] [Gen loss: 3.360086] \n",
      "[Epoch 760/2200]  [Disc loss: -0.664591] [Gen loss: 3.256728] \n",
      "[Epoch 761/2200]  [Disc loss: -0.662168] [Gen loss: 3.042872] \n",
      "[Epoch 762/2200]  [Disc loss: -0.660119] [Gen loss: 3.214141] \n",
      "[Epoch 763/2200]  [Disc loss: -0.668704] [Gen loss: 3.095658] \n",
      "[Epoch 764/2200]  [Disc loss: -0.659530] [Gen loss: 3.260570] \n",
      "[Epoch 765/2200]  [Disc loss: -0.665888] [Gen loss: 3.266137] \n",
      "[Epoch 766/2200]  [Disc loss: -0.658294] [Gen loss: 3.251590] \n",
      "[Epoch 767/2200]  [Disc loss: -0.660329] [Gen loss: 3.281611] \n",
      "[Epoch 768/2200]  [Disc loss: -0.659587] [Gen loss: 3.224344] \n",
      "[Epoch 769/2200]  [Disc loss: -0.658384] [Gen loss: 3.080275] \n",
      "[Epoch 770/2200]  [Disc loss: -0.655669] [Gen loss: 3.243045] \n",
      "[Epoch 771/2200]  [Disc loss: -0.660793] [Gen loss: 3.160669] \n",
      "[Epoch 772/2200]  [Disc loss: -0.655385] [Gen loss: 3.150409] \n",
      "[Epoch 773/2200]  [Disc loss: -0.659291] [Gen loss: 3.341874] \n",
      "[Epoch 774/2200]  [Disc loss: -0.652969] [Gen loss: 3.194442] \n",
      "[Epoch 775/2200]  [Disc loss: -0.663166] [Gen loss: 3.174751] \n",
      "[Epoch 776/2200]  [Disc loss: -0.657801] [Gen loss: 3.226039] \n",
      "[Epoch 777/2200]  [Disc loss: -0.662437] [Gen loss: 3.204545] \n",
      "[Epoch 778/2200]  [Disc loss: -0.653630] [Gen loss: 3.080324] \n",
      "[Epoch 779/2200]  [Disc loss: -0.656324] [Gen loss: 3.239173] \n",
      "[Epoch 780/2200]  [Disc loss: -0.656955] [Gen loss: 3.152937] \n",
      "[Epoch 781/2200]  [Disc loss: -0.656402] [Gen loss: 3.148910] \n",
      "[Epoch 782/2200]  [Disc loss: -0.657084] [Gen loss: 3.131319] \n",
      "[Epoch 783/2200]  [Disc loss: -0.653353] [Gen loss: 3.120254] \n",
      "[Epoch 784/2200]  [Disc loss: -0.653348] [Gen loss: 3.103370] \n",
      "[Epoch 785/2200]  [Disc loss: -0.654035] [Gen loss: 3.152228] \n",
      "[Epoch 786/2200]  [Disc loss: -0.662936] [Gen loss: 3.189250] \n",
      "[Epoch 787/2200]  [Disc loss: -0.653679] [Gen loss: 3.196308] \n",
      "[Epoch 788/2200]  [Disc loss: -0.657759] [Gen loss: 2.996545] \n",
      "[Epoch 789/2200]  [Disc loss: -0.657316] [Gen loss: 3.202708] \n",
      "[Epoch 790/2200]  [Disc loss: -0.656050] [Gen loss: 3.129745] \n",
      "[Epoch 791/2200]  [Disc loss: -0.661976] [Gen loss: 3.063016] \n",
      "[Epoch 792/2200]  [Disc loss: -0.657515] [Gen loss: 3.143677] \n",
      "[Epoch 793/2200]  [Disc loss: -0.655078] [Gen loss: 3.191892] \n",
      "[Epoch 794/2200]  [Disc loss: -0.657835] [Gen loss: 2.928184] \n",
      "[Epoch 795/2200]  [Disc loss: -0.657159] [Gen loss: 3.070616] \n",
      "[Epoch 796/2200]  [Disc loss: -0.657330] [Gen loss: 3.047780] \n",
      "[Epoch 797/2200]  [Disc loss: -0.658514] [Gen loss: 3.033702] \n",
      "[Epoch 798/2200]  [Disc loss: -0.655239] [Gen loss: 3.070727] \n",
      "[Epoch 799/2200]  [Disc loss: -0.656399] [Gen loss: 3.089864] \n",
      "[Epoch 800/2200]  [Disc loss: -0.650672] [Gen loss: 3.067156] \n",
      "[Epoch 801/2200]  [Disc loss: -0.656298] [Gen loss: 2.948575] \n",
      "[Epoch 802/2200]  [Disc loss: -0.652739] [Gen loss: 3.084502] \n",
      "[Epoch 803/2200]  [Disc loss: -0.651154] [Gen loss: 3.046030] \n",
      "[Epoch 804/2200]  [Disc loss: -0.651121] [Gen loss: 3.088076] \n",
      "[Epoch 805/2200]  [Disc loss: -0.659616] [Gen loss: 2.973895] \n",
      "[Epoch 806/2200]  [Disc loss: -0.653360] [Gen loss: 2.992984] \n",
      "[Epoch 807/2200]  [Disc loss: -0.656960] [Gen loss: 3.010665] \n",
      "[Epoch 808/2200]  [Disc loss: -0.650469] [Gen loss: 2.956198] \n",
      "[Epoch 809/2200]  [Disc loss: -0.652546] [Gen loss: 3.020015] \n",
      "[Epoch 810/2200]  [Disc loss: -0.652836] [Gen loss: 3.042105] \n",
      "[Epoch 811/2200]  [Disc loss: -0.652580] [Gen loss: 3.128342] \n",
      "[Epoch 812/2200]  [Disc loss: -0.654532] [Gen loss: 3.150434] \n",
      "[Epoch 813/2200]  [Disc loss: -0.652793] [Gen loss: 3.154025] \n",
      "[Epoch 814/2200]  [Disc loss: -0.652373] [Gen loss: 3.072048] \n",
      "[Epoch 815/2200]  [Disc loss: -0.653340] [Gen loss: 3.064827] \n",
      "[Epoch 816/2200]  [Disc loss: -0.656525] [Gen loss: 3.134211] \n",
      "[Epoch 817/2200]  [Disc loss: -0.652369] [Gen loss: 3.161249] \n",
      "[Epoch 818/2200]  [Disc loss: -0.649239] [Gen loss: 3.155756] \n",
      "[Epoch 819/2200]  [Disc loss: -0.656425] [Gen loss: 3.141808] \n",
      "[Epoch 820/2200]  [Disc loss: -0.651100] [Gen loss: 3.105283] \n",
      "[Epoch 821/2200]  [Disc loss: -0.655436] [Gen loss: 3.131437] \n",
      "[Epoch 822/2200]  [Disc loss: -0.653418] [Gen loss: 3.112047] \n",
      "[Epoch 823/2200]  [Disc loss: -0.649800] [Gen loss: 2.977585] \n",
      "[Epoch 824/2200]  [Disc loss: -0.649820] [Gen loss: 3.130225] \n",
      "[Epoch 825/2200]  [Disc loss: -0.649400] [Gen loss: 3.024076] \n",
      "[Epoch 826/2200]  [Disc loss: -0.657449] [Gen loss: 2.962532] \n",
      "[Epoch 827/2200]  [Disc loss: -0.649257] [Gen loss: 2.994649] \n",
      "[Epoch 828/2200]  [Disc loss: -0.652054] [Gen loss: 2.959811] \n",
      "[Epoch 829/2200]  [Disc loss: -0.649728] [Gen loss: 2.932610] \n",
      "[Epoch 830/2200]  [Disc loss: -0.646636] [Gen loss: 2.919136] \n",
      "[Epoch 831/2200]  [Disc loss: -0.655066] [Gen loss: 2.984262] \n",
      "[Epoch 832/2200]  [Disc loss: -0.647427] [Gen loss: 2.836706] \n",
      "[Epoch 833/2200]  [Disc loss: -0.647823] [Gen loss: 2.851472] \n",
      "[Epoch 834/2200]  [Disc loss: -0.649066] [Gen loss: 2.959339] \n",
      "[Epoch 835/2200]  [Disc loss: -0.650866] [Gen loss: 2.837546] \n",
      "[Epoch 836/2200]  [Disc loss: -0.646645] [Gen loss: 2.905886] \n",
      "[Epoch 837/2200]  [Disc loss: -0.648772] [Gen loss: 2.825327] \n",
      "[Epoch 838/2200]  [Disc loss: -0.655477] [Gen loss: 2.926321] \n",
      "[Epoch 839/2200]  [Disc loss: -0.649901] [Gen loss: 3.000581] \n",
      "[Epoch 840/2200]  [Disc loss: -0.652754] [Gen loss: 2.928086] \n",
      "[Epoch 841/2200]  [Disc loss: -0.646135] [Gen loss: 3.089688] \n",
      "[Epoch 842/2200]  [Disc loss: -0.646391] [Gen loss: 2.962007] \n",
      "[Epoch 843/2200]  [Disc loss: -0.645466] [Gen loss: 3.027551] \n",
      "[Epoch 844/2200]  [Disc loss: -0.654395] [Gen loss: 3.098908] \n",
      "[Epoch 845/2200]  [Disc loss: -0.646780] [Gen loss: 2.987501] \n",
      "[Epoch 846/2200]  [Disc loss: -0.650836] [Gen loss: 3.010804] \n",
      "[Epoch 847/2200]  [Disc loss: -0.645162] [Gen loss: 2.980551] \n",
      "[Epoch 848/2200]  [Disc loss: -0.648855] [Gen loss: 3.077336] \n",
      "[Epoch 849/2200]  [Disc loss: -0.646315] [Gen loss: 2.830092] \n",
      "[Epoch 850/2200]  [Disc loss: -0.644822] [Gen loss: 2.906570] \n",
      "[Epoch 851/2200]  [Disc loss: -0.650671] [Gen loss: 2.805167] \n",
      "[Epoch 852/2200]  [Disc loss: -0.648496] [Gen loss: 2.998890] \n",
      "[Epoch 853/2200]  [Disc loss: -0.642392] [Gen loss: 2.803145] \n",
      "[Epoch 854/2200]  [Disc loss: -0.644251] [Gen loss: 2.872574] \n",
      "[Epoch 855/2200]  [Disc loss: -0.645039] [Gen loss: 2.819170] \n",
      "[Epoch 856/2200]  [Disc loss: -0.642778] [Gen loss: 2.846768] \n",
      "[Epoch 857/2200]  [Disc loss: -0.644785] [Gen loss: 2.918143] \n",
      "[Epoch 858/2200]  [Disc loss: -0.649799] [Gen loss: 2.850675] \n",
      "[Epoch 859/2200]  [Disc loss: -0.646843] [Gen loss: 2.805548] \n",
      "[Epoch 860/2200]  [Disc loss: -0.644061] [Gen loss: 2.779872] \n",
      "[Epoch 861/2200]  [Disc loss: -0.651694] [Gen loss: 2.979078] \n",
      "[Epoch 862/2200]  [Disc loss: -0.647378] [Gen loss: 2.933384] \n",
      "[Epoch 863/2200]  [Disc loss: -0.644270] [Gen loss: 2.926933] \n",
      "[Epoch 864/2200]  [Disc loss: -0.646476] [Gen loss: 2.890666] \n",
      "[Epoch 865/2200]  [Disc loss: -0.648909] [Gen loss: 2.989607] \n",
      "[Epoch 866/2200]  [Disc loss: -0.645817] [Gen loss: 2.892714] \n",
      "[Epoch 867/2200]  [Disc loss: -0.644297] [Gen loss: 2.795539] \n",
      "[Epoch 868/2200]  [Disc loss: -0.644086] [Gen loss: 2.825515] \n",
      "[Epoch 869/2200]  [Disc loss: -0.645885] [Gen loss: 2.808969] \n",
      "[Epoch 870/2200]  [Disc loss: -0.647533] [Gen loss: 2.911614] \n",
      "[Epoch 871/2200]  [Disc loss: -0.649586] [Gen loss: 2.859834] \n",
      "[Epoch 872/2200]  [Disc loss: -0.643113] [Gen loss: 3.009529] \n",
      "[Epoch 873/2200]  [Disc loss: -0.641938] [Gen loss: 2.841092] \n",
      "[Epoch 874/2200]  [Disc loss: -0.643044] [Gen loss: 2.808566] \n",
      "[Epoch 875/2200]  [Disc loss: -0.648491] [Gen loss: 2.899516] \n",
      "[Epoch 876/2200]  [Disc loss: -0.647121] [Gen loss: 2.899955] \n",
      "[Epoch 877/2200]  [Disc loss: -0.637042] [Gen loss: 2.944131] \n",
      "[Epoch 878/2200]  [Disc loss: -0.646157] [Gen loss: 2.853618] \n",
      "[Epoch 879/2200]  [Disc loss: -0.642159] [Gen loss: 2.856113] \n",
      "[Epoch 880/2200]  [Disc loss: -0.642902] [Gen loss: 2.882758] \n",
      "[Epoch 881/2200]  [Disc loss: -0.639803] [Gen loss: 2.941910] \n",
      "[Epoch 882/2200]  [Disc loss: -0.642646] [Gen loss: 2.962038] \n",
      "[Epoch 883/2200]  [Disc loss: -0.639680] [Gen loss: 2.828018] \n",
      "[Epoch 884/2200]  [Disc loss: -0.644581] [Gen loss: 3.008583] \n",
      "[Epoch 885/2200]  [Disc loss: -0.635370] [Gen loss: 2.839626] \n",
      "[Epoch 886/2200]  [Disc loss: -0.645297] [Gen loss: 2.800465] \n",
      "[Epoch 887/2200]  [Disc loss: -0.640842] [Gen loss: 2.909003] \n",
      "[Epoch 888/2200]  [Disc loss: -0.638556] [Gen loss: 2.816802] \n",
      "[Epoch 889/2200]  [Disc loss: -0.637491] [Gen loss: 2.982788] \n",
      "[Epoch 890/2200]  [Disc loss: -0.646894] [Gen loss: 2.999759] \n",
      "[Epoch 891/2200]  [Disc loss: -0.638378] [Gen loss: 2.699210] \n",
      "[Epoch 892/2200]  [Disc loss: -0.641359] [Gen loss: 3.096343] \n",
      "[Epoch 893/2200]  [Disc loss: -0.642628] [Gen loss: 3.036849] \n",
      "[Epoch 894/2200]  [Disc loss: -0.642030] [Gen loss: 2.930583] \n",
      "[Epoch 895/2200]  [Disc loss: -0.635987] [Gen loss: 3.139570] \n",
      "[Epoch 896/2200]  [Disc loss: -0.644007] [Gen loss: 3.071554] \n",
      "[Epoch 897/2200]  [Disc loss: -0.645485] [Gen loss: 2.972986] \n",
      "[Epoch 898/2200]  [Disc loss: -0.638353] [Gen loss: 3.039171] \n",
      "[Epoch 899/2200]  [Disc loss: -0.644141] [Gen loss: 3.158591] \n",
      "[Epoch 900/2200]  [Disc loss: -0.643256] [Gen loss: 3.097411] \n",
      "[Epoch 901/2200]  [Disc loss: -0.640145] [Gen loss: 3.168576] \n",
      "[Epoch 902/2200]  [Disc loss: -0.642547] [Gen loss: 3.047453] \n",
      "[Epoch 903/2200]  [Disc loss: -0.640835] [Gen loss: 3.052936] \n",
      "[Epoch 904/2200]  [Disc loss: -0.635606] [Gen loss: 2.982773] \n",
      "[Epoch 905/2200]  [Disc loss: -0.640003] [Gen loss: 2.946552] \n",
      "[Epoch 906/2200]  [Disc loss: -0.641124] [Gen loss: 2.961450] \n",
      "[Epoch 907/2200]  [Disc loss: -0.633868] [Gen loss: 2.984600] \n",
      "[Epoch 908/2200]  [Disc loss: -0.641840] [Gen loss: 2.903669] \n",
      "[Epoch 909/2200]  [Disc loss: -0.638602] [Gen loss: 2.950850] \n",
      "[Epoch 910/2200]  [Disc loss: -0.638332] [Gen loss: 2.918709] \n",
      "[Epoch 911/2200]  [Disc loss: -0.636671] [Gen loss: 2.918631] \n",
      "[Epoch 912/2200]  [Disc loss: -0.638947] [Gen loss: 2.922199] \n",
      "[Epoch 913/2200]  [Disc loss: -0.631431] [Gen loss: 2.918047] \n",
      "[Epoch 914/2200]  [Disc loss: -0.637312] [Gen loss: 2.872249] \n",
      "[Epoch 915/2200]  [Disc loss: -0.635812] [Gen loss: 2.992428] \n",
      "[Epoch 916/2200]  [Disc loss: -0.635556] [Gen loss: 2.993642] \n",
      "[Epoch 917/2200]  [Disc loss: -0.635162] [Gen loss: 2.874771] \n",
      "[Epoch 918/2200]  [Disc loss: -0.634204] [Gen loss: 2.984085] \n",
      "[Epoch 919/2200]  [Disc loss: -0.635317] [Gen loss: 3.004933] \n",
      "[Epoch 920/2200]  [Disc loss: -0.640024] [Gen loss: 2.888369] \n",
      "[Epoch 921/2200]  [Disc loss: -0.638571] [Gen loss: 2.878655] \n",
      "[Epoch 922/2200]  [Disc loss: -0.642422] [Gen loss: 2.945065] \n",
      "[Epoch 923/2200]  [Disc loss: -0.637797] [Gen loss: 2.969183] \n",
      "[Epoch 924/2200]  [Disc loss: -0.638796] [Gen loss: 2.934435] \n",
      "[Epoch 925/2200]  [Disc loss: -0.639202] [Gen loss: 2.966134] \n",
      "[Epoch 926/2200]  [Disc loss: -0.636460] [Gen loss: 2.918295] \n",
      "[Epoch 927/2200]  [Disc loss: -0.632908] [Gen loss: 2.973919] \n",
      "[Epoch 928/2200]  [Disc loss: -0.638022] [Gen loss: 2.937977] \n",
      "[Epoch 929/2200]  [Disc loss: -0.637433] [Gen loss: 2.907931] \n",
      "[Epoch 930/2200]  [Disc loss: -0.634367] [Gen loss: 3.084087] \n",
      "[Epoch 931/2200]  [Disc loss: -0.637442] [Gen loss: 3.053013] \n",
      "[Epoch 932/2200]  [Disc loss: -0.634329] [Gen loss: 3.130816] \n",
      "[Epoch 933/2200]  [Disc loss: -0.637907] [Gen loss: 3.189616] \n",
      "[Epoch 934/2200]  [Disc loss: -0.631814] [Gen loss: 3.079822] \n",
      "[Epoch 935/2200]  [Disc loss: -0.630200] [Gen loss: 3.115262] \n",
      "[Epoch 936/2200]  [Disc loss: -0.631125] [Gen loss: 3.106508] \n",
      "[Epoch 937/2200]  [Disc loss: -0.631675] [Gen loss: 3.076415] \n",
      "[Epoch 938/2200]  [Disc loss: -0.632864] [Gen loss: 3.114945] \n",
      "[Epoch 939/2200]  [Disc loss: -0.639106] [Gen loss: 3.197426] \n",
      "[Epoch 940/2200]  [Disc loss: -0.631587] [Gen loss: 2.980661] \n",
      "[Epoch 941/2200]  [Disc loss: -0.633486] [Gen loss: 3.065994] \n",
      "[Epoch 942/2200]  [Disc loss: -0.640121] [Gen loss: 3.099867] \n",
      "[Epoch 943/2200]  [Disc loss: -0.633841] [Gen loss: 3.131600] \n",
      "[Epoch 944/2200]  [Disc loss: -0.630805] [Gen loss: 3.081495] \n",
      "[Epoch 945/2200]  [Disc loss: -0.633714] [Gen loss: 3.108140] \n",
      "[Epoch 946/2200]  [Disc loss: -0.631426] [Gen loss: 3.095635] \n",
      "[Epoch 947/2200]  [Disc loss: -0.634307] [Gen loss: 3.098918] \n",
      "[Epoch 948/2200]  [Disc loss: -0.637705] [Gen loss: 3.071177] \n",
      "[Epoch 949/2200]  [Disc loss: -0.629793] [Gen loss: 3.064748] \n",
      "[Epoch 950/2200]  [Disc loss: -0.632989] [Gen loss: 3.071853] \n",
      "[Epoch 951/2200]  [Disc loss: -0.631625] [Gen loss: 3.187018] \n",
      "[Epoch 952/2200]  [Disc loss: -0.635397] [Gen loss: 3.118162] \n",
      "[Epoch 953/2200]  [Disc loss: -0.627820] [Gen loss: 3.087192] \n",
      "[Epoch 954/2200]  [Disc loss: -0.625497] [Gen loss: 3.032474] \n",
      "[Epoch 955/2200]  [Disc loss: -0.630750] [Gen loss: 3.152171] \n",
      "[Epoch 956/2200]  [Disc loss: -0.630845] [Gen loss: 3.028479] \n",
      "[Epoch 957/2200]  [Disc loss: -0.629941] [Gen loss: 2.892657] \n",
      "[Epoch 958/2200]  [Disc loss: -0.632598] [Gen loss: 3.127461] \n",
      "[Epoch 959/2200]  [Disc loss: -0.628080] [Gen loss: 3.172151] \n",
      "[Epoch 960/2200]  [Disc loss: -0.628017] [Gen loss: 3.070911] \n",
      "[Epoch 961/2200]  [Disc loss: -0.631882] [Gen loss: 3.173305] \n",
      "[Epoch 962/2200]  [Disc loss: -0.628375] [Gen loss: 3.169971] \n",
      "[Epoch 963/2200]  [Disc loss: -0.633301] [Gen loss: 3.215585] \n",
      "[Epoch 964/2200]  [Disc loss: -0.627640] [Gen loss: 3.058067] \n",
      "[Epoch 965/2200]  [Disc loss: -0.631594] [Gen loss: 3.147072] \n",
      "[Epoch 966/2200]  [Disc loss: -0.626357] [Gen loss: 3.102194] \n",
      "[Epoch 967/2200]  [Disc loss: -0.626777] [Gen loss: 3.117200] \n",
      "[Epoch 968/2200]  [Disc loss: -0.628018] [Gen loss: 3.188826] \n",
      "[Epoch 969/2200]  [Disc loss: -0.631255] [Gen loss: 3.068297] \n",
      "[Epoch 970/2200]  [Disc loss: -0.625716] [Gen loss: 3.141851] \n",
      "[Epoch 971/2200]  [Disc loss: -0.628990] [Gen loss: 3.115294] \n",
      "[Epoch 972/2200]  [Disc loss: -0.622011] [Gen loss: 3.202909] \n",
      "[Epoch 973/2200]  [Disc loss: -0.628499] [Gen loss: 3.134904] \n",
      "[Epoch 974/2200]  [Disc loss: -0.627385] [Gen loss: 3.213775] \n",
      "[Epoch 975/2200]  [Disc loss: -0.632527] [Gen loss: 3.069472] \n",
      "[Epoch 976/2200]  [Disc loss: -0.626838] [Gen loss: 3.057532] \n",
      "[Epoch 977/2200]  [Disc loss: -0.626175] [Gen loss: 3.238314] \n",
      "[Epoch 978/2200]  [Disc loss: -0.626642] [Gen loss: 3.229098] \n",
      "[Epoch 979/2200]  [Disc loss: -0.627195] [Gen loss: 3.002086] \n",
      "[Epoch 980/2200]  [Disc loss: -0.629398] [Gen loss: 3.205899] \n",
      "[Epoch 981/2200]  [Disc loss: -0.629007] [Gen loss: 3.211689] \n",
      "[Epoch 982/2200]  [Disc loss: -0.621540] [Gen loss: 3.101542] \n",
      "[Epoch 983/2200]  [Disc loss: -0.627425] [Gen loss: 3.114512] \n",
      "[Epoch 984/2200]  [Disc loss: -0.622408] [Gen loss: 3.177550] \n",
      "[Epoch 985/2200]  [Disc loss: -0.626945] [Gen loss: 3.125092] \n",
      "[Epoch 986/2200]  [Disc loss: -0.623893] [Gen loss: 3.198217] \n",
      "[Epoch 987/2200]  [Disc loss: -0.631861] [Gen loss: 3.187400] \n",
      "[Epoch 988/2200]  [Disc loss: -0.624856] [Gen loss: 3.139622] \n",
      "[Epoch 989/2200]  [Disc loss: -0.626501] [Gen loss: 3.088297] \n",
      "[Epoch 990/2200]  [Disc loss: -0.621990] [Gen loss: 3.067154] \n",
      "[Epoch 991/2200]  [Disc loss: -0.623691] [Gen loss: 3.139510] \n",
      "[Epoch 992/2200]  [Disc loss: -0.623075] [Gen loss: 3.033893] \n",
      "[Epoch 993/2200]  [Disc loss: -0.622143] [Gen loss: 3.042730] \n",
      "[Epoch 994/2200]  [Disc loss: -0.625886] [Gen loss: 3.048489] \n",
      "[Epoch 995/2200]  [Disc loss: -0.623079] [Gen loss: 3.026957] \n",
      "[Epoch 996/2200]  [Disc loss: -0.623093] [Gen loss: 3.021321] \n",
      "[Epoch 997/2200]  [Disc loss: -0.624176] [Gen loss: 3.070620] \n",
      "[Epoch 998/2200]  [Disc loss: -0.627520] [Gen loss: 3.167593] \n",
      "[Epoch 999/2200]  [Disc loss: -0.622134] [Gen loss: 3.005666] \n",
      "[Epoch 1000/2200]  [Disc loss: -0.622678] [Gen loss: 3.092385] \n",
      "[Epoch 1001/2200]  [Disc loss: -0.620589] [Gen loss: 3.026636] \n",
      "[Epoch 1002/2200]  [Disc loss: -0.625566] [Gen loss: 3.023403] \n",
      "[Epoch 1003/2200]  [Disc loss: -0.619878] [Gen loss: 3.050327] \n",
      "[Epoch 1004/2200]  [Disc loss: -0.628226] [Gen loss: 3.066712] \n",
      "[Epoch 1005/2200]  [Disc loss: -0.625240] [Gen loss: 3.115399] \n",
      "[Epoch 1006/2200]  [Disc loss: -0.621487] [Gen loss: 3.173823] \n",
      "[Epoch 1007/2200]  [Disc loss: -0.621012] [Gen loss: 2.990418] \n",
      "[Epoch 1008/2200]  [Disc loss: -0.621364] [Gen loss: 3.035840] \n",
      "[Epoch 1009/2200]  [Disc loss: -0.622464] [Gen loss: 3.121182] \n",
      "[Epoch 1010/2200]  [Disc loss: -0.617257] [Gen loss: 3.014802] \n",
      "[Epoch 1011/2200]  [Disc loss: -0.621448] [Gen loss: 3.059814] \n",
      "[Epoch 1012/2200]  [Disc loss: -0.620851] [Gen loss: 3.152807] \n",
      "[Epoch 1013/2200]  [Disc loss: -0.622130] [Gen loss: 3.032848] \n",
      "[Epoch 1014/2200]  [Disc loss: -0.621186] [Gen loss: 3.070622] \n",
      "[Epoch 1015/2200]  [Disc loss: -0.625485] [Gen loss: 3.047955] \n",
      "[Epoch 1016/2200]  [Disc loss: -0.616432] [Gen loss: 2.995365] \n",
      "[Epoch 1017/2200]  [Disc loss: -0.620421] [Gen loss: 2.964510] \n",
      "[Epoch 1018/2200]  [Disc loss: -0.620785] [Gen loss: 3.073958] \n",
      "[Epoch 1019/2200]  [Disc loss: -0.622581] [Gen loss: 2.895802] \n",
      "[Epoch 1020/2200]  [Disc loss: -0.623169] [Gen loss: 3.093898] \n",
      "[Epoch 1021/2200]  [Disc loss: -0.617241] [Gen loss: 2.972954] \n",
      "[Epoch 1022/2200]  [Disc loss: -0.625309] [Gen loss: 3.029290] \n",
      "[Epoch 1023/2200]  [Disc loss: -0.624009] [Gen loss: 3.014894] \n",
      "[Epoch 1024/2200]  [Disc loss: -0.621219] [Gen loss: 2.936987] \n",
      "[Epoch 1025/2200]  [Disc loss: -0.620437] [Gen loss: 2.990702] \n",
      "[Epoch 1026/2200]  [Disc loss: -0.617431] [Gen loss: 3.013010] \n",
      "[Epoch 1027/2200]  [Disc loss: -0.615198] [Gen loss: 2.971218] \n",
      "[Epoch 1028/2200]  [Disc loss: -0.621524] [Gen loss: 2.921650] \n",
      "[Epoch 1029/2200]  [Disc loss: -0.620382] [Gen loss: 3.053848] \n",
      "[Epoch 1030/2200]  [Disc loss: -0.618284] [Gen loss: 2.891090] \n",
      "[Epoch 1031/2200]  [Disc loss: -0.619917] [Gen loss: 3.099976] \n",
      "[Epoch 1032/2200]  [Disc loss: -0.617977] [Gen loss: 3.079596] \n",
      "[Epoch 1033/2200]  [Disc loss: -0.619197] [Gen loss: 2.917972] \n",
      "[Epoch 1034/2200]  [Disc loss: -0.618074] [Gen loss: 3.242926] \n",
      "[Epoch 1035/2200]  [Disc loss: -0.618467] [Gen loss: 3.113626] \n",
      "[Epoch 1036/2200]  [Disc loss: -0.613288] [Gen loss: 2.981952] \n",
      "[Epoch 1037/2200]  [Disc loss: -0.619846] [Gen loss: 3.090348] \n",
      "[Epoch 1038/2200]  [Disc loss: -0.615984] [Gen loss: 2.973442] \n",
      "[Epoch 1039/2200]  [Disc loss: -0.613842] [Gen loss: 2.876612] \n",
      "[Epoch 1040/2200]  [Disc loss: -0.614152] [Gen loss: 3.103865] \n",
      "[Epoch 1041/2200]  [Disc loss: -0.616087] [Gen loss: 2.981557] \n",
      "[Epoch 1042/2200]  [Disc loss: -0.619638] [Gen loss: 2.930841] \n",
      "[Epoch 1043/2200]  [Disc loss: -0.615206] [Gen loss: 2.979637] \n",
      "[Epoch 1044/2200]  [Disc loss: -0.617341] [Gen loss: 2.974870] \n",
      "[Epoch 1045/2200]  [Disc loss: -0.614116] [Gen loss: 3.060865] \n",
      "[Epoch 1046/2200]  [Disc loss: -0.615934] [Gen loss: 2.947209] \n",
      "[Epoch 1047/2200]  [Disc loss: -0.613038] [Gen loss: 3.035539] \n",
      "[Epoch 1048/2200]  [Disc loss: -0.618831] [Gen loss: 2.912673] \n",
      "[Epoch 1049/2200]  [Disc loss: -0.620410] [Gen loss: 3.037008] \n",
      "[Epoch 1050/2200]  [Disc loss: -0.611875] [Gen loss: 2.980911] \n",
      "[Epoch 1051/2200]  [Disc loss: -0.618495] [Gen loss: 3.017039] \n",
      "[Epoch 1052/2200]  [Disc loss: -0.617184] [Gen loss: 3.069383] \n",
      "[Epoch 1053/2200]  [Disc loss: -0.615803] [Gen loss: 2.914386] \n",
      "[Epoch 1054/2200]  [Disc loss: -0.617179] [Gen loss: 3.083652] \n",
      "[Epoch 1055/2200]  [Disc loss: -0.613444] [Gen loss: 2.998355] \n",
      "[Epoch 1056/2200]  [Disc loss: -0.608875] [Gen loss: 2.956657] \n",
      "[Epoch 1057/2200]  [Disc loss: -0.610858] [Gen loss: 2.984947] \n",
      "[Epoch 1058/2200]  [Disc loss: -0.617114] [Gen loss: 3.045243] \n",
      "[Epoch 1059/2200]  [Disc loss: -0.615902] [Gen loss: 3.056838] \n",
      "[Epoch 1060/2200]  [Disc loss: -0.607960] [Gen loss: 3.200913] \n",
      "[Epoch 1061/2200]  [Disc loss: -0.615353] [Gen loss: 2.979168] \n",
      "[Epoch 1062/2200]  [Disc loss: -0.617563] [Gen loss: 2.981006] \n",
      "[Epoch 1063/2200]  [Disc loss: -0.611957] [Gen loss: 2.998805] \n",
      "[Epoch 1064/2200]  [Disc loss: -0.610448] [Gen loss: 3.116468] \n",
      "[Epoch 1065/2200]  [Disc loss: -0.618022] [Gen loss: 3.082771] \n",
      "[Epoch 1066/2200]  [Disc loss: -0.615852] [Gen loss: 2.945180] \n",
      "[Epoch 1067/2200]  [Disc loss: -0.609991] [Gen loss: 3.052370] \n",
      "[Epoch 1068/2200]  [Disc loss: -0.611705] [Gen loss: 2.996053] \n",
      "[Epoch 1069/2200]  [Disc loss: -0.611433] [Gen loss: 3.175380] \n",
      "[Epoch 1070/2200]  [Disc loss: -0.612768] [Gen loss: 2.919661] \n",
      "[Epoch 1071/2200]  [Disc loss: -0.608300] [Gen loss: 2.970187] \n",
      "[Epoch 1072/2200]  [Disc loss: -0.610359] [Gen loss: 3.157681] \n",
      "[Epoch 1073/2200]  [Disc loss: -0.611235] [Gen loss: 3.134624] \n",
      "[Epoch 1074/2200]  [Disc loss: -0.610498] [Gen loss: 3.100936] \n",
      "[Epoch 1075/2200]  [Disc loss: -0.610976] [Gen loss: 3.042741] \n",
      "[Epoch 1076/2200]  [Disc loss: -0.613094] [Gen loss: 3.113194] \n",
      "[Epoch 1077/2200]  [Disc loss: -0.610404] [Gen loss: 3.071494] \n",
      "[Epoch 1078/2200]  [Disc loss: -0.612452] [Gen loss: 3.038738] \n",
      "[Epoch 1079/2200]  [Disc loss: -0.603332] [Gen loss: 3.155719] \n",
      "[Epoch 1080/2200]  [Disc loss: -0.610169] [Gen loss: 3.089094] \n",
      "[Epoch 1081/2200]  [Disc loss: -0.601685] [Gen loss: 2.995527] \n",
      "[Epoch 1082/2200]  [Disc loss: -0.609232] [Gen loss: 2.941916] \n",
      "[Epoch 1083/2200]  [Disc loss: -0.613899] [Gen loss: 2.993437] \n",
      "[Epoch 1084/2200]  [Disc loss: -0.613195] [Gen loss: 3.033814] \n",
      "[Epoch 1085/2200]  [Disc loss: -0.606666] [Gen loss: 3.077523] \n",
      "[Epoch 1086/2200]  [Disc loss: -0.608056] [Gen loss: 3.103114] \n",
      "[Epoch 1087/2200]  [Disc loss: -0.608746] [Gen loss: 2.893357] \n",
      "[Epoch 1088/2200]  [Disc loss: -0.611764] [Gen loss: 3.045232] \n",
      "[Epoch 1089/2200]  [Disc loss: -0.613049] [Gen loss: 3.215706] \n",
      "[Epoch 1090/2200]  [Disc loss: -0.604956] [Gen loss: 2.998458] \n",
      "[Epoch 1091/2200]  [Disc loss: -0.609189] [Gen loss: 3.004542] \n",
      "[Epoch 1092/2200]  [Disc loss: -0.607720] [Gen loss: 3.188348] \n",
      "[Epoch 1093/2200]  [Disc loss: -0.606088] [Gen loss: 3.103525] \n",
      "[Epoch 1094/2200]  [Disc loss: -0.610262] [Gen loss: 3.134694] \n",
      "[Epoch 1095/2200]  [Disc loss: -0.604705] [Gen loss: 2.949215] \n",
      "[Epoch 1096/2200]  [Disc loss: -0.605011] [Gen loss: 3.103728] \n",
      "[Epoch 1097/2200]  [Disc loss: -0.610207] [Gen loss: 3.116681] \n",
      "[Epoch 1098/2200]  [Disc loss: -0.600755] [Gen loss: 3.215864] \n",
      "[Epoch 1099/2200]  [Disc loss: -0.608794] [Gen loss: 3.117002] \n",
      "[Epoch 1100/2200]  [Disc loss: -0.604097] [Gen loss: 3.174812] \n",
      "[Epoch 1101/2200]  [Disc loss: -0.604834] [Gen loss: 3.119963] \n",
      "[Epoch 1102/2200]  [Disc loss: -0.607310] [Gen loss: 3.059869] \n",
      "[Epoch 1103/2200]  [Disc loss: -0.604114] [Gen loss: 3.104939] \n",
      "[Epoch 1104/2200]  [Disc loss: -0.605259] [Gen loss: 3.274996] \n",
      "[Epoch 1105/2200]  [Disc loss: -0.608753] [Gen loss: 2.975076] \n",
      "[Epoch 1106/2200]  [Disc loss: -0.601148] [Gen loss: 3.060459] \n",
      "[Epoch 1107/2200]  [Disc loss: -0.604382] [Gen loss: 3.207433] \n",
      "[Epoch 1108/2200]  [Disc loss: -0.603137] [Gen loss: 3.022967] \n",
      "[Epoch 1109/2200]  [Disc loss: -0.604716] [Gen loss: 3.052866] \n",
      "[Epoch 1110/2200]  [Disc loss: -0.607295] [Gen loss: 3.015473] \n"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eTARoSEVlICZ"
   },
   "outputs": [],
   "source": [
    "torch.no_grad()\n",
    "trainer.G.eval()\n",
    "\n",
    "S = Sampler(generator=trainer.G)\n",
    "latent = S.sample(10) #10 samples\n",
    "latent = latent.detach().cpu().numpy().tolist()\n",
    "\n",
    "sampled_mols_save_path = os.path.join(trainer.output_model_folder, 'sampled')\n",
    "np.save(sampled_mols_save_path+f'_epoch{200}', latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w1VyOt8pH0Dn"
   },
   "outputs": [],
   "source": [
    "x = np.load('/content/model/sampled_epoch200.npy')\n",
    "pd.DataFrame(x).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avXnOi9WH9tg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "JTNN+LatenGAN_train",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
